{
  "schemeVersion": 3,
  "dataVersion": 3,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "isProgramEnabled": true,
  "conference": {
    "id": 10055,
    "startDate": 1604275200000,
    "endDate": 1604448000000,
    "shortName": "VRST",
    "name": "VRST 2020",
    "year": 2020,
    "fullName": "ACM Symposium on Virtual Reality Software and Technology",
    "url": "https://vrst.acm.org/vrst2020",
    "location": "Online",
    "timeZoneOffset": -300,
    "logoUrl": "https://files.sigchi.org/conference/logo/b11a210b-bdc1-664c-4e6e-c35395d639c3.png",
    "hideVideoLinksBeforeConference": true
  },
  "sponsors": [
    {
      "id": 10123,
      "name": "SIGCHI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/447400f5-4f9d-bce4-036b-8f0fbda0ac78.png",
      "levelId": 10089
    },
    {
      "id": 10124,
      "name": "SIGGRAPH",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/4a0abe23-57b0-3631-4a1c-a06a44090b36.png",
      "levelId": 10089
    },
    {
      "id": 10125,
      "name": "Association for Computing Machinery",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/0360570b-b915-b446-2cda-82eba94a5267.png",
      "levelId": 10089
    },
    {
      "id": 10126,
      "name": "Carleton University",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/233c1b65-6598-f9a7-f8b5-000fec536587.png",
      "levelId": 10089
    },
    {
      "id": 10127,
      "name": "ToonRush Inc.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/480506aa-ac2d-5e15-f736-685729e73eb7.png",
      "levelId": 10089
    },
    {
      "id": 10128,
      "name": "VRST",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/0a4efc95-cfcb-cd60-3308-df411c15073b.png",
      "levelId": 10089
    }
  ],
  "sponsorLevels": [
    {
      "id": 10089,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 10412,
      "name": "Discord",
      "typeId": 11699,
      "setup": "Special"
    },
    {
      "id": 10413,
      "name": "Zoom",
      "typeId": 11696,
      "setup": "Special",
      "note": ""
    }
  ],
  "tracks": [
    {
      "id": 11110,
      "name": "VRST 2020 Posters and Demos",
      "typeId": 11699
    },
    {
      "id": 11109,
      "name": "VRST 2020 Technical Papers",
      "typeId": 11696
    }
  ],
  "contentTypes": [
    {
      "id": 11689,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 11690,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20
    },
    {
      "id": 11691,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90
    },
    {
      "id": 11692,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0
    },
    {
      "id": 11693,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90
    },
    {
      "id": 11694,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 11695,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90
    },
    {
      "id": 11698,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240
    },
    {
      "id": 11697,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 60
    },
    {
      "id": 11696,
      "name": "Paper",
      "color": "#08519c",
      "duration": 15
    },
    {
      "id": 11699,
      "name": "Poster/Demo",
      "color": "#969696",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 11636,
      "type": "SESSION",
      "startDate": 1604307600000,
      "endDate": 1604311200000
    },
    {
      "id": 11637,
      "type": "BREAK",
      "startDate": 1604311200000,
      "endDate": 1604313000000
    },
    {
      "id": 11638,
      "type": "SESSION",
      "startDate": 1604313000000,
      "endDate": 1604316600000
    },
    {
      "id": 11640,
      "type": "SESSION",
      "startDate": 1604318400000,
      "endDate": 1604323800000
    },
    {
      "id": 11641,
      "type": "SESSION",
      "startDate": 1604323800000,
      "endDate": 1604327400000
    },
    {
      "id": 11642,
      "type": "SESSION",
      "startDate": 1604329200000,
      "endDate": 1604332800000
    },
    {
      "id": 11639,
      "type": "BREAK",
      "startDate": 1604316600000,
      "endDate": 1604318400000
    },
    {
      "id": 11643,
      "type": "BREAK",
      "startDate": 1604327400000,
      "endDate": 1604329200000
    },
    {
      "id": 11644,
      "type": "SESSION",
      "startDate": 1604480400000,
      "endDate": 1604484000000
    },
    {
      "id": 11645,
      "type": "BREAK",
      "startDate": 1604484000000,
      "endDate": 1604485800000
    },
    {
      "id": 11646,
      "type": "SESSION",
      "startDate": 1604485800000,
      "endDate": 1604489400000
    },
    {
      "id": 11647,
      "type": "BREAK",
      "startDate": 1604489400000,
      "endDate": 1604491200000
    },
    {
      "id": 11648,
      "type": "SESSION",
      "startDate": 1604491200000,
      "endDate": 1604496600000
    },
    {
      "id": 11649,
      "type": "SESSION",
      "startDate": 1604496600000,
      "endDate": 1604500200000
    },
    {
      "id": 11650,
      "type": "SESSION",
      "startDate": 1604394000000,
      "endDate": 1604397600000
    },
    {
      "id": 11651,
      "type": "BREAK",
      "startDate": 1604397600000,
      "endDate": 1604399400000
    },
    {
      "id": 11652,
      "type": "SESSION",
      "startDate": 1604399400000,
      "endDate": 1604403000000
    },
    {
      "id": 11653,
      "type": "BREAK",
      "startDate": 1604403000000,
      "endDate": 1604404800000
    },
    {
      "id": 11654,
      "type": "SESSION",
      "startDate": 1604404800000,
      "endDate": 1604410200000
    },
    {
      "id": 11655,
      "type": "SESSION",
      "startDate": 1604410200000,
      "endDate": 1604413800000
    },
    {
      "id": 11656,
      "type": "BREAK",
      "startDate": 1604413800000,
      "endDate": 1604415600000
    },
    {
      "id": 11657,
      "type": "SESSION",
      "startDate": 1604415600000,
      "endDate": 1604419200000
    }
  ],
  "sessions": [
    {
      "id": 43551,
      "name": "Poster/Demo Session 1",
      "typeId": 11699,
      "roomId": 10412,
      "chairIds": [],
      "contentIds": [
        43703,
        43705,
        43695,
        43691,
        43683,
        43681,
        43679,
        43686,
        43685,
        43706,
        43716,
        43714,
        43713,
        43702
      ],
      "timeSlotId": 11640
    },
    {
      "id": 43552,
      "name": "Poster/Demo Session 2",
      "typeId": 11699,
      "roomId": 10412,
      "chairIds": [],
      "contentIds": [
        43701,
        43700,
        43699,
        43704,
        43687,
        43694,
        43690,
        43682,
        43678,
        43710,
        43709,
        43717,
        43692,
        43684,
        43680
      ],
      "timeSlotId": 11654
    },
    {
      "id": 43350,
      "name": "Haptics",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43533
      ],
      "contentIds": [
        43517,
        43508,
        43514,
        43515
      ],
      "timeSlotId": 11638
    },
    {
      "id": 43548,
      "name": "Displays & Perception",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43541
      ],
      "contentIds": [
        43507,
        43530,
        43526,
        43501
      ],
      "timeSlotId": 11646
    },
    {
      "id": 43553,
      "name": "Poster/Demo Session 3",
      "typeId": 11699,
      "roomId": 10412,
      "chairIds": [],
      "contentIds": [
        43712,
        43707,
        43689,
        43688,
        43696,
        43693,
        43698,
        43697,
        43711,
        43708,
        43718,
        43715,
        43720,
        43719
      ],
      "timeSlotId": 11648
    },
    {
      "id": 43351,
      "name": "Applications II",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43410
      ],
      "contentIds": [
        43506,
        43503,
        43504,
        43519
      ],
      "timeSlotId": 11650
    },
    {
      "id": 43547,
      "name": "Avatars",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43540
      ],
      "contentIds": [
        43512,
        43523,
        43511,
        43502
      ],
      "timeSlotId": 11644
    },
    {
      "id": 43550,
      "name": "Closing Keynote - Mary Whitton",
      "typeId": 11697,
      "roomId": 10413,
      "chairIds": [
        43532
      ],
      "contentIds": [],
      "timeSlotId": 11649
    },
    {
      "id": 43542,
      "name": "Applications I",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43534
      ],
      "contentIds": [
        43518,
        43522,
        43505,
        43527
      ],
      "timeSlotId": 11641
    },
    {
      "id": 43543,
      "name": "Interaction Techniques I",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43535
      ],
      "contentIds": [
        43513,
        43497,
        43509,
        43525
      ],
      "timeSlotId": 11642
    },
    {
      "id": 43531,
      "name": "Interaction Techniques II",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43383
      ],
      "contentIds": [
        43529,
        43498,
        43521,
        43524
      ],
      "timeSlotId": 11652
    },
    {
      "id": 43545,
      "name": "3D Reconstruction & Video",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43538
      ],
      "contentIds": [
        43499,
        43516,
        43528,
        43520
      ],
      "timeSlotId": 11655
    },
    {
      "id": 43546,
      "name": "Collaboration",
      "typeId": 11696,
      "roomId": 10413,
      "chairIds": [
        43539
      ],
      "contentIds": [
        43496,
        43510,
        43500
      ],
      "timeSlotId": 11657
    },
    {
      "id": 43549,
      "name": "Opening Keynote - Doug Bowman",
      "typeId": 11697,
      "roomId": 10413,
      "chairIds": [
        43532
      ],
      "contentIds": [],
      "timeSlotId": 11636
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 43520,
      "typeId": 11696,
      "title": "Effects of Immersive Virtual Reality Content Type to Mindfulness and Physiological Parameters",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual reality (VR) has been applied as a complimentary way to conventional treatment for mental disorders successfully. On the other hand, it has not been clearly shown what kind of immersive media such as VR can directly affect one’s physiological parameters, associated with the state of the mindfulness. We sought to assess how being subjected to differently designed VR contents can affect and modulate one’s anxiety both psychologically and more importantly physiologically. We empirically tested the comparative effects of two polarizing VR content types to this effect: (1) “calm/soothing” content and (2) “disturbing”. Twenty-five adults participated and their mental state, anxiety level and physiological signals were measured before and after experiencing the respective VR content type. The experiment found a statistically significant effect of the content type to the changes in these measures and confirmed that the “calm” content was helpful for one to self-regulate to lower heart rate blood pressure, stable GSR, and the “disturbing” content in the opposite way. We applied this result to calm down and stabilize vital signs of patients during actual coronary angiography and catheterization operations. We were able to observe the same effect with positive comments from the patients and operating team.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 43358
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 43471
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Not Applicable",
              "city": "Incheon",
              "institution": "Gil Medical Center, Gachon University College of Medicine",
              "dsl": "Cardiology Div. Dept. of Internal Medicine"
            }
          ],
          "personId": 43367
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "N/A",
              "city": "Daejon",
              "institution": "Chungnam National University",
              "dsl": "Dept.  of Psychiartry"
            }
          ],
          "personId": 43398
        }
      ]
    },
    {
      "id": 43521,
      "typeId": 11696,
      "title": "Eye-Hand Coordination Training for Sports with Mid-air VR",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "A relatively recent application area for Virtual Reality (VR) systems is sports training and user performance assessment. One of these applications is eye-hand coordination training systems (EHCTSs). Previous research on EHCTSs identified that VR-based training systems have great potential for EHCT and subject performance can even increase with 3D mid-air VR setups. Here, we investigate 3D mid-air interaction in VR-based EHCTSs further. We conducted two user studies to investigate how user performance is affected by different target arrangements, feedback conditions, and handedness in VR-based EHCTSs. In the first study, we explored handedness and vertical and horizontal target arrangements and showed that user performance is increased with the dominant hand and a vertical target plane. In the second study, we investigated different combinations of visual and haptic feedback and how they affect user performance with different target and cursor sizes. Results illustrate that haptic feedback did not increase user performance when it is added to visual feedback. Our results inform the creation of better EHCTSs with mid-air VR systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University ",
              "dsl": "School of Interactive Arts and Technology "
            }
          ],
          "personId": 43403
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "School of Interactive Arts and Technology",
              "dsl": "Simon Fraser University"
            }
          ],
          "personId": 43391
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Stambol Studios Inc.",
              "dsl": ""
            }
          ],
          "personId": 43369
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 43458
        }
      ]
    },
    {
      "id": 43522,
      "typeId": 11696,
      "title": "Investigating Immersive Virtual Reality as an Educational Tool for Quantum Computing",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Quantum computing (QC) is an intrinsically complex yet exciting discipline with increasing practical relevance. A deep understanding of QC requires the integration of knowledge across numerous technical fields, such as physics, computing and mathematics. This work aims to investigate how immersive Virtual Reality (VR) compares to a desktop environment (`web-applet') as an educational tool to help teach individuals QC fundamentals. We developed two interactive learning tutorials, one utilising the `Bloch sphere' visualisation to represent a single-qubit system, and the other exploring multi-qubit systems through the lens of `quantum entanglement'. We evaluate the effectiveness of each medium to teach QC fundamentals in a user study with 24 participants. We find that the Bloch sphere visualisation was well-suited to VR over a desktop environment. Our results also indicate that mathematics literacy is an important factor in facilitating greater learning with this effect being notably more pronounced when using VR. However, VR did not significantly improve learning in the multi-qubit context over the desktop equivalent. Our work provides valuable insights which contribute to the emerging field of Quantum HCI (QHCI), VR for education, as well as educational research more broadly. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Parkville",
              "institution": "The University of Melbourne",
              "dsl": "Computing and Information Systems"
            }
          ],
          "personId": 43479
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Parkville",
              "institution": "The University of Melbourne",
              "dsl": ""
            }
          ],
          "personId": 43435
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "Interaction Design Lab, School of Computing and Information Systems"
            }
          ],
          "personId": 43380
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "The University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 43475
        }
      ]
    },
    {
      "id": 43523,
      "typeId": 11696,
      "title": "Realistic Virtual Humans from Smartphone Videos",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "This paper introduces an automated 3D-reconstruction method for generating high-quality virtual humans from monocular smartphone cameras. The input of our approach are two video clips, one capturing the whole body and the other providing detailed close-ups of head and face. Optical flow analysis and sharpness estimation select individual frames, from which two dense point clouds for the body and head are computed using multi-view reconstruction. Automatically detected landmarks guide the fitting of a virtual human body template to these point clouds, thereby reconstructing the geometry. A graph-cut stitching approach reconstructs a detailed texture. Our results are compared to existing low-cost monocular approaches as well as to expensive multi-camera scan rigs. We achieve visually convincing reconstructions that are almost on par with complex camera rigs while surpassing similar low-cost approaches. The generated high-quality avatars are ready to be processed, animated, and rendered by standard XR simulation and game engines such as Unreal or Unity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 43408
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 43402
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            }
          ],
          "personId": 43444
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 43377
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 43371
        }
      ]
    },
    {
      "id": 43524,
      "typeId": 11696,
      "title": "Empirical Evaluation of Multimodal Gaze-enhanced Menus in Virtual Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "In VR  applications, users often shift their attention between  primary and secondary tasks, e.g., when changing drawing colour in a handheld menu.  In this work, we investigate how eye gaze allows to exploit these attention shifts for interaction design in VR. We investigate three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different balance between gaze and manual input. We present a user study that compares the techniques against two baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection. Furthermore, we provide an analysis of the trade-off involved between eye and manual fatigue. Our research points to new opportunities for integrating gaze input into bimanual interaction in 3D environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            }
          ],
          "personId": 43409
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 43447
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 43379
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Usable Security and Privacy Group",
              "dsl": "Bundeswehr University of Munich"
            }
          ],
          "personId": 43467
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 43460
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Bundeswehr University Munich",
              "dsl": ""
            }
          ],
          "personId": 43373
        }
      ]
    },
    {
      "id": 43525,
      "typeId": 11696,
      "title": "Effects of Synchronized Leg Motion in Walk-in-Place Utilizing Deep Neural Networks for Enhanced Body Ownership and Sense of Presence in VR",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "We investigate the effects of different ways of visualizing the virtual gait of the avatar in the context of Walk-in-Place (WIP) based navigation in a virtual environment (VE). In Study 1, participants navigated through a VE using the WIP method while inhabiting an avatar. We varied the visualization of the avatar's leg motion while performing the WIP gesture: (1) Fixed Body: the legs stood still; (2) Pre-recorded Animation: the legs moved in a fixed predetermined pace (plausible but not in accordance to that of the user in general); (3) Synchronized Motion the legs moved according (synchronized) to those of the user. Our results indicated that the sense of presence and body ownership improved significantly when the leg motion was rendered synchronized to that of the user (Synchronized Motion). In addition, we developed a deep neural network (DNN) that predicted the users' leg postures only with the head position tracking, eliminating the need for any external sensors. We carried out Study 2, to assess the effects of different gait visualizations, under two new factors: (1) virtual gait seen directly by the user looking down, or already visible by one's shadow (i.e., no need to look down); and (2) playing a pre-recorded animation, or pre-recorded animation whose playback speed was adjusted to match with pace of the users' actual leg motions as predicted by the DNN. The results of Study 2 showed that the virtual gait temporally synchronized with that of the user greatly improved the sense of body ownership, whether it was witnessed directly or indirectly with the shadow. However, the effect of virtual gait on presence was less marked when indirectly observed. We discuss our findings and the implications for representing the avatar locomotion in immersive virtual environments. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Imaging Media Research"
            }
          ],
          "personId": 43428
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute for Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 43381
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 43471
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Imaging Media Research"
            }
          ],
          "personId": 43376
        }
      ]
    },
    {
      "id": 43526,
      "typeId": 11696,
      "title": "Bacterial Load of Virtual Reality Headsets",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "As commodity virtual reality (VR) systems become more common, they are rapidly gaining popularity for entertainment, education, and training purposes. VR utilizes headsets which come in contact with or close proximity to the user's eyes, nose, and forehead. In this study, the potential for these headsets to become contaminated with bacteria was analyzed. This study began roughly one year prior to the outbreak of the COVID-19 pandemic in the United States.  We feel it is important to be clear that this study focuses exclusively on bacteria, as opposed to viruses like those responsible for the present pandemic.\r\n\r\nThe nosepieces and foreheads of two HTC Vive headsets were sampled over the course of a seven-week period in a VR software development course. Serial dilutions were performed, and samples were plated on various culture media. Following incubation, counts of bacteria were determined. DNA was extracted from bacterial colonies and the 16S rRNA gene was sequenced to identify bacterial contaminates present on the headsets. Chief among these contaminates was \\textit{Staphylococcus aureus}.  The results of these tests indicated that the \\textit{Staphylococcus aureus} strains isolated from the headsets possessed high levels of antibiotic resistance. Other notable bacterial isolates included \\textit{Moraxella osloensis}, the bacteria responsible for foul odors in laundry and \\textit{Micrococcus luteus}, a communalistic bacterial species capable of causing opportunistic infections. Other bacterial isolates were detected in variable amounts throughout the trial.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "New Orleans",
              "institution": "Louisiana State University",
              "dsl": "Health Sciences Center"
            }
          ],
          "personId": 43492
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "University",
              "institution": "University of Mississippi",
              "dsl": "Department of Biology"
            }
          ],
          "personId": 43361
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "University",
              "institution": "University of Mississippi",
              "dsl": "Hi5 Lab, Dept. of Computer & Information Science, "
            }
          ],
          "personId": 43410
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "University",
              "institution": "University of Mississippi",
              "dsl": "Department of Biology"
            }
          ],
          "personId": 43396
        }
      ]
    },
    {
      "id": 43527,
      "typeId": 11696,
      "title": "Virtual Reality in Computer Science Education: A Systematic Review\t",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Over the last decades, virtual reality technologies have become more affordable and accessible. This opens up new ways in the field of digital learning. Virtual reality (VR) can offer new forms of interactive learning and working, especially for subjects from the STEM area. Therefore, in this paper we investigate the potential and application of VR for computer science education with a systematic review. As a result, we present a formal literature review on the use of virtual reality technologies in computer science education. We focus on the identification of factors such as learning objectives, technologies used, interaction characteristics, and challenges and advantages of using fully immersive VR for computer science education.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "TU Graz",
              "dsl": ""
            }
          ],
          "personId": 43415
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": ""
            }
          ],
          "personId": 43488
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "TU Graz",
              "dsl": ""
            }
          ],
          "personId": 43474
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Institute of Interactive Systems and Data Science",
              "dsl": "Graz University of Technology"
            }
          ],
          "personId": 43419
        }
      ]
    },
    {
      "id": 43528,
      "typeId": 11696,
      "title": "Investigating Effects and User Preferences of Extra- and Intradiegetic Virtual Reality Questionnaires",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual realities (VR) are becoming an integral part of product development across many industries, for example to assess aesthetics and usability of new features in the automotive industry. The recording of the evaluation is typically conducted by filling out questionnaires after the study participants left the virtual environment.\r\nIn this paper, we investigate how questionnaires could be best embedded within the virtual environment. Apart from design choices, we also compare how VR-questionnaires differ from classical post-test evaluations regarding presence, questionnaire completion time, usability and preference.\r\n  \r\nIn a first study (\\emph{N}=11), experts rated four design concepts of questionnaires embedded in VR, of which two were designed as \\emph{extradiegetic} and two as \\emph{intradiegetic} user interfaces. We show that intradiegetic UIs have a significantly higher perceived user experience and presence while the usability remains the same. Intradiegetic UIs are preferred by the majority.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 43422
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wolfsburg",
              "institution": "Volkswagen AG",
              "dsl": ""
            }
          ],
          "personId": 43354
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 43386
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Sachsen-Anhalt",
              "city": "Koethen",
              "institution": "Hochschule Anhalt, University of Applied Sciences",
              "dsl": "Ingenieurinformatik"
            }
          ],
          "personId": 43359
        }
      ]
    },
    {
      "id": 43529,
      "typeId": 11696,
      "title": "Effect of Cognitive Load on Curvature Redirected Walking Thresholds",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "In order to allow users to perform real walking in a virtual environment larger than the physical space, redirected walking (RDW) techniques could be employed. Users do not notice this manipulation and immersion remains intact when RDW is applied within certain thresholds. Although there are many existing studies on RDW detection thresholds and factors that influence them, in none of these studies, users were performing an additional task during the threshold identification process. These existing thresholds could be only conservative estimates and the potential of RDW may not be fully utilized. \r\nIn this paper, we present an experiment to investigate the effect of cognitive load on curvature RDW thresholds. The cognitive load was imposed using a dual task of serial seven subtraction. Results showed that gender and cognitive load have significant effects on curvature RDW thresholds. More specifically, men are on average more sensitive to RDW than women, and being engaged in a dual task increases users’ RDW thresholds.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "Lucerne School of Information Technology"
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Innovation Center Virtual Reality"
            }
          ],
          "personId": 43430
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "University of Zurich",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 43362
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "University Hospital Zurich",
              "dsl": "Department of Neurology"
            }
          ],
          "personId": 43462
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "University of Zurich",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 43400
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Valens",
              "institution": "Valens Rehabilitation Center",
              "dsl": ""
            }
          ],
          "personId": 43451
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "University Hospital Zurich",
              "dsl": "Department of Neurology"
            }
          ],
          "personId": 43425
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "ICVR"
            }
          ],
          "personId": 43441
        }
      ]
    },
    {
      "id": 43530,
      "typeId": 11696,
      "title": "Multiplex Vision: Understanding Information Transfer and F-Formation With Extended 2-Way FOV",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Research in sociology shows that effective conversation relates to people’s spatial and orientational relationship, namely the proxemics (distance, eye contact, synchrony) and the F-formation (orientation and arrangement). In this work, we introduce novel conversational paradigms that effects conventional F-formation by introducing the concept of multi-directional conversation. Multiplex Vision is a head-mounted device capable of providing a 360° field-of-view (FOV) and facilitating multi-user interaction multi-directionally, thereby providing novel methods on how people can interact with each other. We propose 3 possible new forms of interactions from our prototype: one-to-one, one-to-many, and many-to-many. To facilitate them, we manipulate 2 key variables, which are the viewing parameter and the display parameter. To gather feedback for our system, we conducted a pilot study on understanding information transfer between various modes as well as a user study on how different proposed paradigms effect conversation. Finally, we discuss present and future use case scenarios that can benefit from our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio Media Design",
              "dsl": ""
            }
          ],
          "personId": 43423
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "4-1-1,Hiyoshi",
              "dsl": "Keio University Graduate School of Media Design"
            }
          ],
          "personId": 43427
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Ignition Point, Inc.",
              "dsl": "INNOCC"
            }
          ],
          "personId": 43456
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 43399
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Auckland Bioengineering Institute, University of Auckland",
              "dsl": "Empathic Computing Laboratory"
            }
          ],
          "personId": 43385
        }
      ]
    },
    {
      "id": 43678,
      "typeId": 11699,
      "title": "An Open Framework for Infinite Walking Using Saccadic Redirection",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "In this project we created an expandable framework for allowing infinite walking in virtual reality in a closed play area by measuring saccades and redirect the user to stay within the area. A saccade is a rapid eye movement with a unique property: the eye temporarily gathers reduced information -- saccadic suppression. We leverage the suppression to redirect the user’s walking towards the center of the play area by rotating the virtual world around the camera’s location. Modern VR hardware such as the Vive Eye Pro allows a reasonable sample rate of eye movement measurements. A self-developed VR testing environment was used and with corresponding pre and post experience questions we tested a group of participants regarding general motion and VR- sickness parameters. We found a certain angle for the maximum saccade rotation which was base of further testing.\r\nWe found, that our framework and the default settings successfully allow saccadic redirection with only marginal discomfort for the users.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "Cal Poly San Luis Obispo",
              "dsl": "Department of Computer Sience"
            }
          ],
          "personId": 43631
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "American University",
              "dsl": "IDEAS Lab"
            }
          ],
          "personId": 43585
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "%null%",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 43667
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "California Polytechnic State University San Luis Obispo",
              "dsl": "Computer Science"
            }
          ],
          "personId": 43577
        }
      ]
    },
    {
      "id": 43679,
      "typeId": 11699,
      "title": "HexTouch: Affective Robot Touch for Complementary Interactions to Companion Agents in Virtual Reality",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "There is a growing need for social interaction in Virtual Reality (VR). Current social VR applications enable human-agent or interpersonal communication, usually by means of visual and audio cues. Touch, which is also an essential method for affective communication, has not received as much attention. To address this, we introduce HexTouch, a forearm-mounted robot that performs touch behaviors in sync with the behaviors of a companion agent, to complement visual and auditory feedback in virtual reality. The robot consists of four robotic tactors driven by servo motors, which render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy). We demonstrate HexTouch through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase the immersion of the VR experience. The player will receive affective haptic cues while collaborating with the agent to complete the mission in the game. The multisensory system for affective communication also has the potential to enhance sociality in the virtual world.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Parsons School of Design",
              "dsl": "Parsons School of Design"
            }
          ],
          "personId": 43594
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Parsons School of Design",
              "dsl": ""
            }
          ],
          "personId": 43612
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Parsons School of Design",
              "dsl": ""
            }
          ],
          "personId": 43595
        }
      ]
    },
    {
      "id": 43680,
      "typeId": 11699,
      "title": "Viewing Style of Augmented Reality/Virtual Reality Broadcast Contents while Sharing a Virtual Experience",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "A conceptual space-sharing broadcasting service has been proposed, using augmented reality/virtual reality (AR/VR) with a head-mounted display. As proposed, virtual performers are displayed in their real-life sizes; the user experiences proximity to them. Family and friends living apart can also be displayed in this manner, and an individual can communicate with them in real time; this enables both the individual and their peer to enjoy the broadcast media together. In this study, we implemented this concept as a suitable style for daily use and confirmed the effect of the viewing experience. We developed a prototype of an environment for watching AR/VR mixed content along with a person in a distant place, which is expected to become a popular future broadcast media segment. The individual is displayed as a live-action 3D point cloud image, and verbal and nonverbal communication with the individual are enabled. A demonstration experiment showed that the system renders a sense of presence to a distant person and provides the feeling of sharing the same experience among all its users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Science & Technology Research Laboratories, NHK",
              "dsl": ""
            }
          ],
          "personId": 43603
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kinuta, Setagaya-ku, Tokyo",
              "institution": "Japan Broadcasting Corporation",
              "dsl": "Spatial Imaging Research Division, Science & Technology Research Laboratories"
            }
          ],
          "personId": 43675
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Science & Technology Research Laboratories, NHK",
              "dsl": ""
            }
          ],
          "personId": 43563
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Science & Technology Research Laboratories",
              "dsl": "Planning & Coordination Devision"
            }
          ],
          "personId": 43619
        }
      ]
    },
    {
      "id": 43681,
      "typeId": 11699,
      "title": "Computing Object Selection Difficulty in VR using Run-time Contextual Analysis",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "This paper introduces a method for computing the difficulty of selection tasks in virtual environments using pointing metaphors by operationalizing an established human motor behavior model. In contrast to previous work, the difficulty is calculated automatically at run-time for arbitrary environments. We present and provide the implementation of our method within Unity 3D. The difficulty is computed based on a contextual analysis of spatial boundary conditions, i.e., target object size and shape, distance to the user, and occlusion. We believe our method will enable developers to build adaptive systems that automatically equip the user with the most appropriate selection technique according to the context. Further, it provides a standard metric to better evaluate and compare different selection techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg, Department of Computer Science, HCI Group",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg, Department of Computer Science, HCI Group",
              "dsl": ""
            }
          ],
          "personId": 43661
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 43607
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            }
          ],
          "personId": 43600
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 43635
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 43377
        }
      ]
    },
    {
      "id": 43682,
      "typeId": 11699,
      "title": "ARtist: Interactive Augmented Reality for Curating Children’s Artworks",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "ARtist is a mobile app that allows children to curate, display, and document their artworks through mobile augmented reality technology. This application aims to improve the traditional art display environment with augmented reality technology which enables users to utilize virtual space freely with interactive assets. These user interfaces allow users to upload their photo and decorate with provided 3d assets. Then they project their artworks with augmented reality in the real-world environment. ARtist is designed for children through a user-centered design process and developed with Unity and Google ARcore.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": ""
            }
          ],
          "personId": 43664
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Department of Visualization/Soft Interaction Lab"
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Department of Visualization/Soft Interaction Lab"
            }
          ],
          "personId": 43586
        }
      ]
    },
    {
      "id": 43683,
      "typeId": 11699,
      "title": "An Immersive Decision Support System for Disaster Response",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Huge amounts of spatial data are being generated at an increasingly accelerating rate due to the advancements in remote sensing and availability of internet-enabled sensors. The complexity and variance of available parameters present challenges for informed decision-making in relevant geolocational context in the real-world, especially for environmental management and disaster response. As a solution, immersive visualizations and 3D modelling show a great potential to constitute the next-generation of map-based information systems where real-world locations can be recreated with in-situ spatiotemporal data displays and simulated event dynamics. This project introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be served on any web platform and accessed via desktop and mobile devices as well as virtual and augmented reality headsets. The framework is capable of generating realistic simulations of desired locations entailing the terrain, elevation model, structures (e.g. buildings, roads, bridges), dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, surveillance data, occupancy, traffic, weather). These layers serve the purpose of effective and in-situ visualization of useful data to aid public, scientists, officials, and decision-makers in acquiring a bird’s eye view of the current, historical, or forecasted condition of a community. The framework incorporates multiuser support to allow different stakeholders to remotely work on the same VR environment and observe other user’s actions and 3D positions via avatars in real-time, and thus, presenting the potential to be utilized as a virtual incident command center or a meeting room. The purpose of GeospatialVR is to augment existing web-based cyberinfrastructures that have geospatial components to constitute the next-generation of information systems and decision support systems powered by immersive technologies. To demonstrate the framework’s usage and benefits, case studies have been developed for flooding, wildfire, transportation, and active shooter response.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Iowa",
              "city": "Iowa City",
              "institution": "The University of Iowa",
              "dsl": "IIHR - Hydroscience & Engineering"
            }
          ],
          "personId": 43654
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Iowa",
              "city": "Iowa City",
              "institution": "The University of Iowa",
              "dsl": "Civil and Environmental Engineering"
            }
          ],
          "personId": 43642
        }
      ]
    },
    {
      "id": 43684,
      "typeId": 11699,
      "title": "Inconsistencies of Presence Questionnaires in Virtual Reality",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Presence in virtual reality (VR) is typically assessed through questionnaires in the real world and after leaving an immersive experience. Previous research suggests that questionnaires in VR reduce biases caused by the real-world setup. However, it remains unclear whether presence questionnaires still provide valid results when subjects are being surveyed while presence is perceived. In a user study with 36 participants, two standardized presence questionnaires (IPQ, SUSa) were either completed in the real lab, in a virtual lab scene, or in the actual scene after a virtual gaming experience. Our results show inconsistencies between the measurements and that main scores, as well as subscales of the presence measures are significantly affected by the subjects' environment. As presence questionnaires have been designed to be answered after an immersive experience, we recommend revising those tools for measuring presence in VR.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43370
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 43372
        }
      ]
    },
    {
      "id": 43685,
      "typeId": 11699,
      "title": "The Digital Docent: XR storytelling for a Living History Museum",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "In this work, we describe the use of a digital docent: a  3D avatar, presented using virtual and augmented reality, as a means for providing interactive storytelling experiences at a living history museum.   To allow flexibility depending on the user’s location and access to technology, the app is designed to provide a common experience supporting a variety of different delivery modalities including AR devices, mobile AR, and VR on the Web.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43579
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43662
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 43606
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute or Technology",
              "dsl": "School of Design"
            }
          ],
          "personId": 43572
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute or Technology",
              "dsl": "School of Design"
            }
          ],
          "personId": 43623
        }
      ]
    },
    {
      "id": 43686,
      "typeId": 11699,
      "title": "VR Training for Warehouse Management",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual reality (VR) has evolved into a trending technology that has proven its worth in various application domains (e.g., education, medicine, manufacturing, etc.). VR is especially helpful for the training of complex tasks in special environments. In the area of logistics, warehouse management involves a complex workflow that consists of different order picking activities. Due to this complex workflow, stock discrepancies and misplaced wares are typical problems that often occur. To overcome this problem, we have developed a VR training application that integrates an existing warehouse management system and trains typical order picking processes. In our VR training demo, we simulate a real warehouse that is supplied with real stocks and real orders. The benefit of our solution is presented based on a usability study that has been conducted with our industrial partner. The usability evaluation results show that our system supports the training of new and experienced warehouse workers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": ""
            }
          ],
          "personId": 43564
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": ""
            }
          ],
          "personId": 43677
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": ""
            }
          ],
          "personId": 43634
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": ""
            }
          ],
          "personId": 43591
        }
      ]
    },
    {
      "id": 43687,
      "typeId": 11699,
      "title": "Teaching Scrum with a Virtual Sprint Simulation: Initial Design and Considerations",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Scrum is a well-developed and utilized agile project management framework, which requires extensive training and hands-on experience to master. The latter is not always possible, i.e. during the recent lockdown due to COVID-19. Thus, we propose the creation of virtual simulation of a Scrum sprint that can provide an immersive training experience to remote trainees. Herein, we discuss the design considerations, elements of the virtual learning environment and the development process for the platform.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "Central Jutland",
              "city": "Herning",
              "institution": "Aarhus University",
              "dsl": "Extended Reality and Robotics Lab"
            }
          ],
          "personId": 43583
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "AU",
              "dsl": "BTECH"
            }
          ],
          "personId": 43602
        }
      ]
    },
    {
      "id": 43688,
      "typeId": 11699,
      "title": "Evaluation of Headset-based Viewing and Desktop-based Viewing of Remote Lectures in a Social VR Platform",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "We study experiences of students attending classes remotely from home using a social VR platform, considering both desktop-based and headset-based viewing of remote lectures. Ratings varied widely. Headset viewing produced higher presence overall. Strong negative correlations between headset simulator sickness symptoms and overall experience ratings, and some other ratings, suggest that the headset experience was much better for comfortable users than for others. Reduced sickness symptoms, and no similar correlations, were found for desktop viewing. Desktop viewing appears to be a good alternative for students not comfortable with headsets. Future VR systems are expected to provide more stable and comfortable visuals, providing benefits to more users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "CACS VR Lab"
            }
          ],
          "personId": 43605
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "Center for Advanced Computer Studies"
            }
          ],
          "personId": 43557
        }
      ]
    },
    {
      "id": 43689,
      "typeId": 11699,
      "title": "Portals With a Twist: Cable Twist-Free Natural Walking in Room-Scaled Virtual Reality",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "To provide naturally walking in small virtual reality (VR) tracking spaces while preventing cables of head-mounted displays (HMDs) getting twisted, we developed Portals With A Twist (PorTwist), a redirected walking method using a portal metaphor. We compared PorTwist with a teleportation method in a 2m x 2.7m tracking space in user study (N = 34). PorTwist can prevent HMD cables from getting twisted and provides comparable levels of perceived presence and simulator sickness as teleportation. We further identified potential to improve usability in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 43592
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 43575
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 43658
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 43665
        }
      ]
    },
    {
      "id": 43690,
      "typeId": 11699,
      "title": "Smart Thermo-Haptic Bracelet for VR Environment",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a lightweight smart haptic bracelet-based stimulation system for VR applications. This wireless system equipped with vibrotactile tactors and peltier actuator, that generates different haptic, thermal patterns and levels for different materials being touched in a VR environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Algeria",
              "state": "",
              "city": "Boumerdes",
              "institution": "Institute of Electrical & Electronic Engineering (IGEE ex INELEC)",
              "dsl": ""
            }
          ],
          "personId": 43643
        },
        {
          "affiliations": [
            {
              "country": "Algeria",
              "state": "",
              "city": "Algiers",
              "institution": "CDTA",
              "dsl": ""
            }
          ],
          "personId": 43659
        },
        {
          "affiliations": [
            {
              "country": "Algeria",
              "state": "",
              "city": "Algiers",
              "institution": "CDTA",
              "dsl": ""
            }
          ],
          "personId": 43614
        }
      ]
    },
    {
      "id": 43691,
      "typeId": 11699,
      "title": "Molecular MR Multiplayer: a cross-platform collaborative interactive game for scientists",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Collaborative mixed reality is a new step over Virtual Reality solutions for the human-machine and human-human interactions. Increasing human performance through technology is a recurring theme of science and culture.\r\nWe present {\\textbf{Molecular MR Multiplayer}} -- a cross-platform collaborative interactive application for material design. Our application transforms a research process into engaging gaming experience on Mixed Reality headsets, phones, and computers (Fig.~\\ref{fig:teaser}, panels 1--3). It shows a concept of an emerging multiplayer holographic experience for scientists, aiming to help discover novel chemical compounds, preview, manipulate, and analyze them. For a material scientist, it is essential to visualize compounds of interest and understand how they interact with each other.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ukraine",
              "state": "-Select-",
              "city": "Lviv",
              "institution": "SoftServe Inc.",
              "dsl": "Research and Development"
            }
          ],
          "personId": 43618
        },
        {
          "affiliations": [
            {
              "country": "Ukraine",
              "state": "",
              "city": "Lviv",
              "institution": "SoftServe Inc.",
              "dsl": "Research and Development"
            },
            {
              "country": "Ukraine",
              "state": "-Select-",
              "city": "Lviv",
              "institution": "Institute for Condensed Matter Physics, NAS Ukraine",
              "dsl": ""
            }
          ],
          "personId": 43566
        },
        {
          "affiliations": [
            {
              "country": "Ukraine",
              "state": "-Select-",
              "city": "Lviv",
              "institution": "SoftServe Inc.",
              "dsl": "Research and Development"
            }
          ],
          "personId": 43668
        }
      ]
    },
    {
      "id": 43692,
      "typeId": 11699,
      "title": "An Adaptive Approach to Display Different Scenes to Multi-users with Low GPU Consumption for the Light Field Display",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "  As a promising alternative to VR head-mounted display, current light field displays require high GPU consumption for multiple-view rendering and do not display different scenes for multiple viewers with motion parallax. On the basis of a prior work that saves its GPU burden by only rendering visible views, we propose an improved approach to only render the visible views of different scenes towards multiple viewers' eyes. Moreover, we found that the number of visible views decreases as the viewing distances increase. Hereupon, our approach dynamically adjusts the number of rendered views according to the viewing distances. This approach can be easily adapt to the off-the-shelf light field displays to display different 3D scenes for at least two viewers according to their head positions with reduced GPU costs.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": "School of computer science"
            }
          ],
          "personId": 43576
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 43555
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Technology",
              "dsl": "Beijing Dublin International College"
            }
          ],
          "personId": 43652
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Dublin",
              "city": "Dublin",
              "institution": "University College Dublin ",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 43673
        }
      ]
    },
    {
      "id": 43693,
      "typeId": 11699,
      "title": "Mediated-Timescale Learning: Manipulating Timescales in Virtual Reality to Improve Real-World Tennis Forehand Volley",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "In tennis training, beginner players often fail to return the ball when the ball moves faster than what they can react to. In this paper, we propose a new training process of Mediated-Timescale Learning (MTL) to manipulate the incoming ball's motions. The ball firstly moves in slow motions, allowing more time for beginner players to react and develop skills. Along with improvements in players' skills, the ball moves in faster motions to provide a further challenge. To evaluate MTL, we implemented it in a virtual reality (VR)-oriented tennis training system. We piloted the MTL implementations (N = 12) to decide the most enjoyable one. We then conducted a preliminary user study (N = 8) to validate MTL's training effects in the real world. We found that while comparing to real-world training, five participants presented more improvements with MTL.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 43651
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 43589
        }
      ]
    },
    {
      "id": 43694,
      "typeId": 11699,
      "title": "ATOM: HMD-VR Interface to Learn Atomic Structure, Bonding and Historical Research Experiments",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual Reality (VR), especially supported through Head Mounted Displays (HMDs) have shown promising results in training and education. This is mainly due to its ability to vary the content size, transcend the limitation of time and physics, visualise the unseen and its ability to communicate difficult concepts with ease. The reducing cost of 6-DoF VR devices and easy availability of inexpensive 3-DoF VR platform further amplifies the use of VR in training and education. In this paper, we present ATOM - an HMD VR interface to teach students about atomic structure and historical research experiments conducted in understanding the structure of atoms. ATOM is designed to complement the classroom learning for students of grade 9. Preliminary evaluation showed positive results across dimensions like – effectiveness, user acceptance, adoption and increase in learnability compared to traditional digital platforms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Assam",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology Guwahati",
              "dsl": "Embedded Interaction Lab, Department of Design"
            }
          ],
          "personId": 43560
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology Guwahati",
              "dsl": "Department of Design"
            }
          ],
          "personId": 43561
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology ",
              "dsl": "Department of Design"
            }
          ],
          "personId": 43669
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Assam",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology (IIT) Guwahati",
              "dsl": "Department of Design"
            }
          ],
          "personId": 43653
        }
      ]
    },
    {
      "id": 43695,
      "typeId": 11699,
      "title": "Creating AR Applications for the IOT : a New Pipeline",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Prototyping augmented reality applications for smart environments is still a difficult task. \r\n  We then propose a pipeline to help designers and developers to create Augmented Reality (AR) applications for monitoring and controlling indoor environments equipped with connected objects. This pipeline starts with the capture (geometry and objects) of the real environment with an AR device. Then, it proposes a Virtual Reality (VR) tool to configure augmentations in this captured environment. This VR tool includes a feature to simulate AR devices to help anticipate the application's rendering on real devices. The created application can then be seamlessly deployed on various AR devices including smartphones, tablets and glasses.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Orange Labs",
              "dsl": ""
            }
          ],
          "personId": 43666
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Orange Labs",
              "dsl": ""
            }
          ],
          "personId": 43559
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Orange Labs",
              "dsl": ""
            }
          ],
          "personId": 43590
        }
      ]
    },
    {
      "id": 43696,
      "typeId": 11699,
      "title": "CasualVRVideos: VR videos from casual stationary videos",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Thanks to the ubiquity of devices capable of recording and playing back video, the amount of video files is growing at a rapid rate. Most of us have now video recordings of major events in our lives. However, until today, these videos are captured mainly in 2D and are mostly used for screen-based video replay. Currently there is no way for watching them in more immersive environments such as on a VR headset. They are simply not optimized for playback in stereoscopic displays or even tracked Virtual Reality devices. \r\n\r\nIn this work, we present CasualVRVideos, a first approach that works towards solving these issues by extracting spatial information from video footage recorded in 2D, so that it can later be played back in VR displays to increase the immersion. We focus in particular on the challenging scenario when the camera itself is not moving.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 43630
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 43611
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "California Polytechnic State University",
              "dsl": "Department of Computer Science and Software Engineering"
            }
          ],
          "personId": 43609
        }
      ]
    },
    {
      "id": 43697,
      "typeId": 11699,
      "title": "Text Input  Methods in Virtual Reality using Radial Layouts",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Currently, the most popular text input method in VR is Controller Pointing (CP). While this method is easy and intuitive to use, it requires users to have steady hands, and the overlaying of the keyboard onto the virtual scene occludes a part of the scene. In this\r\nwork, we proposed two new text input methods: Sector Input and T9VR, that utilize a circular keyboard that is attached to the HTC Vive controller. A preliminary study with 24 subjects was conducted to explore the potential of the proposed methods, in comparison\r\nwith CP. While CP performed significantly better than the proposed methods in terms of objective typing speed and error rate, T9VR was able to match with CP in a number of subjective measures.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "Lucerne School of Information Technology"
            }
          ],
          "personId": 43430
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "Lucerne School of Information Technology"
            }
          ],
          "personId": 43636
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "Lucerne School of Information Technology"
            }
          ],
          "personId": 43565
        }
      ]
    },
    {
      "id": 43698,
      "typeId": 11699,
      "title": "HMDPose: A large-scale trinocular IR Augmented Reality Glasses Pose Dataset",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented Reality Glasses usually implement an Inside-Out tracking.\r\nIn case of a driving scenario or glasses with less computation\r\ncapabilities, an Outside-In tracking approach is required. However,\r\nto the best of our knowledge, no public datasets exist that collects\r\nimages of users wearing AR glasses.\r\nTo address this problem, we present HMDPose, an infrared trinocular\r\ndataset of four different AR Head-mounted displays captured in\r\na car. It contains sequences of 14 subjects captured by three different\r\ncameras running at 60 FPS each, adding up to more than 3,000,000\r\nlabeled images in total. We provide a ground truth 6DoF-pose, captured\r\nby a marker-based tracker with submillimeter accuracy. We\r\nmake this dataset publicly available for non-profit, academic use\r\nand non-commercial benchmarking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "BMW Group Research, New Technology, Innovations",
              "dsl": ""
            }
          ],
          "personId": 43626
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 43629
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "TU Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 43676
        }
      ]
    },
    {
      "id": 43699,
      "typeId": 11699,
      "title": "Creating a Smart Virtual-Reality based Contextual Diary for People with Persistent Postural-Perceptual Dizziness to Facilitate Habituation",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Postural-Perceptual Dizziness (PPPD) has variable levels of severity and triggers. Hence the use of a e-diary to capture triggers could be useful for both the patient and treating clinician.  Virtual reality (VR) is not new to health sciences. This paper proposes a strategy that by using immersive VR environments at home, the technology could facilitate the user to identify baseline symptoms with an inbuilt e-diary, plus an ability to alter the virtual environments to assess triggers, habituation of triggers and also treatment improvements. We discuss the type of VR designs that could be useful to incorporate a PPPD e-diary from the perspective of a VR designer. We also consider the development of the virtual reality environment that could be paired with diary responses.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Media Design School",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "Torrens University Australia",
              "dsl": ""
            }
          ],
          "personId": 43608
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "Torrens University Australia",
              "dsl": "Research Director Center for Healthy Futures"
            }
          ],
          "personId": 43621
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "Torrens University Australia",
              "dsl": ""
            }
          ],
          "personId": 43672
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Media Design School",
              "dsl": ""
            }
          ],
          "personId": 43596
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Media Design School",
              "dsl": ""
            }
          ],
          "personId": 43646
        }
      ]
    },
    {
      "id": 43700,
      "typeId": 11699,
      "title": "Augmented Reality World Editor",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Image inpainting allows for filling masked areas of an image with synthesized content that is indistinguishable from its environment. We present a video inpainting pipeline that enables users to “erase” physical objects in their environment using a mobile device. The pipeline includes an augmented reality application and an on-device conditional adversarial model for generating the inpainted textures. Users are able to interactively remove clutter in their physical space in realtime. The pipeline preserves frame to frame coherence even with camera movements using ARCore, an augmented reality SDK by Google.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California Santa Barbara",
              "dsl": ""
            }
          ],
          "personId": 43645
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 43641
        }
      ]
    },
    {
      "id": 43701,
      "typeId": 11699,
      "title": "Ray-Casting-based 3D pointing and Dragging Interface for Naked-Eye Stereoscopic Displays",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "In this work, a ray-casting-based three-dimensional (3D) pointing and dragging interface for naked-eye stereoscopic displays is proposed. When a user holds a stylus and points it to a display, the proposed system displays a ray, which extends from the stylus to the virtual space in the display. This ray can be used to interact with objects in the virtual space. By conducting a user study, we found that the proposed method allows users to perform 3D pointing with smaller hand movements compared to a hand-capture-based interface. A model, which extends Fitts’s law, is also proposed. This model is capable of well predicting the time required for a 3D pointing task.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 43620
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 43624
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "TOkyo",
              "institution": "Shibaura tech",
              "dsl": "computer science and engineering"
            }
          ],
          "personId": 43588
        }
      ]
    },
    {
      "id": 43702,
      "typeId": 11699,
      "title": "Using Multiple Perspective Projections to Guide Visual Attention in Glyph-based Data Visualisations in VR",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "An important goal of glyph based data visualization is to detect anomalies in data sets or to manage complex search tasks by guiding the users attention to regions of interest. Previous studies suggest that the user's attention is tied to regions where different perspective projections meet. We describe a method for perspective distortion of glyphs within a virtual scene, while the rest of the scene is projected according to the common model of the computer graphics camera. The result is a multi-perspective image that directs the user's attention. The perspective distortion is only applied if the user focuses on an irrelevant area of the data visualization. As soon as the gaze moves towards the relevant glyph, the perspective distortion is gradually removed. Therefore we evaluate eye tracking data in our prototypical implementation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Chair of Media Design, Technische Universität Dresden",
              "dsl": ""
            }
          ],
          "personId": 43637
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Chair of Media Design, Technische Universität Dresden",
              "dsl": ""
            }
          ],
          "personId": 43613
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Chair of Media Design, Technische Universität Dresden",
              "dsl": ""
            }
          ],
          "personId": 43617
        }
      ]
    },
    {
      "id": 43703,
      "typeId": 11699,
      "title": "An Examination of Effects of Fear Using VR Content on Time Estimation",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "In recent years, the use of virtual reality (VR) attractions has been increasing in amusement facilities with the spread of head-mounted displays (HMDs) and the increase of VR content. VR attractions require less physical space than conventional attractions. However, because it takes a considerable time to adjust an HMD one by one, not many customers can play the attractions. In order to provide VR content with a long subjective time in a short time, this study presents VR content on an HMD that can evoke fear. Fear is one of the factors that influence psychological time. We investigate whether the VR content with fear influences time estimation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tochigi",
              "city": "Utsunomiya",
              "institution": "Utsunomiya University",
              "dsl": ""
            }
          ],
          "personId": 43647
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tochigi",
              "city": "Utsunomiya",
              "institution": "Utsunomiya University",
              "dsl": ""
            }
          ],
          "personId": 43604
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tochigi",
              "city": "Utsunomiya",
              "institution": "Utsunomiya University",
              "dsl": ""
            }
          ],
          "personId": 43570
        }
      ]
    },
    {
      "id": 43704,
      "typeId": 11699,
      "title": "Human Following Behavior In Virtual Reality",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Redirected Walking (RDW) allows users to perform real walking in virtual worlds that are larger than the available physical space. Many RDW algorithms rely on the prediction of users' possible paths in the virtual environment (VE) to calculate where users should be redirected to. This prediction could be obtained from the structure of the VE, where users look, or from existing path models. In this work, we examine users' walking behaviors in the presence of a virtual agent acting as a tour guide. Results showed that users changed their speed significantly to match the agent's walking speed. Furthermore, users also tend to adapt their trajectories to match with the agent's path. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Innovation Center Virtual Reality"
            }
          ],
          "personId": 43430
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Innovation Center Virtual Reality"
            }
          ],
          "personId": 43584
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "ICVR"
            }
          ],
          "personId": 43441
        }
      ]
    },
    {
      "id": 43705,
      "typeId": 11699,
      "title": "Immersive Analysis of Crystallographic Diffraction Data",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Single crystal structure determination is the foremost method to determine atomic structures -- from minerals to viruses.\r\nHowever, it is a complex process in which errors do not only lead to flawed structures, but may also hinder structure solution entirely.\r\nMany of these errors can be recognized by visualizing the measured diffraction data in reciprocal space. Here, we present an immersive tool to support such an analysis. We aim to supplement   this traditionally 2D desktop-based investigation of 3D diffraction data with the strengths of immersive visualization, especially depth perception and spatially tracked input devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-Universität",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43663
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-Universität",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43648
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Hamburg Advanced Research Centre for Bioorganic Chemistry (HARBOR)"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Rudolf-Virchow-Center - Center for Integrative and Translational Bioimaging"
            }
          ],
          "personId": 43671
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Hamburg Advanced Research Centre for Bioorganic Chemistry (HARBOR)"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Rudolf-Virchow-Center - Center for Integrative andTranslational Bioimaging"
            }
          ],
          "personId": 43568
        }
      ]
    },
    {
      "id": 43706,
      "typeId": 11699,
      "title": "ARLS: An asymmetrical remote learning system for sharing anatomy between an HMD and a light field display",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "VR remote learning is an environment-friendly approach for anatomy learning compared with the paper-based, physical model or prepared specimens. However, VR devices can potentially be costly pieces of equipment, and inexperienced users can experience unwanted symptoms like motion sickness. To solve these problems, an asymmetrical system has been developed to connect an experienced head-mounted-display user (lecturer) with light field display users (students) through the Internet. The scenes of lecturer's end and students' end are adjusted to match the corresponding displays technologies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 43627
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": "School of computer science"
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": "School of computer science"
            }
          ],
          "personId": 43576
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Dublin",
              "city": "Dublin",
              "institution": "University College Dublin ",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Ireland",
              "state": "Dublin",
              "city": "Dublin",
              "institution": "University College Dublin ",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 43673
        }
      ]
    },
    {
      "id": 43707,
      "typeId": 11699,
      "title": "On the Interplay of Foveated Rendering and Video Encoding",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Humans have sharp central vision but low peripheral visual acuity. Prior work has taken advantage of this phenomenon in two ways: foveated rendering (FR) reduces the computational workload of rendering by producing lower visual quality for peripheral regions and foveated video encoding (FVE) reduces the bitrate of streamed video through heavier compression of peripheral regions. Remote rendering systems require both rendering and video encoding and the two techniques can be combined to reduce both computing and bandwidth consumption. We report early results from such a combination with remote VR rendering. The results highlight that FR causes large bitrate overhead when combined with normal video encoding but combining it with FVE can mitigate it.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science, School of Science"
            }
          ],
          "personId": 43639
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43628
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science, School of Science"
            }
          ],
          "personId": 43622
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science, School of Science"
            }
          ],
          "personId": 43633
        }
      ]
    },
    {
      "id": 43708,
      "typeId": 11699,
      "title": "Introduction to AR-Bot, an AR system for robot navigation",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a system enabling to assign navigation tasks to a\r\nself-moving robot using an Augmented Reality (AR) application\r\nrunning on a smartphone. The system relies on a robot controller\r\nand a central server hosted on a PC. The user points a target location\r\nin the phone camera view and the robot moves accordingly. The\r\nrobot and the phone are independently located in the 3D space\r\nthanks to registration methods running on the server, hence they\r\ndo not need to be spatially registered to each other nor in direct\r\nline of sight.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sévigné",
              "institution": "Interdigital R&D",
              "dsl": "Immersive Lab"
            }
          ],
          "personId": 43578
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "InterDigital",
              "dsl": "Immersive Lab"
            }
          ],
          "personId": 43556
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Ille-et-Vilaine",
              "city": "Rennes",
              "institution": "InterDigital",
              "dsl": ""
            }
          ],
          "personId": 43638
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigné 35510",
              "institution": "InterDigital R&I France",
              "dsl": "IML"
            }
          ],
          "personId": 43615
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sévigné",
              "institution": "InterDigital R&D",
              "dsl": "Immersive Lab"
            }
          ],
          "personId": 43587
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "InterDigital R&D",
              "dsl": "Immersive Lab"
            }
          ],
          "personId": 43656
        }
      ]
    },
    {
      "id": 43709,
      "typeId": 11699,
      "title": "Haptic Selfies: Bold and Beautiful Living for the Blind and Visually Impaired",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "The blind and visually impaired (VI) miss subtle non-verbal gestures, facial expressions, or prosodic features of speech during social interaction. The inability to interpret one’s own or interaction partner’s visual, non-verbal cues impede communication, leading to awkward and embarrassing moments. This may eventually lead to social avoidance or isolation. In this paper, we discuss the concept of haptic selfies, a dynamic tangible interface that could make the blind and VI aware of their looks and define inner and outer beauty. Haptic selfies can promote enhanced social interaction and better integration into mainstream society through a better understanding and evaluation of interaction partners’ facial expressions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Cheney",
              "institution": "Eastern Washington University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 43593
        }
      ]
    },
    {
      "id": 43710,
      "typeId": 11699,
      "title": "Assessing the Suitability and Effectiveness of Mixed Reality Interfaces for Accurate Robot Teleoperation",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Over the last decade, many studies have focused on Virtual Reality (VR) and Mixed Reality (MR) frameworks for remotely controlling robotic systems. Although MR systems have been used to teleoperate robots in simple scenarios, their effectiveness in terms of accuracy, speed, and usability has not been rigorously evaluated for complex tasks that require accurate trajectories. In this work, an MR framework is evaluated to assess if it can be efficiently used in teleoperation tasks that require an accurate control of the robot end-effector. To achieve this, RGB-D cameras are employed to capture the robot and its local environment, and a remote user controls the robot arm motion through VR controllers. The captured data is streamed through the network and reconstructed in 3D, allowing the remote user to monitor the state of execution in real time through a VR headset. The system is compared with two other interface alternatives: i) teleoperation in pure VR, with the robot model rendered with respect to its real joint states, and ii) teleoperation in MR, with the rendered model of the robot superimposed on the real one.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Politecnico di Torino",
              "dsl": ""
            }
          ],
          "personId": 43657
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 43649
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Auckland Bioengineering Institute"
            }
          ],
          "personId": 43569
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Politecnico di Torino",
              "dsl": ""
            }
          ],
          "personId": 43580
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 43632
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Auckland Bioengineering Institute"
            }
          ],
          "personId": 43655
        }
      ]
    },
    {
      "id": 43711,
      "typeId": 11699,
      "title": "AffectivelyVR: Towards VR Personalized Emotion Recognition",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "We present AffectivelyVR, a personalized real-time emotion recognition system in VR that enables an emotion-adaptive virtual environment. We used off-the-shelf Electroencephalogram (EEG) and Galvanic Skin Response (GSR) physiological sensors to train user-specific machine learning models while exposing users to affective 360° VR videos. Since emotions are largely dependent on interpersonal experiences and expressed in different ways for different people, we personalize the model instead of generalizing it. By doing this, we achieved an emotion recognition accuracy of 96.5% using the personalized KNN algorithm, and 83.7% using the generalized SVM algorithm.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Empathic Computing Laboratory"
            }
          ],
          "personId": 43625
        },
        {
          "affiliations": [
            {
              "country": "Serbia and Montenegro",
              "state": "",
              "city": "Novi Sad",
              "institution": "University of Novi Sad",
              "dsl": "Faculty of Technical Sciences"
            }
          ],
          "personId": 43574
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Auckland Bioengineering Institute, University of Auckland",
              "dsl": "Empathic Computing Laboratory"
            }
          ],
          "personId": 43385
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "Empathic Computing Laboratory"
            }
          ],
          "personId": 43655
        }
      ]
    },
    {
      "id": 43712,
      "typeId": 11699,
      "title": "Dual Phone AR: Exploring the use of Phones as Controllers for Mobile Augmented Reality",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "The possible interactions with Mobile Augmented Reality applications today are largely limited to on-screen gestures and spatial movement. There is an opportunity to design new interaction methods that address common issues and go beyond the screen. Through this project, we explore the idea of using a second phone as a controller for mobile AR experiences. We develop prototypes that demonstrate the use of a second phone controller for basic mobile AR tasks such as pointing, selecting, and drawing in 3D space. We use these prototypes and insights from initial remote evaluations to discuss the benefits and drawbacks of such an interaction method. We conclude by outlining opportunities for future research on Dual Phone AR for multiple usage configurations, and in collaborative settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Maharashtra",
              "city": "Mumbai",
              "institution": "Indian Institute of Technology Bombay",
              "dsl": "IDC School of Design"
            },
            {
              "country": "India",
              "state": "Maharashtra",
              "city": "Mumbai",
              "institution": "Indian Institute of Technology Bombay",
              "dsl": "IDC School of Design"
            }
          ],
          "personId": 43640
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Mumbai",
              "institution": "Indian Institute of Technology, Bombay",
              "dsl": "IDC School of Design"
            },
            {
              "country": "India",
              "state": "",
              "city": "Mumbai",
              "institution": "Indian Institute of Technology, Bombay",
              "dsl": "IDC School of Design"
            }
          ],
          "personId": 43571
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Maharashtra",
              "city": "Mumbai",
              "institution": "Indian Institute of Technology Bombay",
              "dsl": "IDC School of Design"
            },
            {
              "country": "India",
              "state": "Maharashtra",
              "city": "Mumbai",
              "institution": "Indian Institute of Technology Bombay",
              "dsl": "IDC School of Design"
            }
          ],
          "personId": 43599
        }
      ]
    },
    {
      "id": 43713,
      "typeId": 11699,
      "title": "Life-size Sequential Photography in a Mixed Reality Environment",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "We visualize life-size sequential photographs of sports activities in a mixed reality (MR) environment. Wearing a video-see-through head-mounted display, an observer records the motions of a player using a handheld camera. Our system then places billboards in the three-dimensional MR space, on which the sequential photographs of the player’s motion are presented at life-size. In a user study, we found that the observers perceived the size of the motions more accurately than when viewing sequential photographs on a monitor display.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 43582
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "TOkyo",
              "institution": "Shibaura tech",
              "dsl": "computer science and engineering"
            }
          ],
          "personId": 43588
        }
      ]
    },
    {
      "id": 43714,
      "typeId": 11699,
      "title": "Impact of Social Distancing to Mitigate the Spread of COVID-19 in a Virtual Environment",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "A novel strand of Coronavirus, which is given the name SARS-CoV-2, has caused the associated respiratory syndrome, called COVID-19, to spread to the point of becoming a threatening pandemic. In order to mitigate the spread of this disease, many different policies have been adopted over the past months, ranging from strict national lockdown to milder government guidelines: one common aspect is that they mostly rely around keeping distance between individuals. The aim of this work is to provide means of visualizing the impact of social distancing in an immersive environment by making use of the virtual reality technology. We create a virtual environment which resembles a university setting, and we populate it with many AI agents that move around the environment to complete a specific set of tasks. We assume that the minimum social distance is 2 meters. The main contribution of this work is the multi-disciplinary aspect that results from visualizing the social distancing: the user can navigate in the virtual environment and receives visual feedback when getting close to other agents. We named our application SoDAlVR, which stands for Social Distancing Algorithm in Virtual Reality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Derbyshire",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing"
            }
          ],
          "personId": 43601
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Derbyshire",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing"
            }
          ],
          "personId": 43674
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "University of Derby",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Department of Computing"
            },
            {
              "country": "Spain",
              "state": "Andalucia",
              "city": "Granada",
              "institution": "Universidad de Granada",
              "dsl": "Ingeniería Informática"
            }
          ],
          "personId": 43567
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Derbyshire",
              "city": "Derby",
              "institution": "University of Derby",
              "dsl": "Dept of Electronics, Computing and Mathematics"
            }
          ],
          "personId": 43610
        }
      ]
    },
    {
      "id": 43715,
      "typeId": 11699,
      "title": "Analyzing the Trade-off between Selection and Navigation in VR",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Navigation and selection are critical in very large VEs, such as a whole town. In practice, many VR applications require both of the therse modalities to work together. We compare different combinations of two navigation and two selection methods in VR, in this study. The aim of our work is to discover the trade-off between navigation and selection techniques and to identify which combination leads to better interaction performance in large VEs. The results showed that, users could complete their task faster with the fly/drive method and traveled less distance, compared to the teleportation method. Additionally, the raycasting method exhibited a better performance in terms of time and (less) distance traveled, however, it significantly increased the error rate for the selection of targets.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Surrey",
              "institution": "Simon Fraser University",
              "dsl": ""
            }
          ],
          "personId": 43598
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 43458
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 43670
        }
      ]
    },
    {
      "id": 43716,
      "typeId": 11699,
      "title": "An Examination of Influence on Weight Perception by Visual Change of the Own Arm",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "The purpose of this research is to make a change in weight perception by utilizing the potential impression that a person has acquired from visual information. Recently, the illusion caused by changes in visual information, which is represented by the rubber hand illusion and the Proteus effect, has been reported. In this research, we make one’s self-awareness change by the appearance of the own arm through AR technology. We conduct an experiment to verify that the change of the self-awareness influences the weight perception of grasping an actual object.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tochigi",
              "institution": "Utsunomiya University",
              "dsl": ""
            }
          ],
          "personId": 43573
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Electro-Communications",
              "dsl": ""
            }
          ],
          "personId": 43616
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tochigi",
              "city": "Utsunomiya",
              "institution": "Utsunomiya University",
              "dsl": ""
            }
          ],
          "personId": 43570
        }
      ]
    },
    {
      "id": 43717,
      "typeId": 11699,
      "title": "Physicalizing Virtual Objects with Affordances to Support Tangible Interactions in AR",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a method of physicalizing virtual objects with physical properties by providing tangible feedback via mechanical moving. In addition, we developed an interaction prototype system that could simulate contact feedback for several different virtual objects. The system is efficient in providing natural and consistent interactions when putting physical objects onto virtual objects in Augmented Reality (AR) scenarios. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi’an",
              "institution": "Cyper-Physical Interaction Lab",
              "dsl": "Northwestern Polytechnical University"
            }
          ],
          "personId": 43644
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical university",
              "dsl": "Cyber-Physical Interaction Lab.（CPI lab）"
            }
          ],
          "personId": 43597
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University",
              "dsl": "Cyber-Physical Interaction Lab."
            }
          ],
          "personId": 43562
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University",
              "dsl": "Cyper-Physical Interaction Lab"
            }
          ],
          "personId": 43660
        }
      ]
    },
    {
      "id": 43718,
      "typeId": 11699,
      "title": "Metachron: A framework for time perception research in VR",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "The perception of time is closely related to our well-being. Psycho-pathological conditions such as depression, schizophrenia and autism are often linked to a disturbed sense of time. In this paper we present a novel framework, called \\textit{\\textit{Metachron}}, which aims to support research in the field of time perception and manipulation in Virtual Reality (VR). Our system allows the systematic modification of events in real time along three main event axis i) \\textit{Velocity}, ii) \\textit{Syncronicity} and iii) \\textit{Density}.  Our future work will investigate the impact of each dimensions on the passage of time (varying velocity of time flow) and structure of time (varying synchronicity of events) which should provide insights to build VR diagnostic and therapeutic tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            }
          ],
          "personId": 43581
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Department of Computer Science, HCI Group"
            }
          ],
          "personId": 43650
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 43635
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 43377
        }
      ]
    },
    {
      "id": 43719,
      "typeId": 11699,
      "title": "RoadVR: Mitigating the Effect of Vection and Sickness by Distortion of Pathways for In-Car Virtual Reality",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "We explore a method to reduce motion sickness and allow people to use virtual reality while moving in vehicles. We put forth a usage scenario where the target VR content is based on constant road navigation so that the actual motion can enhance the VR experience. The method starts with a virtual scene and objects around an infinitely straight road. The motion of the vehicle is sensed by the GPS and IMU module. The sensed motion is reflected in a way that the virtual scene is navigated according to the vehicle motion, and its pathways distorted such that the virtual motion has a near-identical optical flow pattern to the actual. This would align the user’s visual and vestibular sense and reduce the effect of vection and motion sickness. We ran an pilot experiment to validate our approach, comparing the before and after sickness levels with the VR content (1) not aligned to the motion of the vehicle and (2) aligned by our method. Our preliminary results have shown the sickness was reduced significantly (but not eliminated to a negligible level yet) with our approach.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "N/A",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 43558
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 43471
        }
      ]
    },
    {
      "id": 43720,
      "typeId": 11699,
      "title": "Volumetric capture for narrative films",
      "trackId": 11110,
      "tags": [],
      "keywords": [],
      "abstract": "Volumetric capture is a recording technique that allows to create ``holographic'' recordings of actors, sets and props. The technique can be used to create immersive stories that sometimes reflect aspects of reality better than realistic 3D models. For example, volumetric captures of actors do not seem to cause uncanny valley effect. In this paper, we provide an overview of volumetric capture technology. As a driving example, we use ``****'', a transmedia project, in which the same story was told in three different ways: as a traditional film, a 360 video, and a volumetric film.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "American University",
              "dsl": "IDEAS Lab"
            }
          ],
          "personId": 43585
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "California Polytechnic State University San Luis Obispo",
              "dsl": "Computer Science"
            }
          ],
          "personId": 43577
        }
      ]
    },
    {
      "id": 43496,
      "typeId": 11696,
      "title": "Anonymity vs Familiarity: Self-Disclosure and Privacy in Social Virtual Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding how and why users reveal information about their self in online social spaces and what they perceive as privacy online is a central research agenda in HCI. Drawing on 30 in-depth interviews, in this paper we focus on what type of information users disclose, to whom they reveal information, and concerns they had regarding self-disclosure in social Virtual Reality (VR) - where multiple users can interact with one another through VR head-mounted displays in 3D virtual spaces. Our findings show that overall, users felt comfortable to disclose their emotions, personal experience, and personal information in social VR. However, they also acknowledged that disclosing personal information in social VR was an inevitable trade-off: giving up bio-metric information in order to better use the system. We contribute to existing literature on self-disclosure and privacy online by focusing on social VR as an emerging novel online social space. We also explicate implications for designing future social VR applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing, Human Centered Computing"
            }
          ],
          "personId": 43395
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 43442
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 43493
        }
      ]
    },
    {
      "id": 43497,
      "typeId": 11696,
      "title": "Towards Physically Interactive Virtual Environments: Reactive Alignment with Redirected Walking",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Interactions with the physical environment, such as passive haptic feedback, have been previously shown to provide richer and more immersive virtual reality experiences. A strict correspondence between the virtual and real world coordinate systems is a staple requirement for physical interaction. However, many of the commonly employed VR locomotion techniques allow for, or even require, this relationship to change as the experience progresses. The outcome is that experience designers frequently have to choose between flexible locomotion or physical interactivity, as the two are often mutually exclusive. To address this limitation, this paper introduces reactive environmental alignment, a novel framework that leverages redirected walking techniques to achieve a desired configuration of the virtual and real world coordinate systems.  This approach can transition the system from a misaligned state to an aligned state, thereby enabling the user to interact with physical proxy objects or passive haptic surfaces.  Simulation-based experiments demonstrate the effectiveness of reactive alignment and provide insight into the mechanics and potential applications of the proposed algorithm. In the future, reactive environmental alignment can enhance the interactivity of virtual reality systems and inform new research vectors that combine redirected walking and passive haptics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 43384
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 43389
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Computer Science & Engineering"
            }
          ],
          "personId": 43383
        }
      ]
    },
    {
      "id": 43498,
      "typeId": 11696,
      "title": "The Effects of Visual Realism on Spatial Memory and Exploration Patterns in Virtual Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding the effects of environmental features such as visual realism on spatial memory can inform a human-centered design of virtual environments. This paper investigates the effects of visual realism on object location memory in virtual reality, taking account of individual differences, gaze, and locomotion. Participants freely explored two environments which varied in visual realism, and then recalled the locations of objects by returning the misplaced objects back to original locations. Overall, we did not find a significant relationship between visual realism and object location memory. We found, however, that individual differences such as spatial ability and gender accounted for more variance. Gaze and locomotion analysis suggest that participants exhibited higher gaze duration and more clustered movement patterns in the low realism condition. Preliminary inspection further found that locomotion hotspots coincided with objects that showed a significant gaze time difference between high and low visual realism levels. These results suggest that high visual realism still provides positive spatial learning affordances but the effects are more intricate.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "Department of Geography"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "Center for Immersive Experiences"
            }
          ],
          "personId": 43357
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "Department of Geography"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "Center for Immersive Experiences"
            }
          ],
          "personId": 43397
        }
      ]
    },
    {
      "id": 43499,
      "typeId": 11696,
      "title": "Temporal Consistent Motion Parallax for Omnidirectional Stereo Panorama Video",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "We present a new pipeline to enable head-motion parallax in omnidirectional stereo (ODS) panorama video rendering using a neural depth decoder. While recent ODS panorama cameras record short-baseline horizontal stereo parallax to offer the impression of binocular depth, they do not support the necessary translational degrees-of-freedom (DoF) to also provide for head-motion parallax in virtual reality (VR) applications. \r\nTo overcome this limitation, we propose a pipeline that enhances the classical ODS panorama format with 6 DoF free-viewpoint rendering by decomposing the scene into a multi-layer mesh representation. Given a spherical stereo panorama video, we use the horizontal disparity to store explicit depth information for both eyes in a simple neural decoder architecture. While this approach produces reasonable results for individual frames, video rendering usually suffers from temporal depth inconsistencies. Thus, we perform successive optimization to improve temporal consistency by fine-tuning our depth decoder for both temporal and spatial smoothness. \r\nUsing a consumer-grade ODS camera, we thoroughly evaluate our approach on a number of real-world scene recordings and demonstrate the versatility and robustness of the proposed pipeline.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Braunschweig",
              "institution": "TU Braunschweig",
              "dsl": ""
            }
          ],
          "personId": 43452
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Braunschweig",
              "institution": "Institut für Computergraphik",
              "dsl": "TU Braunschweig"
            }
          ],
          "personId": 43450
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Braunschweig",
              "institution": "TU Braunschweig",
              "dsl": ""
            }
          ],
          "personId": 43418
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Braunschweig",
              "institution": "Institut für Computergraphik",
              "dsl": "TU Braunschweig"
            }
          ],
          "personId": 43406
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Braunschweig",
              "institution": "TU Braunschweig",
              "dsl": "Computer Graphics"
            }
          ],
          "personId": 43494
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Braunschweig",
              "institution": "Institut für Computergraphik",
              "dsl": "TU Braunschweig"
            }
          ],
          "personId": 43470
        }
      ]
    },
    {
      "id": 43500,
      "typeId": 11696,
      "title": "The Effects of Self- and External Perception of Avatars on Cognitive Task Performance in Virtual Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual Reality (VR) allows embodying any possible avatar. Known as the Proteus effect, avatars can change users' behavior and attitudes. Previous work found that embodying Albert Einstein can increase cognitive task performance. The behavioral confirmation paradigm, however, predicts that our behavior is also affected by others' perception of us. Therefore, we investigated the cognitive performance in collaborative VR when self-perception and external perception of the own avatar differ. 32 male participants performed a Tower of London task in pairs. One participant embodied Einstein or a young adult while the other perceived the participant as Einstein or a young adult. We show that the perception by others affects cognitive performance. The Einstein avatar also decreased the perceived workload. Results imply that avatars' appearance to both, the user and the others must be considered when designing for cognitively demanding tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "Media Informatics Group",
              "dsl": "University of Regensburg"
            }
          ],
          "personId": 43439
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "Media Informatics Group",
              "dsl": "University of Regensburg"
            }
          ],
          "personId": 43388
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 43372
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "Media Informatics Group",
              "dsl": "University of Regensburg"
            }
          ],
          "personId": 43366
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "Media Informatics Group",
              "dsl": "University of Regensburg"
            }
          ],
          "personId": 43365
        }
      ]
    },
    {
      "id": 43501,
      "typeId": 11696,
      "title": "Dynamic Projection Mapping of Deformable Stretchable Materials",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "We present a method for dynamic projection mapping on deformable materials (e.g. cloth) including stretchable elastic materials using a time of flight (ToF) depth camera (e.g. Azure Kinect or Pico-Flexx) that come equipped with an IR camera. We use Bezier surfaces to model the projection surface without explicitly modeling the deformation. Leveraging the nature of deformable and stretchable materials, we devise an efficient tracking method that can track the boundary of the surface material using the IR and depth camera available in any such ToF camera. This achieves realistic mapping even in the interior of the surface, with simple markers (e.g. black dots or squares) or without markers entirely, such that the projection appears to be printed on the material. The surface representation is updated in real-time using GPU based computations. Further, we also show that the speed of these updates is limited by the speed of the camera and therefore can be adopted for higher speed cameras as well. This technique can be used to project on several stretchable moving materials to change their appearance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "UC Irvine",
              "dsl": "Interactive Graphics & Visualization Lab"
            }
          ],
          "personId": 43387
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Interactive Graphics & Visualization Lab"
            }
          ],
          "personId": 43355
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "UCI",
              "dsl": "Computer science"
            }
          ],
          "personId": 43453
        }
      ]
    },
    {
      "id": 43502,
      "typeId": 11696,
      "title": "Virtual Human in AR: Evaluation of Presentation Concepts in an Industrial Assistance Use Case",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Utilizing a virtual human for educational settings enables the transfer of approved real-world concepts of expert-based learning by observation and imitation to extended reality (XR) scenarios. Whilst researchers have developed and investigated various presentation concepts of virtual humans for learning in domains such as sports and rehabilitation, lack of knowledge exists regarding the transferability to industrial assistance use cases. In the prior work of Lampen et al. it was shown for the first time that within a manual assembly use case the effectiveness of a three-dimensional (3D) registered virtual human assistance is comparable to state of the art AR assistances. We extend that work by conducting a comparable user study (N=30), with the goal to verify the worth of extra implementation costs, caused by the incorporation of assistive behavior features and the enabling of a 3D registration. With regard to the dimensional view, the results of evaluated performance criteria reveal that the concept of a true to scale 3D registered representation of a virtual human is limited and comparable to a two-dimensional screen aligned presentation form. However, by incorporating additional assistive behaviors, the assistance concept of a 3D registered virtual human is enhanced and shows significant advantages in terms of cognitive savings and faultlessness. Respecting the industrial assistance use case, it can be concluded, that a 3D registered virtual human with assistive behavior is valuable in situations where time is less crucial, e.g. in learning scenarios or during complex tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Neu-Ulm",
              "institution": "EvoBus GmbH",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "CITEC"
            }
          ],
          "personId": 43390
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Neu-Ulm",
              "institution": "EvoBus GmbH",
              "dsl": ""
            }
          ],
          "personId": 43440
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Emden",
              "institution": "University of Applied Sciences Emden/Leer",
              "dsl": "Faculty of Technology"
            }
          ],
          "personId": 43457
        }
      ]
    },
    {
      "id": 43503,
      "typeId": 11696,
      "title": "Body LayARs: A Toolkit for Body-Based Augmented Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Technological advances are enabling a new class of augmented reality (AR) applications that use bodies as substrates for input and output. In contrast to sensing and augmenting objects, body-based AR applications track people around the user and layer information on them. However, prototyping such applications is complex, time-consuming, and cumbersome, due to a lack of easily accessible tooling and infrastructure. We present Body LayARs, a toolkit for fast development of body-based AR prototypes. Instead of directly programming for a device, Body LayARs provides an extensible graphical programming environment with a device-independent runtime abstraction. We focus on face-based experiences for headset AR, and show how Body LayARs makes a range of body-based AR applications fast and easy to prototype.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": ""
            }
          ],
          "personId": 43491
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": ""
            }
          ],
          "personId": 43375
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": ""
            }
          ],
          "personId": 43434
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43417
        }
      ]
    },
    {
      "id": 43504,
      "typeId": 11696,
      "title": "CorFix: Virtual Reality Cardiac Surgical Planning System forDesigning Patient Specific Vascular Grafts",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Patients with single ventricle heart defect undergo Fontan surgery to reroute the blood flow from the lower body to the lung by connecting the inferior vena cava to the pulmonary artery using a vascular graft. Despite the unique anatomical structure of each patient, shape, size, and growth potential of current vascular grafts are extremely limited, which contribute significantly to the long-term failure rate of Fontan surgeries. Tissue engineered vascular grafts have the potential to reduce these complications by creating biodegradable and patient specific designed grafts. Since surgeons have limited time and rely on a deformed anatomical structure where all blood is evacuated from heart during the operations, assistance of engineers is required. However, they not only lack of medical background but their tools, computer aided design (CAD) software, also lack to provide depth perception, surgical features, and design guidelines for creating a vascular graft. These limitations may lead to long lead times, inconsistent workflow, and surgically infeasible graft designs. In this paper, we introduce a novel virtual reality (VR) vascular graft modeling software - CorFix, that provides solutions to these challenges. CorFix includes several visualization features for performing diagnostics and surgical features with design guidelines for creating a patient specific tube-shaped grafts in 3D. The designed vascular graft can be exported into a 3D model, which can be utilized for 3D printing and performing computational fluid dynamic analysis. The patient specific vascular graft designs in CorFix were compared to an engineering CAD software, SolidWorks (Dassault Systèmes, Vélizy-Villacoublay, France), by 8 experienced engineers. CorFix showed a higher success rate of designing surgically feasible vascular grafts with 3.4 times less design time than the CAD software. Additionally, CorFix scored higher in usability and lower in perceived workload than CAD despite the one time 10-minute tutorial. The study showed expanding a potential users of CorFix to medical doctors without engineering or 3D modeling background. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "A James Clark School of Engineering"
            },
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baitmore",
              "institution": "Johns Hopkins University",
              "dsl": "Whiting School of Engineering"
            }
          ],
          "personId": 43363
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": ""
            }
          ],
          "personId": 43405
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "A James Clark School of Engineering"
            }
          ],
          "personId": 43412
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "A James Clark School of Engineering"
            },
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Batlimore",
              "institution": "Johns Hopkins University",
              "dsl": "Whiting School of Engineering"
            }
          ],
          "personId": 43445
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Northwest",
              "institution": "Children's National Hospital",
              "dsl": ""
            }
          ],
          "personId": 43481
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Northwest",
              "institution": "Children's National Hospital",
              "dsl": ""
            }
          ],
          "personId": 43378
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "The University of Chicago Medicine",
              "dsl": "Division of Surgery"
            }
          ],
          "personId": 43356
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Northwest",
              "institution": "Childen's National Hospital",
              "dsl": ""
            }
          ],
          "personId": 43485
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "A James Clark School of Engineering"
            },
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Batlimore",
              "institution": "Johns Hopkins University",
              "dsl": "Whiting School of Engineering"
            }
          ],
          "personId": 43420
        }
      ]
    },
    {
      "id": 43505,
      "typeId": 11696,
      "title": "Characterizing In-Air Eyes-Free Typing Movements in VR",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "We empirically explore fundamental requirements for achieving VR in-air typing by observing the unconstrained eyes-free in-air typing of touch typists. We show that unconstrained typing movements differ substantively from previously observed constrained in-air typing movements and introduce a novel binary categorization of typing strategies: typists who use finger movements alone (FINGER) and those who combine finger movement with gross hand movement (HAND). We examine properties of finger kinematics, correlated movement of fingers, interrelation in consecutive key-strokes, and 3D distribution of key-stroke movements. We report that, compared to constrained typing, unconstrained typing generates shorter (49 mm) and faster (764 mm/s) key-strokes with a high correlation of finger movement and that the HAND strategy group exhibits more dynamic key-strokes. We discuss how these findings can inform the design of future in-air typing systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology",
              "dsl": "Department of Human Factors Engineering"
            }
          ],
          "personId": 43468
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "UNIST",
              "dsl": "Human Factors Engineering"
            }
          ],
          "personId": 43473
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 43393
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 43374
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "UNIST",
              "dsl": ""
            }
          ],
          "personId": 43382
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43459
        }
      ]
    },
    {
      "id": 43506,
      "typeId": 11696,
      "title": "Virtual Projection Planes for the Visual Comparison of Photogrammetric 3D Reconstructions with Photo Footage",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "The latest advancements of virtual reality and image-based reconstruction promise novel opportunities to explore and analyze 3D reconstructions of real objects, buildings and places in immersive 3D environments. However, the faithfulness of the presented data is not always obvious and, in most cases, a 3D reconstruction can not be compared directly to its corresponding real world instance. However, in case of  reconstruction methods based on structure from motion (SFM), a large number of raw photos is available. This motivated us to develop a novel interaction technique for the visual comparison of 3D models with such spatially registered images, e.g.\\ in order to rapidly verify the authenticity of perceived features. The results of a formal user study (n=18) demonstrate the effectiveness and low interaction overhead of the use of such visual provenance information. Further observations on the usability of implementation details informed our iterative design process and led to relevant improvements of our interaction techniques. Our final implementation retains the efficiency of the original approach while eliminating the observed issues.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus Universität Weimar ",
              "dsl": "Visualization Research and Virtual Reality"
            }
          ],
          "personId": 43401
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Visualization Research and Virtual Reality"
            }
          ],
          "personId": 43495
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research"
            }
          ],
          "personId": 43454
        }
      ]
    },
    {
      "id": 43507,
      "typeId": 11696,
      "title": "Don’t Bother Me: How to Handle Content-Irrelevant Objects in Handheld Augmented Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "In augmented reality (AR), realistic interactions between real objects (ROs) with virtual objects (VOs) are known to have a vital impact on the AR experience. \r\nHowever, ROs also can negatively affect AR experiences. \r\nFor example, a pedestrian passing in proximity to VOs could distract users from attending to the AR contents. \r\nParticularly, if the users could see the unaugmented real environment with their bare eyes, as in the case of the handheld AR, they might glimpse the pedestrian, taking eyes off the screen momentarily. \r\nIn this paper, we applied the concept of diminished reality (DR) to remove content-irrelevant and undesirable (e.g., distracting) pedestrian (i.e., real object) in the context of handheld AR. \r\nIn consideration of the additional visual inconsistency introduced by DR in a handheld device (i.e., a RO is seen outside the screen but not on the screen), we prepared three AR view conditions: \r\nin Transparent (TP) condition, we removed the pedestrian entirely; in Semi-transparent (ST) condition, the pedestrian became semi-transparent; lastly, in Default (DF) condition, the pedestrian appeared as is. \r\nWe conducted a user study to compare the effects of the three AR view conditions on users' engagement to the AR content and perception of a virtual pet in the content. \r\nOur findings revealed that users felt less distracted by the pedestrian and allocated more attention to the AR content in TP and ST conditions, compared to DF condition. \r\nFurthermore, in the TP compared to the DF condition, users felt the virtual pet as more life-like, its behavior more plausible, and a higher spatial presence of the pet in the real environment. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "Korea University Graduate School",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Imaging Media Reasearch"
            }
          ],
          "personId": 43411
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University, Graduate School",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 43480
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 43381
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 43471
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Imaging Media Research"
            }
          ],
          "personId": 43376
        }
      ]
    },
    {
      "id": 43508,
      "typeId": 11696,
      "title": "Docking Haptics: Dynamic Combinations of Grounded and Worn Devices",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Grounded haptic devices can provide a variety of forces but have limited working volumes. Wearable haptic devices operate over a large volume but are relatively restricted in the types of stimuli they can generate. We propose the concept of docking haptics, in which different types of haptic devices are dynamically docked at run time. This creates a hybrid system, where the potential feedback depends on the user's location. We show a prototype docking haptic workspace, combining a grounded six degree-of-freedom force feedback arm with a hand exoskeleton. We are able to create the sensation of weight on the hand when it is within reach of the grounded device, but away from the grounded device, hand-referenced force feedback is still available. A user study demonstrates that users can successfully discriminate weight when using docking haptics, but not with the exoskeleton alone. Such hybrid systems would be able to change configuration further, for example docking two grounded devices to a hand in order to deliver twice the force, or extend the working volume. We suggest that the docking haptics concept can thus extend the practical utility of haptics in user interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43443
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 43424
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College london",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43437
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 43465
        }
      ]
    },
    {
      "id": 43509,
      "typeId": 11696,
      "title": "Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual environments with a wide range of scale are becoming commonplace in Virtual Reality applications. Methods to control locomotion parameters may help users explore such environments more easily or hinder their navigation. For multi-scale virtual environments, point-and-teleport locomotion with a well-designed distance control interface can enable mid-air teleportation, which makes it competitive to flying interfaces. Yet, automatic distance control for point-and-teleport has not been studied in such environments. We present a new method to automatically control the distance for point-and-teleport, a very common navigation technique in VR applications.\r\nIn our first user study, we used a solar system environment to compare three methods: automatic distance control for point-and-teleport, manual distance control for point-and-teleport, and automatic speed control for flying. Results showed that automatic control significantly reduces overshoot compared with manual control for point-and-teleport, but the discontinuous nature of teleportation made users prefer flying with automatic speed control. We conducted a second study to compare automatic-speed-controlled flying and two versions of our teleportation method with automatic distance control, one incorporating optical flow cues. We found that point-and-teleport with optical flow cues and automatic distance control was more preferred than point-and-teleport without the cues, and navigation with it is more accurate than flying with automatic speed control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Surrey",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts and Technology"
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Surrey",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts and Technology"
            }
          ],
          "personId": 43448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe",
              "dsl": "Adobe Research"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Adobe",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 43477
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San jose",
              "institution": "Adobe Inc.",
              "dsl": "Adobe Research"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San jose",
              "institution": "Adobe Inc.",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 43484
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Adobe Systems Incorporated",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Adobe Systems Incorporated",
              "dsl": ""
            }
          ],
          "personId": 43463
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 43458
        }
      ]
    },
    {
      "id": 43510,
      "typeId": 11696,
      "title": "A Quest for Co-Located Mixed Reality: Aligning and Assessing SLAM Tracking for Same-Space Multi-User Experiences",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Current solutions for creating co-located Mixed Reality (MR) experiences typically rely on platform-specific synchronisation of spatial anchors or Simultaneous Localisation and Mapping (SLAM) data across clients, often coupled to cloud services. This introduces significant costs (in development and deployment), constraints (with interoperability across platforms often limited), and privacy concerns. For practitioners, support is needed for creating platform-agnostic co-located MR experiences. This paper provides this support by: 1) surveying approaches toward aligning disparate device coordinate spaces, formalizing their theoretical accuracy and limitations; 2) providing skeleton implementations for audience-based, small-scale and large-scale co-location using said alignment approaches; and 3) detailing how we can assess the accuracy and safety of 6DoF/SLAM tracking solutions for any arbitrary device and dynamic environment without the need for an expensive ground truth optical tracking, by using trilateration and a \\$30 laser distance meter. Through this, we hope to further democratise the creation of cross-platform co-located MR experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 43364
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Institut Polytechnique de Paris",
              "dsl": "Télécom Paris - LTCI"
            }
          ],
          "personId": 43489
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 43478
        }
      ]
    },
    {
      "id": 43511,
      "typeId": 11696,
      "title": "The Experience of Social Touch in Multi-User Virtual Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "We present user study results on the experience of virtual body contact in a two-user VR scenario, in which participants performed different touches with a research assistant. We found small main and interaction effects indicating that intimacy, touch direction, and sex affected the experience similarly as they would in the physical world. Qualitative data support the assumption that intimacy, among aspects related to the partner, realism, and perceived stimuli, played a decisive role in the experience. Further, individual comfort with interpersonal touch was positively associated with relaxation and happiness during VR interaction. Interestingly, we found substantial inter-individual differences regarding the intensity of the emotional reactions. We discuss the results with regards to implications for follow-up studies and for the design of social VR applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Duisburg",
              "institution": "University of Duisburg-Essen",
              "dsl": ""
            }
          ],
          "personId": 43404
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "D-47057 Duisburg",
              "institution": "University of Duisburg-Essen",
              "dsl": "Faculty of Engineering / Department for Computer Science and Cognitive Science / Entertainment Computing"
            }
          ],
          "personId": 43407
        }
      ]
    },
    {
      "id": 43512,
      "typeId": 11696,
      "title": "The Effects of Full-Body Avatar Movement Predictions in Virtual Reality using Neural Networks",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Motion tracking technologies and avatars in virtual reality (VR) showing the movements of the own body enable high levels of presence and a strong illusion of body ownership (IBO) -- key features of immersive systems and gaming experiences in virtual environments. Previous work suggests using software-based algorithms that can not only compensate system latency but also predict future movements of the user to increase input performance. However, the effects of movement prediction in VR on input performance are largely unknown. In this paper, we investigate neural network-based predictions of full-body avatar movements in two scenarios: In the first study, we used a standardized 2D Fitts' Law task to examine the information throughput in VR. In the second study, we utilized a full-body VR game to determine the users' performance. We found that both performance and subjective measures in a standardized 2D Fitts' law task could not benefit from the predicted avatar movements. In an immersive gaming scenario, however, the perceived accuracy of the own body location improved. Presence and body assessments remained more stable and were higher than during the Fitts' task. We conclude that ML-based predictions could be used to compensate system-related latency but participants only subjectively benefit under certain conditions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 43372
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43490
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43352
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43416
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43464
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bayern",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43353
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43392
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43365
        }
      ]
    },
    {
      "id": 43513,
      "typeId": 11696,
      "title": "Virtual Navigation considering User Workspace: Automatic and Manual Positioning before Teleportation",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Teleportation is a navigation technique widely used in virtual reality, especially for applications using head-mounted displays. Basic teleportation usually moves a user’s viewpoint to a new destination of the virtual environment without taking into account the physical space surrounding them. However, considering the user’s real workspace is crucial for preventing them from reaching its limits and thus managing direct access to multiple virtual objects. In this paper, we propose to display a virtual representation of the user’s real workspace before the teleportation, and compare manual and automatic techniques for positioning such a virtual workspace. For manual positioning, the user adjusts themself the position and orientation of their future virtual workspace. A first controlled experiment compared exocentric and egocentric manipulation techniques with different virtual workspace representations, including or not an avatar at the user’s future destination. Although exocentric and egocentric techniques result in a similar level of performance, representations with an avatar help the user to understand better how they will land after teleportation. For automatic positioning, the user selects their future virtual workspace among relevant options generated at runtime by an algorithm that considers the layout of interactive objects. A second controlled experiment shows that the manual technique selected from the first experiment and this automatic technique are more efficient than the basic teleportation. Besides, the manual technique seems to be more suitable for crowded scenes than the automatic one.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LIMSI, VENISE team ",
              "dsl": ""
            }
          ],
          "personId": 43429
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LIMSI",
              "dsl": ""
            }
          ],
          "personId": 43483
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LIMSI, VENISE team",
              "dsl": ""
            }
          ],
          "personId": 43426
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, Inria, LRI",
              "dsl": ""
            }
          ],
          "personId": 43432
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LIMSI, VENISE team",
              "dsl": ""
            }
          ],
          "personId": 43394
        }
      ]
    },
    {
      "id": 43514,
      "typeId": 11696,
      "title": "VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Accurate sketching in virtual 3D environments is challenging due to limited depth perception. To address this issue, we developed VRSketchPen. VRSketchPen uses two haptic modalities to support virtual sketching without constraining user actions: (1) pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2) vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate the efficiency of VRSketchPen, we conducted a lab experiment (N=20) to compare pneumatic, vibrotactile, and a combination of both with snapping and no assistance for flat and curved surfaces in a 3Dvirtual environment. Our findings showed that usage of pneumatic, vibrotactile, or a combination significantly improves 2D shape accuracy and leads to significantly diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence, and user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Offenbach am Main",
              "institution": "Honda Research Institute Europe",
              "dsl": ""
            }
          ],
          "personId": 43421
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 43469
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 43438
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 43446
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 43368
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University of Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 43455
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University of Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 43461
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University of Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 43487
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Offenbach am Main",
              "institution": "Honda Research Institute Europe",
              "dsl": ""
            }
          ],
          "personId": 43476
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 43466
        }
      ]
    },
    {
      "id": 43515,
      "typeId": 11696,
      "title": "The Impact of Missing Fingers in Virtual Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Avatars in virtual reality (VR) can have body structures that differ from the physical self. Game designers, for example, often stylize virtual characters by reducing the number of fingers. Previous work found that the sensation of presence in VR depends on avatar realism and the number of limbs. However, it is currently unknown how the removal of individual fingers affects the VR experience, body perception, and how fingers are used instead. In a study with 24 participants, we investigate the effects of missing fingers and avatar realism on presence, phantom pain perception, and finger usage. Our results show that particularly missing index fingers decrease presence, show the highest phantom pain ratings, and significantly change hand interaction behavior. We found that relative usage of thumb and index fingers in contrast to middle, ring, and little finger usage was higher with abstract hands than with realistic ones -- even when the fingers were missing. We assume that dominant fingers are firstly integrated into the own body scheme when an avatar does not resemble one's own appearance. We discuss cognitive mechanisms in experiencing virtual limb loss, task limitations, and implications for VR applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "Universität Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43439
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 43370
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 43372
        }
      ]
    },
    {
      "id": 43516,
      "typeId": 11696,
      "title": "Capture to Rendering Pipeline for Generating Dynamically Relightable Virtual Objects with Handheld RGB-D Cameras",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "We present a complete end-to-end pipeline for generating dynamically relightable virtual objects captured using a single handheld consumer-grade RGB-D camera.  The proposed system plausibly replicates the geometry, texture, illumination, and surface reflectance properties of non-Lambertian objects, making them suitable for integration within virtual reality scenes that contain arbitrary illumination.  First, the geometry of the target object is reconstructed from depth images captured using a handheld camera. To get nearly drift-free texture maps of the virtual object, a set of selected images from the original color stream is used for camera pose optimization. Our approach further separates these images into diffuse (view-independent) and specular (view-dependent) components using low-rank decomposition. The lighting conditions during capture and reflectance properties of the virtual object are subsequently estimated from the computed specular maps. By combining these parameters with the diffuse texture, the reconstructed model can then be rendered in real-time virtual reality scenes that plausibly replicate real world illumination at the point of capture. Furthermore, these objects can interact with arbitrary virtual lights that vary in direction, intensity, and color.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institution of Creative Technologies",
              "dsl": "Computer Science/School of Engineering/MxR Lab"
            }
          ],
          "personId": 43436
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Computer Science & Engineering"
            }
          ],
          "personId": 43383
        }
      ]
    },
    {
      "id": 43517,
      "typeId": 11696,
      "title": "DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user’s hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user’s hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object’s surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 43431
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 43486
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 43482
        }
      ]
    },
    {
      "id": 43518,
      "typeId": 11696,
      "title": "Moving Virtual Reality out of its Comfort Zone and Into the African Kalahari Desert Field: Experiences From Technological Co-Exploration With an Indigenous San Community in Namibia",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "Indigenous people (IP) living in remote areas, at the margins of mainstream society, are often the last ones to experience emerging technologies and even less to shape those experiences. It could be argued technology exposure and experience is necessary for IP to gain agency in making informed decisions on the rejection or appropriation of novel technologies. In this paper, VR is introduced to a remote San community within a broader community-based research collaboration considering political and ethical perspectives of technology inclusion. The intent was to familiarise the community with the technology through the development and playthrough of a game, to explore future opportunities for joint co-designs of VR applications, meanwhile gauging the barriers for how VR operates outside of its intended setting. The community members expressed their excitement about the experience and the desire to re-create traditional San games in VR. The paper reflects on the community experiences, the setup and use of VR in remote settings, and the choices made to facilitate the familiarization of emerging technology. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Architecture & Media Technology"
            },
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science & Technology",
              "dsl": ""
            }
          ],
          "personId": 43413
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "Khomas",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 43472
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Sceince and Technology",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 43360
        }
      ]
    },
    {
      "id": 43519,
      "typeId": 11696,
      "title": "A call for auditable Virtual, Augmented and Mixed Reality",
      "trackId": 11109,
      "tags": [],
      "keywords": [],
      "abstract": "XR (Virtual, Augmented and Mixed Reality) technologies are growing in prominence. \r\nHowever, such systems present risks, as they are increasingly being used in sectors and in situations that can \r\nresult in harms. As such, this paper argues the need for auditability to become a key consideration of XR systems. Auditability entails capturing information of a system's design and operation to enable its review, oversight, inspection or investigation. Things can and will go wrong, and information that helps unpack situations of failure or harm, and that enables accountability and recourse, will be crucial to XR technology's adoption and acceptance. \r\nIn drawing attention to the urgent need for auditability, we illustrate some risks associated with XR technology and their audit implications, and present some initial findings from a survey with developers indicating the current \"haphazard\" approach towards such concerns. \r\nWe also highlight some challenges and considerations of XR audit in practice, as well as areas of future work for taking this important area of research forward.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science & Technology"
            }
          ],
          "personId": 43433
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science & Technology"
            }
          ],
          "personId": 43414
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science & Technology"
            }
          ],
          "personId": 43449
        }
      ]
    }
  ],
  "people": [
    {
      "id": 43532,
      "firstName": "Robert",
      "lastName": "Teather",
      "affiliations": []
    },
    {
      "id": 43533,
      "firstName": "Daniel",
      "lastName": "Zielasko",
      "affiliations": []
    },
    {
      "id": 43534,
      "firstName": "Misha",
      "lastName": "Sra",
      "affiliations": []
    },
    {
      "id": 43535,
      "firstName": "Kyle",
      "lastName": "Johnsen",
      "affiliations": []
    },
    {
      "id": 43538,
      "firstName": "Stefanie",
      "lastName": "Zollmann",
      "affiliations": []
    },
    {
      "id": 43539,
      "firstName": "Francisco",
      "lastName": "Ortega",
      "affiliations": []
    },
    {
      "id": 43540,
      "firstName": "Torsten",
      "lastName": "Kuhlen",
      "affiliations": []
    },
    {
      "id": 43541,
      "firstName": "Mayra",
      "lastName": "Barrera Machuca",
      "affiliations": []
    },
    {
      "id": 43555,
      "firstName": "mengya",
      "lastName": "zheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43556,
      "firstName": "caroline",
      "lastName": "BAILLARD",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43557,
      "firstName": "Christoph",
      "lastName": "Borst",
      "middleInitial": "W",
      "affiliations": []
    },
    {
      "id": 43558,
      "firstName": "Hyung-jun",
      "lastName": "Cho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43559,
      "firstName": "Jérémy",
      "lastName": "Lacoche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43560,
      "firstName": "Shimmila",
      "lastName": "Bhowmick",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43561,
      "firstName": "Anjali",
      "lastName": "Kaushik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43562,
      "firstName": "Li",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43563,
      "firstName": "Takuya",
      "lastName": "Handa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43564,
      "firstName": "Enes",
      "lastName": "Yigitbas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43565,
      "firstName": "Markus",
      "lastName": "Zank",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43566,
      "firstName": "Maksym",
      "lastName": "Druchok",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43567,
      "firstName": "Alejandro",
      "lastName": "Pinel Martínez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43568,
      "firstName": "Andrea",
      "lastName": "Thorn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43569,
      "firstName": "Huidong",
      "lastName": "Bai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43570,
      "firstName": "Mie",
      "lastName": "Sato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43571,
      "firstName": "Amarnath",
      "lastName": "Murugan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43572,
      "firstName": "Amanda",
      "lastName": "Doherty",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43573,
      "firstName": "Yuzuki",
      "lastName": "Okubo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43574,
      "firstName": "Jovana",
      "lastName": "Lazarevic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43575,
      "firstName": "Guillermo",
      "lastName": "Carbonell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43576,
      "firstName": "xingyu",
      "lastName": "pan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43577,
      "firstName": "Christian",
      "lastName": "Eckhardt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43578,
      "firstName": "Vincent",
      "lastName": "Alleaume",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43579,
      "firstName": "Joe",
      "lastName": "Geigel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43580,
      "firstName": "Andrea",
      "lastName": "Sanna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43581,
      "firstName": "Maximilian",
      "lastName": "Landeck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43582,
      "firstName": "Shunji",
      "lastName": "Muto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43583,
      "firstName": "Unnikrishnan",
      "lastName": "Radhakrishnan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43584,
      "firstName": "Pascal",
      "lastName": "Wüest",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43585,
      "firstName": "Krzysztof",
      "lastName": "Pietroszek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43586,
      "firstName": "Jinsil Hwaryoung",
      "lastName": "Seo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43587,
      "firstName": "Anthony",
      "lastName": "Laurent",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43588,
      "firstName": "Takashi",
      "lastName": "Ijiri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43589,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43590,
      "firstName": "Eric",
      "lastName": "Villain",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43591,
      "firstName": "Gregor",
      "lastName": "Engels",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43592,
      "firstName": "Sebastian",
      "lastName": "Misztal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43593,
      "firstName": "Shamima",
      "lastName": "Yasmin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43594,
      "firstName": "Ran",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43595,
      "firstName": "Harpreet",
      "lastName": "Sareen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43596,
      "firstName": "Tim",
      "lastName": "Sinclair",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43597,
      "firstName": "Weiping",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43598,
      "firstName": "Morteza",
      "lastName": "Malekmakan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43599,
      "firstName": "Jayesh",
      "lastName": "Pillai",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 43600,
      "firstName": "Martin",
      "lastName": "Fischbach",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43601,
      "firstName": "Diego",
      "lastName": "Marti Mason",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43602,
      "firstName": "Konstantinos",
      "lastName": "Koumaditis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43603,
      "firstName": "Kazuma",
      "lastName": "Yoshino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43604,
      "firstName": "Kei",
      "lastName": "Kanari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43605,
      "firstName": "Andrew",
      "lastName": "Yoshimura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43606,
      "firstName": "Juilee",
      "lastName": "Decker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43607,
      "firstName": "Ronja",
      "lastName": "Heinrich",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43608,
      "firstName": "Syed Fawad M.",
      "lastName": "Zaidi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43609,
      "firstName": "Jonathan",
      "lastName": "Ventura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43610,
      "firstName": "Leonardo",
      "lastName": "Stella",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43611,
      "firstName": "Anthony",
      "lastName": "Dickson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43612,
      "firstName": "Yanzhe",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43613,
      "firstName": "Tobias",
      "lastName": "Günther",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43614,
      "firstName": "Nadia Zenati",
      "lastName": "Henda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43615,
      "firstName": "pierrick",
      "lastName": "Jouet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43616,
      "firstName": "Naoki",
      "lastName": "Hashimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43617,
      "firstName": "Rainer",
      "lastName": "Groh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43618,
      "firstName": "Sam",
      "lastName": "Frish",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43619,
      "firstName": "Kensuke",
      "lastName": "Hisatomi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43620,
      "firstName": "Raku",
      "lastName": "Egawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43621,
      "firstName": "Craig",
      "lastName": "McLachlan",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 43622,
      "firstName": "Teemu",
      "lastName": "Kämäräinen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43623,
      "firstName": "Gary",
      "lastName": "Jacobs",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43624,
      "firstName": "Yucheng",
      "lastName": "Qiu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43625,
      "firstName": "Kunal",
      "lastName": "Gupta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43626,
      "firstName": "Ahmet",
      "lastName": "Firintepe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43627,
      "firstName": "Xuanhui",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43628,
      "firstName": "Matti",
      "lastName": "Siekkinen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43629,
      "firstName": "Alain",
      "lastName": "Pagani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43630,
      "firstName": "Stefanie",
      "lastName": "Zollmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43631,
      "firstName": "ELIJAH",
      "lastName": "PINSON",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43632,
      "firstName": "Minas",
      "lastName": "Liarokapis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43633,
      "firstName": "Antti",
      "lastName": "Ylä-Jääski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43634,
      "firstName": "Janis",
      "lastName": "Scholand",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43635,
      "firstName": "Jean-Luc",
      "lastName": "Lugrin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43636,
      "firstName": "Samuel",
      "lastName": "Bittmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43637,
      "firstName": "Robert",
      "lastName": "Richter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43638,
      "firstName": "Matthieu",
      "lastName": "Fradet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43639,
      "firstName": "Gazi Karam",
      "lastName": "Illahi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43640,
      "firstName": "Rishi",
      "lastName": "Vanukuru",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43641,
      "firstName": "Misha",
      "lastName": "Sra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43642,
      "firstName": "Ibrahim",
      "lastName": "Demir",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43643,
      "firstName": "Ahmed",
      "lastName": "Bentaleb",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43644,
      "firstName": "Silian",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43645,
      "firstName": "Jake",
      "lastName": "Guida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43646,
      "firstName": "Clancy",
      "lastName": "Duncan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43647,
      "firstName": "Masaki",
      "lastName": "Kitajima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43648,
      "firstName": "Sebastian",
      "lastName": "von Mammen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43649,
      "firstName": "Gal",
      "lastName": "Gorjup",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43650,
      "firstName": "Fabian",
      "lastName": "Unruh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43651,
      "firstName": "Shixin",
      "lastName": "Jiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43652,
      "firstName": "Jinyuan",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43653,
      "firstName": "Keyur",
      "lastName": "Sorathia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43654,
      "firstName": "Yusuf",
      "lastName": "Sermet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43655,
      "firstName": "Mark",
      "lastName": "Billinghurst",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43656,
      "firstName": "Tao",
      "lastName": "Luo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43657,
      "firstName": "Francesco",
      "lastName": "De Pace",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43658,
      "firstName": "Nils",
      "lastName": "Ganther",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43659,
      "firstName": "Samir",
      "lastName": "BENBELKACEM",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43660,
      "firstName": "Yupeng",
      "lastName": "Hu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43661,
      "firstName": "Chris",
      "lastName": "Zimmerer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43662,
      "firstName": "Kunal",
      "lastName": "Shitut",
      "middleInitial": "Shailesh",
      "affiliations": []
    },
    {
      "id": 43663,
      "firstName": "Andreas",
      "lastName": "Knote",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43664,
      "firstName": "Eun Sun",
      "lastName": "Chu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43665,
      "firstName": "Jonas",
      "lastName": "Schild",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43666,
      "firstName": "Barnabé",
      "lastName": "Soedji",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43667,
      "firstName": "Qi",
      "lastName": "SUN",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43668,
      "firstName": "Hlib",
      "lastName": "Shchur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43669,
      "firstName": "Rohan",
      "lastName": "Bhatia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43670,
      "firstName": "Bernhard",
      "lastName": "Riecke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43671,
      "firstName": "Yunyun",
      "lastName": "Gao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43672,
      "firstName": "Prof. Justin",
      "lastName": "Beilby",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43673,
      "firstName": "Abraham G.",
      "lastName": "Campbell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43674,
      "firstName": "Matej",
      "lastName": "Kapinaj",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43675,
      "firstName": "Hiroyuki",
      "lastName": "Kawakita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43676,
      "firstName": "Didier",
      "lastName": "Stricker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43677,
      "firstName": "Ivan",
      "lastName": "Jovanovikj",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43352,
      "firstName": "Jakob",
      "lastName": "Fehle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43353,
      "firstName": "Christoph",
      "lastName": "Tögel",
      "middleInitial": "Rudolf",
      "affiliations": []
    },
    {
      "id": 43354,
      "firstName": "Mareike",
      "lastName": "Stamer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43355,
      "firstName": "Gopi",
      "lastName": "Meenakshisundaram",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43356,
      "firstName": "Narutoshi",
      "lastName": "Hibino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43357,
      "firstName": "Jiawei",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43358,
      "firstName": "Seulki",
      "lastName": "Min",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43359,
      "firstName": "Johannes",
      "lastName": "Tümler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43360,
      "firstName": "Heike",
      "lastName": "Winschiers-Theophilus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43361,
      "firstName": "Caitlin J.",
      "lastName": "Rinz-Jones",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43362,
      "firstName": "Yannick",
      "lastName": "Rothacher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43363,
      "firstName": "Byeol",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43364,
      "firstName": "Mark",
      "lastName": "McGill",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43365,
      "firstName": "Niels",
      "lastName": "Henze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43366,
      "firstName": "Christian",
      "lastName": "Wolff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43367,
      "firstName": "Jung-geun",
      "lastName": "Moon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43368,
      "firstName": "Florian",
      "lastName": "Müller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43369,
      "firstName": "Dogu",
      "lastName": "Taskiran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43370,
      "firstName": "Sarah",
      "lastName": "Graf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43371,
      "firstName": "Mario",
      "lastName": "Botsch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43372,
      "firstName": "Valentin",
      "lastName": "Schwind",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43373,
      "firstName": "Florian",
      "lastName": "Alt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43374,
      "firstName": "Inwook",
      "lastName": "Hwang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43375,
      "firstName": "Tor-Salve",
      "lastName": "Dalsgaard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43376,
      "firstName": "Jae-In",
      "lastName": "Hwang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43377,
      "firstName": "Marc Erich",
      "lastName": "Latoschik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43378,
      "firstName": "Paige",
      "lastName": "Mass",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43379,
      "firstName": "Sarah",
      "lastName": "Delgado Rodriguez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43380,
      "firstName": "Eduardo",
      "lastName": "Velloso",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43381,
      "firstName": "Myungho",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43382,
      "firstName": "Ian",
      "lastName": "Oakley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43383,
      "firstName": "Evan",
      "lastName": "Suma Rosenberg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43384,
      "firstName": "Jerald",
      "lastName": "Thomas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43385,
      "firstName": "Yun Suen",
      "lastName": "Pai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43386,
      "firstName": "Johannes",
      "lastName": "Schöning",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43387,
      "firstName": "Muhammad Twaha",
      "lastName": "Ibrahim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43388,
      "firstName": "Philipp",
      "lastName": "Schauhuber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43389,
      "firstName": "Courtney",
      "lastName": "Hutton Pospick",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43390,
      "firstName": "Eva",
      "lastName": "Lampen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43391,
      "firstName": "Xintian",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43392,
      "firstName": "Julian",
      "lastName": "Dietz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43393,
      "firstName": "Hyungki",
      "lastName": "Son",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43394,
      "firstName": "Patrick",
      "lastName": "Bourdot",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43395,
      "firstName": "Divine",
      "lastName": "Maloney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43396,
      "firstName": "Colin",
      "lastName": "Jackson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43397,
      "firstName": "Alexander",
      "lastName": "Klippel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43398,
      "firstName": "Chul-Hyun",
      "lastName": "Cho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43399,
      "firstName": "Kai",
      "lastName": "Kunze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43400,
      "firstName": "Bigna",
      "lastName": "Lenggenhager",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43401,
      "firstName": "Ephraim",
      "lastName": "Schott",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43402,
      "firstName": "Jascha",
      "lastName": "Achenbach",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43403,
      "firstName": "Anil Ufuk",
      "lastName": "Batmaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43404,
      "firstName": "Philipp",
      "lastName": "Sykownik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43405,
      "firstName": "Phong",
      "lastName": "Nguyen",
      "middleInitial": "Danh",
      "affiliations": []
    },
    {
      "id": 43406,
      "firstName": "Paul",
      "lastName": "Bittner",
      "middleInitial": "Maximilian",
      "affiliations": []
    },
    {
      "id": 43407,
      "firstName": "Maic",
      "lastName": "Masuch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43408,
      "firstName": "Stephan",
      "lastName": "Wenninger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43409,
      "firstName": "Ken",
      "lastName": "Pfeuffer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43410,
      "firstName": "Adam",
      "lastName": "Jones",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43411,
      "firstName": "Hanseob",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43412,
      "firstName": "Pratham",
      "lastName": "Nar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43413,
      "firstName": "Kasper",
      "lastName": "Rodil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43414,
      "firstName": "Chris",
      "lastName": "Norval",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43415,
      "firstName": "Johanna",
      "lastName": "Pirker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43416,
      "firstName": "Jonathan",
      "lastName": "Sasse",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43417,
      "firstName": "Kasper",
      "lastName": "Hornbæk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43418,
      "firstName": "Marc",
      "lastName": "Kassubeck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43419,
      "firstName": "Saeed",
      "lastName": "Safikhani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43420,
      "firstName": "Axel",
      "lastName": "Krieger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43421,
      "firstName": "Hesham",
      "lastName": "Elsayed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43422,
      "firstName": "Nadine",
      "lastName": "Wagener",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43423,
      "firstName": "Mark",
      "lastName": "Armstrong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43424,
      "firstName": "Sebastian",
      "lastName": "Friston",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 43425,
      "firstName": "Lukas",
      "lastName": "Imbach",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43426,
      "firstName": "Huyen",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43427,
      "firstName": "Keitaro",
      "lastName": "Tsuchiya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43428,
      "firstName": "Juyoung",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43429,
      "firstName": "Yiran",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43430,
      "firstName": "Anh",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43431,
      "firstName": "Yudai",
      "lastName": "Tanaka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43432,
      "firstName": "Cedric",
      "lastName": "Fleury",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43433,
      "firstName": "Richard",
      "lastName": "Cloete",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43434,
      "firstName": "Vesa",
      "lastName": "Krasniqi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43435,
      "firstName": "Lloyd",
      "lastName": "Hollenberg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43436,
      "firstName": "Chih-Fan",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43437,
      "firstName": "Vijay",
      "lastName": "Pawar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43438,
      "firstName": "Christian",
      "lastName": "Schaarschmidt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43439,
      "firstName": "Martin",
      "lastName": "Kocur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43440,
      "firstName": "Jannes",
      "lastName": "Lehwald",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43441,
      "firstName": "Andreas",
      "lastName": "Kunz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43442,
      "firstName": "Samaneh",
      "lastName": "Zamanifard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43443,
      "firstName": "Anthony",
      "lastName": "Steed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43444,
      "firstName": "Andrea",
      "lastName": "Bartl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43445,
      "firstName": "Xiaolong",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43446,
      "firstName": "Karola",
      "lastName": "Marky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43447,
      "firstName": "Lukas",
      "lastName": "Mecke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43448,
      "firstName": "Jong-In",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43449,
      "firstName": "Jat",
      "lastName": "Singh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43450,
      "firstName": "Moritz",
      "lastName": "Kappel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43451,
      "firstName": "Peter",
      "lastName": "Brugger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43452,
      "firstName": "Moritz",
      "lastName": "Muehlhausen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43453,
      "firstName": "Aditi",
      "lastName": "Majumder",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43454,
      "firstName": "Bernd",
      "lastName": "Froehlich",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43455,
      "firstName": "Jan",
      "lastName": "Riemann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43456,
      "firstName": "Feng",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43457,
      "firstName": "Thies",
      "lastName": "Pfeiffer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43458,
      "firstName": "Wolfgang",
      "lastName": "Stuerzlinger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43459,
      "firstName": "Jin Ryong",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43460,
      "firstName": "Hannah",
      "lastName": "Maier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43461,
      "firstName": "Andrii",
      "lastName": "Matviienko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43462,
      "firstName": "Evdokia",
      "lastName": "Efthymiou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43463,
      "firstName": "Yeojin",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43464,
      "firstName": "Andreas",
      "lastName": "Pfaffelhuber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43465,
      "firstName": "David",
      "lastName": "Swapp",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43466,
      "firstName": "Max",
      "lastName": "Mühlhäuser",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43467,
      "firstName": "Mariam",
      "lastName": "Hassib",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43468,
      "firstName": "Hyunjae",
      "lastName": "Gil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43469,
      "firstName": "Mayra",
      "lastName": "Barrera Machuca",
      "middleInitial": "Donaji",
      "affiliations": []
    },
    {
      "id": 43470,
      "firstName": "Marcus",
      "lastName": "Magnor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43471,
      "firstName": "Gerard",
      "lastName": "Kim",
      "middleInitial": "Jounghyun",
      "affiliations": []
    },
    {
      "id": 43472,
      "firstName": "Donovan",
      "lastName": "Maasz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43473,
      "firstName": "Yonghwan",
      "lastName": "Shin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43474,
      "firstName": "Michael",
      "lastName": "Holly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43475,
      "firstName": "Jorge",
      "lastName": "Goncalves",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43476,
      "firstName": "Martin",
      "lastName": "Weigel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43477,
      "firstName": "Paul",
      "lastName": "Asente",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43478,
      "firstName": "Euan",
      "lastName": "Freeman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43479,
      "firstName": "Alexander",
      "lastName": "Zable",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43480,
      "firstName": "TaeHyung",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43481,
      "firstName": "Yue-Hin",
      "lastName": "Loke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43482,
      "firstName": "Xiang 'Anthony'",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43483,
      "firstName": "Nicolas",
      "lastName": "Ladeveze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43484,
      "firstName": "Byungmoon",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43485,
      "firstName": "Laura",
      "lastName": "Olivieri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43486,
      "firstName": "Arata",
      "lastName": "Horie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43487,
      "firstName": "Martin",
      "lastName": "Schmitz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43488,
      "firstName": "Andreas",
      "lastName": "Dengel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43489,
      "firstName": "Jan",
      "lastName": "Gugenheimer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43490,
      "firstName": "David",
      "lastName": "Halbhuber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43491,
      "firstName": "Henning",
      "lastName": "Pohl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43492,
      "firstName": "Benjamin",
      "lastName": "Creel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43493,
      "firstName": "Guo",
      "lastName": "Freeman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43494,
      "firstName": "Susana",
      "lastName": "Castillo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 43495,
      "firstName": "Alexander",
      "lastName": "Kulik",
      "middleInitial": "",
      "affiliations": []
    }
  ]
}