var e=Object.defineProperty,t=Object.getOwnPropertySymbols,a=Object.prototype.hasOwnProperty,i=Object.prototype.propertyIsEnumerable,o=(t,a,i)=>a in t?e(t,a,{enumerable:!0,configurable:!0,writable:!0,value:i}):t[a]=i,s=(e,s)=>{for(var n in s||(s={}))a.call(s,n)&&o(e,n,s[n]);if(t)for(var n of t(s))i.call(s,n)&&o(e,n,s[n]);return e};import{S as n,i as r,s as c,e as l,a as d,b as h,c as u,d as p,f as m,g,h as f,j as w,k as y,l as v,n as b,r as k,m as T,t as S,o as D,p as I,q as P,w as A,u as x,v as R,x as M,y as C,z as U,A as j,B as z,C as q,D as H,E as W,F as L,G as O,H as E,I as F,J as N,K as V,L as B,M as K,N as Y,O as G,P as J,Q as Z,R as Q,T as X,U as _,V as $}from"./vendor.476c8803.js";function ee(e){let t,a,i,o,s,n,r;return{c(){t=l("label"),a=l("input"),i=d(),o=h("svg"),s=h("path"),this.h()},l(e){t=u(e,"LABEL",{class:!0});var n=p(t);a=u(n,"INPUT",{type:!0,class:!0}),i=m(n),o=u(n,"svg",{class:!0,xmlns:!0,viewBox:!0,"aria-label":!0},1);var r=p(o);s=u(r,"path",{class:!0,d:!0},1),p(s).forEach(g),r.forEach(g),n.forEach(g),this.h()},h(){f(a,"type","checkbox"),f(a,"class","svelte-zmhaev"),f(s,"class","star svelte-zmhaev"),f(s,"d","m55,237 74-228 74,228L9,96h240"),f(o,"class","star-img svelte-zmhaev"),f(o,"xmlns","http://www.w3.org/2000/svg"),f(o,"viewBox","0 0 260 245"),f(o,"aria-label","Star this item"),f(t,"class","star-item svelte-zmhaev")},m(c,l){w(c,t,l),y(t,a),a.checked=e[0],y(t,i),y(t,o),y(o,s),n||(r=[v(a,"change",e[2]),v(a,"change",e[1])],n=!0)},p(e,[t]){1&t&&(a.checked=e[0])},i:b,o:b,d(e){e&&g(t),n=!1,k(r)}}}function te(e,t,a){let{starred:i=!1}=t;const o=T();return e.$$set=e=>{"starred"in e&&a(0,i=e.starred)},[i,function(){o("star",{})},function(){i=this.checked,a(0,i)}]}class ae extends n{constructor(e){super(),r(this,e,te,ee,c,{starred:0})}}function ie(e){let t,a=void 0!==e[2]&&function(e){let t,a,i,o,s,n,r,c,b,k,T,P,A;return{c(){t=l("a"),a=l("figure"),i=l("img"),s=d(),n=l("figcaption"),r=h("svg"),c=h("polygon"),b=d(),k=l("span"),T=S(e[1]),this.h()},l(o){t=u(o,"A",{href:!0,"aria-label":!0,class:!0});var l=p(t);a=u(l,"FIGURE",{class:!0});var d=p(a);i=u(d,"IMG",{alt:!0,src:!0,class:!0}),s=m(d),n=u(d,"FIGCAPTION",{class:!0});var h=p(n);r=u(h,"svg",{"aria-label":!0,xmlns:!0,x:!0,y:!0,viewBox:!0,class:!0},1);var f=p(r);c=u(f,"polygon",{fill:!0,points:!0,class:!0},1),p(c).forEach(g),f.forEach(g),b=m(h),k=u(h,"SPAN",{class:!0});var w=p(k);T=D(w,e[1]),w.forEach(g),h.forEach(g),d.forEach(g),l.forEach(g),this.h()},h(){f(i,"alt",e[1]),i.src!==(o=e[2])&&f(i,"src",o),f(i,"class","svelte-1b1tco5"),f(c,"fill","#ffffff"),f(c,"points","0,0 0,600 600,300 "),f(c,"class","svelte-1b1tco5"),f(r,"aria-label",e[1]),f(r,"xmlns","http://www.w3.org/2000/svg"),f(r,"x","0px"),f(r,"y","0px"),f(r,"viewBox","-600 0 1800 600"),f(r,"class","svelte-1b1tco5"),f(k,"class","svelte-1b1tco5"),f(n,"class","svelte-1b1tco5"),f(a,"class","svelte-1b1tco5"),f(t,"href",e[0]),f(t,"aria-label",e[1]),f(t,"class","svelte-1b1tco5")},m(o,l){w(o,t,l),y(t,a),y(a,i),y(a,s),y(a,n),y(n,r),y(r,c),y(n,b),y(n,k),y(k,T),P||(A=v(t,"click",e[4]),P=!0)},p(e,a){2&a&&f(i,"alt",e[1]),2&a&&f(r,"aria-label",e[1]),2&a&&I(T,e[1]),1&a&&f(t,"href",e[0]),2&a&&f(t,"aria-label",e[1])},d(e){e&&g(t),P=!1,A()}}}(e);return{c(){a&&a.c(),t=P()},l(e){a&&a.l(e),t=P()},m(e,i){a&&a.m(e,i),w(e,t,i)},p(e,[t]){void 0!==e[2]&&a.p(e,t)},i:b,o:b,d(e){a&&a.d(e),e&&g(t)}}}function oe(e,t,a){let{url:i}=t,{name:o=""}=t;const s=i&&i.split("v=")[1],n=s&&`https://img.youtube.com/vi/${s}/mqdefault.jpg`,r=T();function c(){r("watch",{ytid:s})}return e.$$set=e=>{"url"in e&&a(0,i=e.url),"name"in e&&a(1,o=e.name)},[i,o,n,c,e=>{e.preventDefault(),c()}]}class se extends n{constructor(e){super(),r(this,e,oe,ie,c,{url:0,name:1})}}const ne=function(e){const{subscribe:t,update:a}=A(e);return{subscribe:t,toggle(e){a((([...t])=>{const a=t.indexOf(e);return a>-1?t.splice(a,1):t.push(e),t}))}}}("undefined"!=typeof window&&JSON.parse(window.localStorage.getItem("sigchi-gallery-stars"))||[]);"undefined"!=typeof window&&ne.subscribe((e=>{const t=JSON.stringify(e);window.localStorage.setItem("sigchi-gallery-stars",t)}));function re(e,t,a){const i=e.slice();return i[10]=t[a][0],i[11]=t[a][1],i}function ce(e,t,a){const i=e.slice();return i[10]=t[a][0],i[11]=t[a][1],i}function le(e){let t,a,i,o=e[5],s=[];for(let c=0;c<o.length;c+=1)s[c]=de(ce(e,o,c));const n=e=>U(s[e],1,1,(()=>{s[e]=null}));let r=0===e[5].length&&function(e){let t,a=e[6],i=[];for(let o=0;o<a.length;o+=1)i[o]=he(re(e,a,o));return{c(){for(let e=0;e<i.length;e+=1)i[e].c();t=P()},l(e){for(let t=0;t<i.length;t+=1)i[t].l(e);t=P()},m(e,a){for(let t=0;t<i.length;t+=1)i[t].m(e,a);w(e,t,a)},p(e,o){if(64&o){let s;for(a=e[6],s=0;s<a.length;s+=1){const n=re(e,a,s);i[s]?i[s].p(n,o):(i[s]=he(n),i[s].c(),i[s].m(t.parentNode,t))}for(;s<i.length;s+=1)i[s].d(1);i.length=a.length}},d(e){H(i,e),e&&g(t)}}}(e);return{c(){t=l("div");for(let e=0;e<s.length;e+=1)s[e].c();a=d(),r&&r.c(),this.h()},l(e){t=u(e,"DIV",{class:!0});var i=p(t);for(let t=0;t<s.length;t+=1)s[t].l(i);a=m(i),r&&r.l(i),i.forEach(g),this.h()},h(){f(t,"class","svelte-1au7wbi")},m(e,o){w(e,t,o);for(let a=0;a<s.length;a+=1)s[a].m(t,null);y(t,a),r&&r.m(t,null),i=!0},p(e,i){if(32&i){let r;for(o=e[5],r=0;r<o.length;r+=1){const n=ce(e,o,r);s[r]?(s[r].p(n,i),x(s[r],1)):(s[r]=de(n),s[r].c(),x(s[r],1),s[r].m(t,a))}for(z(),r=o.length;r<s.length;r+=1)n(r);q()}0===e[5].length&&r.p(e,i)},i(e){if(!i){for(let e=0;e<o.length;e+=1)x(s[e]);i=!0}},o(e){s=s.filter(Boolean);for(let t=0;t<s.length;t+=1)U(s[t]);i=!1},d(e){e&&g(t),H(s,e),r&&r.d()}}}function de(e){let t,a;return t=new se({props:{name:e[10],url:e[11]}}),t.$on("watch",e[9]),{c(){R(t.$$.fragment)},l(e){M(t.$$.fragment,e)},m(e,i){C(t,e,i),a=!0},p:b,i(e){a||(x(t.$$.fragment,e),a=!0)},o(e){U(t.$$.fragment,e),a=!1},d(e){j(t,e)}}}function he(e){let t,a;return{c(){t=l("img"),this.h()},l(e){t=u(e,"IMG",{alt:!0,src:!0,class:!0}),this.h()},h(){f(t,"alt",e[10]),t.src!==(a=e[11])&&f(t,"src",a),f(t,"class","svelte-1au7wbi")},m(e,a){w(e,t,a)},p:b,d(e){e&&g(t)}}}function ue(e){let t,a,i,o,s,n,r=e[0].abstract+"";return{c(){t=l("aside"),a=l("h4"),i=S("Abstract"),o=d(),s=l("p"),n=S(r),this.h()},l(e){t=u(e,"ASIDE",{class:!0});var c=p(t);a=u(c,"H4",{});var l=p(a);i=D(l,"Abstract"),l.forEach(g),o=m(c),s=u(c,"P",{});var d=p(s);n=D(d,r),d.forEach(g),c.forEach(g),this.h()},h(){f(t,"class","svelte-1au7wbi")},m(e,r){w(e,t,r),y(t,a),y(a,i),y(t,o),y(t,s),y(s,n)},p(e,t){1&t&&r!==(r=e[0].abstract+"")&&I(n,r)},d(e){e&&g(t)}}}function pe(e){let t,a,i,o,s,n,r,c,h,v,b,k,T,P,A,H,W,L,O,E,F,N,V,B,K,Y,G=e[0].track+"",J=e[0].sequence+"",Z=e[0].title+"",Q=e[0].authors.join(", ")+"",X=e[0].affiliations.join(" | ")+"";a=new ae({props:{starred:e[2]}}),a.$on("star",e[8]);let _="list"!==e[1]&&le(e),$="detail"===e[1]&&ue(e);return{c(){t=l("article"),R(a.$$.fragment),i=d(),o=l("section"),s=l("h3"),n=S(G),r=S(" #"),c=S(J),h=S(":"),v=d(),b=l("p"),k=S(e[3]),T=d(),P=l("h3"),A=S(Z),H=d(),W=l("p"),L=S(Q),O=d(),E=l("p"),F=S(X),N=d(),V=l("p"),B=d(),_&&_.c(),K=d(),$&&$.c(),this.h()},l(l){t=u(l,"ARTICLE",{class:!0});var d=p(t);M(a.$$.fragment,d),i=m(d),o=u(d,"SECTION",{class:!0});var f=p(o);s=u(f,"H3",{class:!0});var w=p(s);n=D(w,G),r=D(w," #"),c=D(w,J),h=D(w,":"),w.forEach(g),v=m(f),b=u(f,"P",{class:!0});var y=p(b);k=D(y,e[3]),y.forEach(g),T=m(f),P=u(f,"H3",{class:!0});var S=p(P);A=D(S,Z),S.forEach(g),H=m(f),W=u(f,"P",{class:!0});var I=p(W);L=D(I,Q),I.forEach(g),O=m(f),E=u(f,"P",{class:!0});var x=p(E);F=D(x,X),x.forEach(g),N=m(f),V=u(f,"P",{class:!0}),p(V).forEach(g),B=m(f),_&&_.l(f),f.forEach(g),K=m(d),$&&$.l(d),d.forEach(g),this.h()},h(){f(s,"class","svelte-1au7wbi"),f(b,"class","svelte-1au7wbi"),f(P,"class","svelte-1au7wbi"),f(W,"class","svelte-1au7wbi"),f(E,"class","svelte-1au7wbi"),f(V,"class","svelte-1au7wbi"),f(o,"class","svelte-1au7wbi"),f(t,"class","svelte-1au7wbi")},m(l,d){w(l,t,d),C(a,t,null),y(t,i),y(t,o),y(o,s),y(s,n),y(s,r),y(s,c),y(s,h),y(o,v),y(o,b),y(b,k),y(o,T),y(o,P),y(P,A),y(o,H),y(o,W),y(W,L),y(o,O),y(o,E),y(E,F),y(o,N),y(o,V),V.innerHTML=e[4],y(o,B),_&&_.m(o,null),y(t,K),$&&$.m(t,null),Y=!0},p(e,[i]){const s={};4&i&&(s.starred=e[2]),a.$set(s),(!Y||1&i)&&G!==(G=e[0].track+"")&&I(n,G),(!Y||1&i)&&J!==(J=e[0].sequence+"")&&I(c,J),(!Y||1&i)&&Z!==(Z=e[0].title+"")&&I(A,Z),(!Y||1&i)&&Q!==(Q=e[0].authors.join(", ")+"")&&I(L,Q),(!Y||1&i)&&X!==(X=e[0].affiliations.join(" | ")+"")&&I(F,X),"list"!==e[1]?_?(_.p(e,i),2&i&&x(_,1)):(_=le(e),_.c(),x(_,1),_.m(o,null)):_&&(z(),U(_,1,1,(()=>{_=null})),q()),"detail"===e[1]?$?$.p(e,i):($=ue(e),$.c(),$.m(t,null)):$&&($.d(1),$=null)},i(e){Y||(x(a.$$.fragment,e),x(_),Y=!0)},o(e){U(a.$$.fragment,e),U(_),Y=!1},d(e){e&&g(t),j(a),_&&_.d(),$&&$.d()}}}function me(e,t,a){let{content:i={}}=t,{sessions:o={}}=t,{mode:s="list"}=t,{starred:n=!1}=t;const r=o[i.session[0]].name,c=Object.entries(i.links).map((([e,t])=>`<a href="${t}">${e}</a>`)).join(" | "),l=i.videos&&Object.entries(i.videos)||[],d=i.images&&Object.entries(i.images)||[];return e.$$set=e=>{"content"in e&&a(0,i=e.content),"sessions"in e&&a(7,o=e.sessions),"mode"in e&&a(1,s=e.mode),"starred"in e&&a(2,n=e.starred)},[i,s,n,r,c,l,d,o,function(t){W(e,t)},function(t){W(e,t)}]}class ge extends n{constructor(e){super(),r(this,e,me,pe,c,{content:0,sessions:7,mode:1,starred:2})}}function fe(e,t){return t.split(".").reduce(((e,t)=>e[t]),e)}function we([...e]){for(let t=e.length-1;t>0;t--){const a=Math.floor(Math.random()*(t+1)),i=e[t];e[t]=e[a],e[a]=i}return e}function ye(e,t,a){const i=e.slice();return i[11]=t[a],i}function ve(e){let t,a=e[3],i=[];for(let o=0;o<a.length;o+=1)i[o]=be(ye(e,a,o));return{c(){for(let e=0;e<i.length;e+=1)i[e].c();t=P()},l(e){for(let t=0;t<i.length;t+=1)i[t].l(e);t=P()},m(e,a){for(let t=0;t<i.length;t+=1)i[t].m(e,a);w(e,t,a)},p(e,o){if(10&o){let s;for(a=e[3],s=0;s<a.length;s+=1){const n=ye(e,a,s);i[s]?i[s].p(n,o):(i[s]=be(n),i[s].c(),i[s].m(t.parentNode,t))}for(;s<i.length;s+=1)i[s].d(1);i.length=a.length}},d(e){H(i,e),e&&g(t)}}}function be(e){let t,a;return{c(){t=l("option"),this.h()},l(e){t=u(e,"OPTION",{value:!0}),p(t).forEach(g),this.h()},h(){t.__value=a=e[1]?fe(e[11],e[1]):e[11],t.value=t.__value},m(e,a){w(e,t,a)},p(e,i){10&i&&a!==(a=e[1]?fe(e[11],e[1]):e[11])&&(t.__value=a,t.value=t.__value)},d(e){e&&g(t)}}}function ke(e){let t,a,i,o,s,n=void 0!==e[3]&&ve(e);return{c(){t=l("input"),a=d(),i=l("datalist"),n&&n.c(),this.h()},l(e){t=u(e,"INPUT",{type:!0,"aria-label":!0,placeholder:!0,list:!0}),a=m(e),i=u(e,"DATALIST",{id:!0});var o=p(i);n&&n.l(o),o.forEach(g),this.h()},h(){f(t,"type","text"),f(t,"aria-label",e[0]),f(t,"placeholder",e[0]),f(t,"list","typeahead"),f(i,"id","typeahead")},m(r,c){w(r,t,c),L(t,e[2]),w(r,a,c),w(r,i,c),n&&n.m(i,null),o||(s=v(t,"input",e[9]),o=!0)},p(e,[a]){1&a&&f(t,"aria-label",e[0]),1&a&&f(t,"placeholder",e[0]),4&a&&t.value!==e[2]&&L(t,e[2]),void 0!==e[3]?n?n.p(e,a):(n=ve(e),n.c(),n.m(i,null)):n&&(n.d(1),n=null)},i:b,o:b,d(e){e&&g(t),e&&g(a),e&&g(i),n&&n.d(),o=!1,s()}}}function Te(e,t,a){let i,o,n,{placeholder:r="Search"}=t,{candidates:c=[]}=t,{key:l=""}=t,{results:d=(e=>e)}=t,{selected:h}=t,u="";const p={includeScore:!0};return e.$$set=e=>{"placeholder"in e&&a(0,r=e.placeholder),"candidates"in e&&a(5,c=e.candidates),"key"in e&&a(1,l=e.key),"results"in e&&a(6,d=e.results),"selected"in e&&a(4,h=e.selected)},e.$$.update=()=>{if(2&e.$$.dirty&&a(7,i=s(s({},p),l?{keys:[l]}:{})),160&e.$$.dirty&&a(8,o=new O(c,i)),460&e.$$.dirty)if(u){const e=o.search(u,i),t=e.filter((e=>e.score<.1)).map((e=>e.item));a(3,n=e.map((e=>e.item))),a(4,h=t.length>0?d(t):d(n))}else a(3,n=a(4,h=void 0))},[r,l,u,n,h,c,d,i,o,function(){u=this.value,a(2,u)}]}class Se extends n{constructor(e){super(),r(this,e,Te,ke,c,{placeholder:0,candidates:5,key:1,results:6,selected:4})}}function De(e){let t,a,i,o,s,n,r,c,d;return{c(){t=l("div"),a=l("div"),i=l("iframe"),this.h()},l(e){t=u(e,"DIV",{class:!0});var o=p(t);a=u(o,"DIV",{class:!0});var s=p(a);i=u(s,"IFRAME",{title:!0,src:!0,frameborder:!0,allow:!0,allowfullscreen:!0,class:!0}),p(i).forEach(g),s.forEach(g),o.forEach(g),this.h()},h(){f(i,"title","video-playback"),i.src!==(o="https://www.youtube.com/embed/"+e[0])&&f(i,"src",o),f(i,"frameborder","0"),f(i,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"),i.allowFullscreen=!0,f(i,"class","svelte-jfuj7j"),f(a,"class","svelte-jfuj7j"),f(t,"class","modal-bg svelte-jfuj7j")},m(o,s){w(o,t,s),y(t,a),y(a,i),r=!0,c||(d=v(t,"click",e[1]),c=!0)},p(e,[t]){(!r||1&t&&i.src!==(o="https://www.youtube.com/embed/"+e[0]))&&f(i,"src",o)},i(e){r||(E((()=>{s||(s=F(a,N,{y:-200},!0)),s.run(1)})),E((()=>{n||(n=F(t,V,{},!0)),n.run(1)})),r=!0)},o(e){s||(s=F(a,N,{y:-200},!1)),s.run(0),n||(n=F(t,V,{},!1)),n.run(0),r=!1},d(e){e&&g(t),e&&s&&s.end(),e&&n&&n.end(),c=!1,d()}}}function Ie(e,t,a){let{watchId:i}=t;return e.$$set=e=>{"watchId"in e&&a(0,i=e.watchId)},[i,function(t){W(e,t)}]}class Pe extends n{constructor(e){super(),r(this,e,Ie,De,c,{watchId:0})}}function Ae(e,t,a){const i=e.slice();return i[26]=t[a],i}function xe(e,t,a){const i=e.slice();return i[29]=t[a],i}function Re(e,t,a){const i=e.slice();return i[32]=t[a],i}function Me(e,t,a){const i=e.slice();return i[35]=t[a],i}function Ce(e){let t,a,i,o,s,n,r,c,h=e[35]+"";return{c(){t=l("label"),a=l("input"),o=d(),s=S(h),n=d(),this.h()},l(e){t=u(e,"LABEL",{});var i=p(t);a=u(i,"INPUT",{type:!0,value:!0}),o=m(i),s=D(i,h),n=m(i),i.forEach(g),this.h()},h(){f(a,"type","checkbox"),a.__value=i=e[35],a.value=a.__value,e[17][0].push(a)},m(i,l){w(i,t,l),y(t,a),a.checked=~e[2].indexOf(a.__value),y(t,o),y(t,s),y(t,n),r||(c=v(a,"change",e[16]),r=!0)},p(e,t){1&t[0]&&i!==(i=e[35])&&(a.__value=i,a.value=a.__value),4&t[0]&&(a.checked=~e[2].indexOf(a.__value)),1&t[0]&&h!==(h=e[35]+"")&&I(s,h)},d(i){i&&g(t),e[17][0].splice(e[17][0].indexOf(a),1),r=!1,c()}}}function Ue(e){let t,a,i,o,s,n,r,c=e[32]+"";return{c(){t=l("label"),a=l("input"),i=d(),o=S(c),s=d(),this.h()},l(e){t=u(e,"LABEL",{});var n=p(t);a=u(n,"INPUT",{type:!0,value:!0}),i=m(n),o=D(n,c),s=m(n),n.forEach(g),this.h()},h(){f(a,"type","radio"),a.__value=e[32],a.value=a.__value,e[17][1].push(a)},m(c,l){w(c,t,l),y(t,a),a.checked=a.__value===e[3],y(t,i),y(t,o),y(t,s),n||(r=v(a,"change",e[19]),n=!0)},p(e,t){8&t[0]&&(a.checked=a.__value===e[3])},d(i){i&&g(t),e[17][1].splice(e[17][1].indexOf(a),1),n=!1,r()}}}function je(e){let t,a,i,o,s,n,r,c=e[29]+"";return{c(){t=l("label"),a=l("input"),i=d(),o=S(c),s=d(),this.h()},l(e){t=u(e,"LABEL",{});var n=p(t);a=u(n,"INPUT",{type:!0,value:!0}),i=m(n),o=D(n,c),s=m(n),n.forEach(g),this.h()},h(){f(a,"type","radio"),a.__value=e[29],a.value=a.__value,e[17][2].push(a)},m(c,l){w(c,t,l),y(t,a),a.checked=a.__value===e[7],y(t,i),y(t,o),y(t,s),n||(r=v(a,"change",e[21]),n=!0)},p(e,t){128&t[0]&&(a.checked=a.__value===e[7])},d(i){i&&g(t),e[17][2].splice(e[17][2].indexOf(a),1),n=!1,r()}}}function ze(e,t){let a,i,o;return i=new ge({props:{content:t[26],mode:t[7],sessions:t[0].sessions,starred:t[6].includes(t[26].id)}}),i.$on("star",(function(){return t[22](t[26])})),i.$on("watch",t[23]),{key:e,first:null,c(){a=P(),R(i.$$.fragment),this.h()},l(e){a=P(),M(i.$$.fragment,e),this.h()},h(){this.first=a},m(e,t){w(e,a,t),C(i,e,t),o=!0},p(e,a){t=e;const o={};256&a[0]&&(o.content=t[26]),128&a[0]&&(o.mode=t[7]),1&a[0]&&(o.sessions=t[0].sessions),320&a[0]&&(o.starred=t[6].includes(t[26].id)),i.$set(o)},i(e){o||(x(i.$$.fragment,e),o=!0)},o(e){U(i.$$.fragment,e),o=!1},d(e){e&&g(a),j(i,e)}}}function qe(e){let t,a;return t=new Pe({props:{watchId:e[9]}}),t.$on("click",e[24]),{c(){R(t.$$.fragment)},l(e){M(t.$$.fragment,e)},m(e,i){C(t,e,i),a=!0},p(e,a){const i={};512&a[0]&&(i.watchId=e[9]),t.$set(i)},i(e){a||(x(t.$$.fragment,e),a=!0)},o(e){U(t.$$.fragment,e),a=!1},d(e){j(t,e)}}}function He(e){let t,a,i,o,s,n,r,c,h,b,T,I,A,W,L,O,E,F,N,V,J,$,ee,te,ae,ie,oe,se,ne,re,ce,le,de,he,ue,pe=[],me=new Map,ge=e[0].tracks,fe=[];for(let l=0;l<ge.length;l+=1)fe[l]=Ce(Me(e,ge,l));const we=[e[10]];function ye(t){e[18](t)}let ve={};for(let l=0;l<we.length;l+=1)ve=B(ve,we[l]);void 0!==e[4]&&(ve.selected=e[4]),L=new Se({props:ve}),Z.push((()=>Q(L,"selected",ye)));let be=e[11],ke=[];for(let l=0;l<be.length;l+=1)ke[l]=Ue(Re(e,be,l));let Te=e[12],De=[];for(let l=0;l<Te.length;l+=1)De[l]=je(xe(e,Te,l));let Ie=e[8];const Pe=e=>e[26].id;for(let l=0;l<Ie.length;l+=1){let t=Ae(e,Ie,l),a=Pe(t);me.set(a,pe[l]=ze(a,t))}let He=e[9]&&qe(e);return{c(){t=l("form"),a=l("fieldset"),i=l("legend"),o=S("Categories"),s=d(),n=l("label"),r=l("input"),c=S("\n      Starred"),h=d();for(let e=0;e<fe.length;e+=1)fe[e].c();b=d(),T=l("fieldset"),I=l("legend"),A=S("Filtering"),W=d(),R(L.$$.fragment),E=d();for(let e=0;e<ke.length;e+=1)ke[e].c();F=d(),N=l("fieldset"),V=l("legend"),J=S("View"),$=d(),ee=l("button"),te=S("shuffle"),ae=d(),ie=l("button"),oe=S("ending soon"),se=d();for(let e=0;e<De.length;e+=1)De[e].c();ne=d(),re=l("div");for(let e=0;e<pe.length;e+=1)pe[e].c();ce=d(),He&&He.c(),le=P(),this.h()},l(e){t=u(e,"FORM",{class:!0});var l=p(t);a=u(l,"FIELDSET",{class:!0});var d=p(a);i=u(d,"LEGEND",{});var f=p(i);o=D(f,"Categories"),f.forEach(g),s=m(d),n=u(d,"LABEL",{});var w=p(n);r=u(w,"INPUT",{type:!0}),c=D(w,"\n      Starred"),w.forEach(g),h=m(d);for(let t=0;t<fe.length;t+=1)fe[t].l(d);d.forEach(g),b=m(l),T=u(l,"FIELDSET",{class:!0});var y=p(T);I=u(y,"LEGEND",{});var v=p(I);A=D(v,"Filtering"),v.forEach(g),W=m(y),M(L.$$.fragment,y),E=m(y);for(let t=0;t<ke.length;t+=1)ke[t].l(y);y.forEach(g),F=m(l),N=u(l,"FIELDSET",{class:!0});var k=p(N);V=u(k,"LEGEND",{});var S=p(V);J=D(S,"View"),S.forEach(g),$=m(k),ee=u(k,"BUTTON",{});var x=p(ee);te=D(x,"shuffle"),x.forEach(g),ae=m(k),ie=u(k,"BUTTON",{});var R=p(ie);oe=D(R,"ending soon"),R.forEach(g),se=m(k);for(let t=0;t<De.length;t+=1)De[t].l(k);k.forEach(g),l.forEach(g),ne=m(e),re=u(e,"DIV",{class:!0});var C=p(re);for(let t=0;t<pe.length;t+=1)pe[t].l(C);C.forEach(g),ce=m(e),He&&He.l(e),le=P(),this.h()},h(){f(r,"type","checkbox"),f(a,"class","svelte-10fq890"),f(T,"class","svelte-10fq890"),f(N,"class","svelte-10fq890"),f(t,"class","svelte-10fq890"),f(re,"class","sigchi-gallery-root")},m(l,d){w(l,t,d),y(t,a),y(a,i),y(i,o),y(a,s),y(a,n),y(n,r),r.checked=e[1],y(n,c),y(a,h);for(let e=0;e<fe.length;e+=1)fe[e].m(a,null);y(t,b),y(t,T),y(T,I),y(I,A),y(T,W),C(L,T,null),y(T,E);for(let e=0;e<ke.length;e+=1)ke[e].m(T,null);y(t,F),y(t,N),y(N,V),y(V,J),y(N,$),y(N,ee),y(ee,te),y(N,ae),y(N,ie),y(ie,oe),y(N,se);for(let e=0;e<De.length;e+=1)De[e].m(N,null);w(l,ne,d),w(l,re,d);for(let e=0;e<pe.length;e+=1)pe[e].m(re,null);w(l,ce,d),He&&He.m(l,d),w(l,le,d),de=!0,he||(ue=[v(r,"change",e[15]),v(ee,"click",e[20]),v(ie,"click",e[13]),v(t,"submit",We)],he=!0)},p(e,t){if(2&t[0]&&(r.checked=e[1]),5&t[0]){let i;for(ge=e[0].tracks,i=0;i<ge.length;i+=1){const o=Me(e,ge,i);fe[i]?fe[i].p(o,t):(fe[i]=Ce(o),fe[i].c(),fe[i].m(a,null))}for(;i<fe.length;i+=1)fe[i].d(1);fe.length=ge.length}const i=1024&t[0]?K(we,[Y(e[10])]):{};if(!O&&16&t[0]&&(O=!0,i.selected=e[4],X((()=>O=!1))),L.$set(i),2056&t[0]){let a;for(be=e[11],a=0;a<be.length;a+=1){const i=Re(e,be,a);ke[a]?ke[a].p(i,t):(ke[a]=Ue(i),ke[a].c(),ke[a].m(T,null))}for(;a<ke.length;a+=1)ke[a].d(1);ke.length=be.length}if(4224&t[0]){let a;for(Te=e[12],a=0;a<Te.length;a+=1){const i=xe(e,Te,a);De[a]?De[a].p(i,t):(De[a]=je(i),De[a].c(),De[a].m(N,null))}for(;a<De.length;a+=1)De[a].d(1);De.length=Te.length}961&t[0]&&(Ie=e[8],z(),pe=G(pe,t,Pe,1,e,Ie,me,re,_,ze,null,Ae),q()),e[9]?He?(He.p(e,t),512&t[0]&&x(He,1)):(He=qe(e),He.c(),x(He,1),He.m(le.parentNode,le)):He&&(z(),U(He,1,1,(()=>{He=null})),q())},i(e){if(!de){x(L.$$.fragment,e);for(let e=0;e<Ie.length;e+=1)x(pe[e]);x(He),de=!0}},o(e){U(L.$$.fragment,e);for(let t=0;t<pe.length;t+=1)U(pe[t]);U(He),de=!1},d(e){e&&g(t),H(fe,e),j(L),H(ke,e),H(De,e),e&&g(ne),e&&g(re);for(let t=0;t<pe.length;t+=1)pe[t].d();e&&g(ce),He&&He.d(e),e&&g(le),he=!1,k(ue)}}}const We=e=>{e.preventDefault()};function Le(e,t,a){let i,o,s;J(e,ne,(e=>a(6,s=e)));let{program:n={sessions:{},contents:[],authors:[],tracks:[]}}=t;const r=["title","session","abstract","author"],c=["list","compact","detail"],l=Object.entries(n.sessions);let d,h,u=!1,p=[...n.tracks],m=r[0],g=c[1],f=we(n.contents),w=[];const y=[[],[],[]];return e.$$set=e=>{"program"in e&&a(0,n=e.program)},e.$$.update=()=>{102&e.$$.dirty[0]&&a(14,i=u?f.filter((e=>s.includes(e.id)&&p.includes(e.track))):f.filter((e=>p.includes(e.track)))),16393&e.$$.dirty[0]&&a(10,o=function(e,t,a,i){switch(e){case"title":return{candidates:t,key:"title",results:e=>e};case"abstract":return{candidates:t,key:"abstract",results:e=>e};case"session":return{candidates:i,key:"1.name",results(e){const a=e.map((e=>Number(e[0])));return t.filter((e=>a.includes(e.session[0])))}};case"author":return{candidates:a,results:e=>t.filter((t=>t.authors.some((t=>e.includes(t)))))}}}(m,i,n.authors,l)),16400&e.$$.dirty[0]&&a(8,w=d||i)},[n,u,p,m,d,f,s,g,w,h,o,r,c,function(){const e=Date.now();a(5,f=[...n.contents].sort(((t,a)=>{const[i,o]=[t,a].map((t=>{const a=Math.min(...t.session.map((e=>n.sessions[e]?n.sessions[e].end:1/0)));return"number"==typeof a?e>a?1/0:a:1/0}));return i-o||t.id-a.id})))},i,function(){u=this.checked,a(1,u)},function(){p=$(y[0],this.__value,this.checked),a(2,p)},y,function(e){d=e,a(4,d)},function(){m=this.__value,a(3,m)},()=>{a(5,f=we(f))},function(){g=this.__value,a(7,g)},e=>ne.toggle(e.id),e=>{a(9,h=e.detail.ytid)},()=>{a(9,h=void 0)}]}var Oe={tracks:["Paper","Demo","Poster","SIC","DC"],contents:[{id:42024,sequence:1,track:"Paper",session:[42218],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415833"},authors:["Axel Antoine","Mathieu Nancel","Ella Ge","Jingjie Zheng","Navid Zolghadr","Géry Casiez"],affiliations:["Université de Lille","Inria Lille - Nord Europe","Google","Inria"],title:"Modeling and Reducing Spatial Jitter caused by Asynchronous Input and Output Rates",keywords:[],abstract:"Jitter in interactive systems occurs when visual feedback is perceived as unstable or trembling even though the input signal is smooth or stationary. It can have multiple causes such as sensing noise, or feedback calculations introducing or exacerbating sensing imprecisions. Jitter can however occur even when each individual component of the pipeline works perfectly, as a result of the differences between the input frequency and the display refresh rate. This asynchronicity can introduce rapidly-shifting latencies between the rendered feedbacks and their display on screen, which can result in trembling cursors or viewports. This paper contributes a better understanding of this particular type of jitter. We first detail the problem from a mathematical standpoint, from which we develop a predictive model of jitter amplitude as a function of input and output frequencies, and a new metric to measure this spatial jitter. Using touch input data gathered in a study, we developed a simulator to validate this model and to assess the effects of different techniques and settings with any output frequency. The most promising approach, when the time of the next display refresh is known, is to estimate (interpolate or extrapolate) the user’s position at a fixed time interval before that refresh. When input events occur at 125 Hz, as is common in touch screens, we show that an interval of 4 to 6 ms works well for a wide range of display refresh rates. This method effectively cancels most of the jitter introduced by input/output asynchronicity, while introducing minimal imprecision or latency.",qa:"discord://discordapp.com/channels/734723053036109904/764103693175357461",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=XNKE3f2-gnQ","Short Talk":"https://www.youtube.com/watch?v=j0GBhVKsD7U"},images:{}},{id:42025,sequence:2,track:"Paper",session:[42227],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415872"},authors:["Andrew Schoen","Curt Henrichs","Mathias Strohkirch","Bilge Mutlu"],affiliations:["University of Wisconsin-Madison","University of Wisconsin - Madison"],title:"Authr: A Task Authoring Environment for Human-Robot Teams",keywords:[],abstract:"Collaborative robots promise to transform work across many industries and promote “human-robot teaming” as a novel paradigm. However, realizing this promise requires the understanding of how existing tasks, developed for and performed by humans, can be effectively translated into tasks that robots can singularly or human-robot teams can collaboratively perform. In the interest of developing tools that facilitate this process we present Authr, an end-to-end task authoring environment that assists engineers at manufacturing facilities in translating existing manual tasks into plans applicable for human-robot teams and simulates these plans as they would be performed by the human and robot. We evaluated Authr with two user studies, which demonstrate the usability and effectiveness of Authr as an interface and the benefits of assistive task allocation methods for designing complex tasks for human-robot teams. We discuss the implications of these findings for the design of software tools for authoring human-robot collaborative plans. ",qa:"discord://discordapp.com/channels/734723053036109904/764103698750111794",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{Preview:"https://www.youtube.com/watch?v=Dbjtg6N4cjY","Short Talk":"https://www.youtube.com/watch?v=mmdvRNBR7mc"},images:{}},{id:42026,sequence:3,track:"Paper",session:[42224],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415821"},authors:["Carlos de la Torre-Ortiz","Michiel Marten Spape","Lauri Kangassalo","Tuukka Ruotsalo"],affiliations:["University of Helsinki"],title:"Brain Relevance Feedback for Interactive Image Generation",keywords:[],abstract:"Brain-computer interfaces (BCIs) are increasingly used to perform simple operations such as a moving a cursor, but have remained of limited use for more complex tasks. In our new approach to BCI, we use brain relevance feedback to control a generative adversarial network (GAN). We obtained EEG data from 31 participants who viewed face images while concentrating on particular facial features. Following, an EEG relevance classifier was trained and propagated as feedback on the latent image representation provided by the GAN. Estimates for individual vectors matching the relevant criteria were iteratively updated to optimize an image generation process towards mental targets. A double-blind evaluation showed high performance (86.26% accuracy) against random feedback (18.71%), and not significantly lower than explicit feedback (93.30%). Furthermore, we show the feasibility of the method with simultaneous task targets demonstrating BCI operation beyond individual task constraints. Thus, brain relevance feedback can validly control a generative model, overcoming a critical limitation of current BCI approaches.",qa:"discord://discordapp.com/channels/734723053036109904/764103697998675988",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{Preview:"https://www.youtube.com/watch?v=nIsXB-T2RKQ","Short Talk":"https://www.youtube.com/watch?v=hLCSUt3ImPA"},images:{}},{id:42027,sequence:4,track:"Paper",session:[42225],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415820"},authors:["Toby Jia-Jun Li","Jingya Chen","Haijun Xia","Tom Mitchell","Brad A Myers"],affiliations:["Carnegie Mellon University","UC San Diego"],title:"Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs",keywords:[],abstract:"A major problem in task-oriented conversational agents is the lack of support for the repair of conversational breakdowns. Prior studies have shown that current repair strategies for these kinds of errors are often ineffective due to: (1) the lack of transparency about the state of the system’s understanding of the users utterance; and (2) the system’s limited capabilities to understand the user’s verbal attempts to repair natural language understanding errors. This paper introduces SOVITE, a new multi-modal (speech plus direct manipulation) interface that helps users discover, identify the causes of, and recover from conversational breakdowns using the resources of existing mobile app GUIs for grounding. SOVITE displays the system’s understanding of user intents using GUI screenshots, allows users to refer to third-party apps and their GUI screens in conversations as inputs for intent disambiguation, and enables users to repair breakdowns using direct manipulation on these screenshots. The results from a remote user study with 10 users using SOVITE in 7 scenarios suggested that SOVITE’s approach is usable and effective.",qa:"discord://discordapp.com/channels/734723053036109904/764103700234895360",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=flVetXripWk","Short Talk":"https://www.youtube.com/watch?v=S8F6xoor9BQ"},images:{}},{id:42028,sequence:5,track:"Paper",session:[42216],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415884"},authors:["Mengqi Peng","Li-yi Wei","Rubaiat Habib Kazi","Vladimir Kim"],affiliations:["University of Hong Kong","Adobe Research","Adobe"],title:"Autocomplete Animated Sculpting",keywords:[],abstract:"Keyframe-based sculpting provides unprecedented freedom to author animated organic models, which can be difficult to create with other methods such as simulation, scripting, and rigging. However, sculpting animated objects can require significant artistic skill and manual labor, even more so than sculpting static 3D shapes or drawing 2D animations, which are already quite challenging. We present a keyframe-based animated sculpting system with the capability to autocomplete user editing under a simple and intuitive brushing interface. Similar to current desktop sculpting and VR brushing tools, users can brush surface details and volume structures. Meanwhile, our system analyzes their workflows and predicts what they might do in the future, both spatially and temporally. Users can accept or ignore these suggestions and thus maintain full control. We propose the first interactive suggestive keyframe sculpting system, specifically for spatio-temporal repetitive tasks, including low-level spatial details and high-level brushing structures across multiple frames. Our key ideas include a deformation-based optimization framework to analyze recorded workflows and synthesize predictions, and a semi-causal global similarity measurement to support flexible brushing stroke sequences and complex shape changes. Our system supports a variety of shape and motion styles, including those difficult to achieve via existing animation systems, such as topological changes that cannot be accomplished via simple rig-based deformations and stylized physically-implausible motions that cannot be simulated. We evaluate our system via a pilot user study that demonstrates the effectiveness of our system.",qa:"discord://discordapp.com/channels/734723053036109904/764103694244642856",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=oKtUmgefEyE",Preview:"https://www.youtube.com/watch?v=YVAujbZ8V-Y","Short Talk":"https://www.youtube.com/watch?v=c82o7nr689w"},images:{}},{id:42029,sequence:6,track:"Paper",session:[42204],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415825"},authors:["Camilo Luciano Fosco","Vincent M Casser","Amish Kumar Bedi","Zoya Bylinskii","Aaron Hertzmann","Peter O'Donovan"],affiliations:["MIT","Massachusetts Institute of Technology","Adobe Inc.","Adobe Research","Adobe","Adobe Systems, Inc."],title:"Predicting Visual Importance Across Graphic Design Types",keywords:[],abstract:"This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance. The model, code, and importance dataset are available at: http://predimportance.mit.edu. ",qa:"discord://discordapp.com/channels/734723053036109904/764103683565813790",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{Preview:"https://www.youtube.com/watch?v=n564GAfdL24","Short Talk":"https://www.youtube.com/watch?v=BBozhcDszao"},images:{}},{id:42030,sequence:7,track:"Paper",session:[42227],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415848"},authors:["Ritam Jyoti Sarmah","Yunpeng Ding","Di Wang","Cheuk Yin Phipson Lee","Toby Jia-Jun Li","Xiang 'Anthony' Chen"],affiliations:["UCLA HCI","University of California, Los Angeles","Carnegie Mellon University","UCLA"],title:"Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications",keywords:[],abstract:"Supporting voice commands in applications presents significant benefits to users. However, adding such support to existing GUIbased web apps is effort-consuming with a high learning barrier, as shown in our formative study, due to the lack of unified support for creating multimodal interfaces. We present Geno—a developer tool for adding the voice input modality to existing web apps without requiring significant NLP expertise. Geno provides a high-level workflow for developers to specify functionalities to be supported by voice (intents), create language models for detecting intents and the relevant information (parameters) from user utterances, and fulfill the intents by either programmatically invoking the corresponding functions or replaying GUI actions on the web app. Geno further supports multimodal references to GUI context in voice commands (e.g., “move this [event] to next week” while pointing at an event with the cursor). In a study, developers with little NLP expertise were able to add multimodal voice command support for two existing web apps using Geno.",qa:"discord://discordapp.com/channels/734723053036109904/764103698750111794",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=ynFYP2rinHg",Preview:"https://www.youtube.com/watch?v=rogkyPaQWIU","Short Talk":"https://www.youtube.com/watch?v=J33HFgG2yMo"},images:{}},{id:42031,sequence:8,track:"Paper",session:[42221],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415813"},authors:["Vidya Setlur","Enamul Hoque","Dae Hyun Kim","Angel Xuan Chang"],affiliations:["Tableau Research","York University","Stanford University","Simon Fraser University"],title:"Sneak Pique: Exploring Autocompletion as a Data Discovery Scaffold for Supporting Visual Analysis",keywords:[],abstract:"Natural language interaction has evolved as a useful modality to help users explore and interact with their data during visual analysis. Little work has been done to explore how autocompletion can help with data discovery while helping users formulate analytical questions. We developed a system called Sneak Pique as a design probe to better understand the usefulness of autocompletion for visual analysis. We ran three Mechanical Turk studies to evaluate user preferences for various text- and visualization widget-based autocompletion design variants for helping with partial search queries. Our findings indicate that users found data previews to be useful in the suggestions. Widgets were preferred for previewing temporal, geospatial, and numerical data while text autocompletion was preferred for categorical and hierarchical data. We conducted an exploratory analysis of our system implementing this specific subset of preferred autocompletion variants. Our insights regarding the efficacy of these autocompletion suggestions can inform the future design of natural language interfaces supporting visual analysis.",qa:"discord://discordapp.com/channels/734723053036109904/764103697260347402",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{Preview:"https://www.youtube.com/watch?v=A4YFk_yBW5k","Short Talk":"https://www.youtube.com/watch?v=n2evJc0BVHg"},images:{}},{id:42032,sequence:9,track:"Paper",session:[42206],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415846"},authors:["Jeremy Hartmann","Stephen DiVerdi","Cuong Nguyen","Daniel Vogel"],affiliations:["University of Waterloo","Adobe","Adobe Research"],title:"View-Dependent Effects for 360° Virtual Reality Video",keywords:[],abstract:"“View-dependent effects” have parameters that change with the user’s view and are rendered dynamically at runtime. They can be used to simulate physical phenomena such as exposure adaptation, as well as for dramatic purposes such as vignettes. We present a technique for adding view-dependent effects to 360◦ video, by interpolating spatial keyframes across an equirectangular video to control effect parameters during playback. An in-headset authoring tool is used to confgure effect parameters and set keyframe positions. We evaluate the utility of view-dependent effects with expert 360◦ flmmakers and the perception of the effects with a general audience. Results show that experts fnd view-dependent effects desirable for their creative purposes and that these effects can evoke novel experiences in an audience. ",qa:"discord://discordapp.com/channels/734723053036109904/764103686623985664",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=moyto2NSmF8","Short Talk":"https://www.youtube.com/watch?v=WF4xr-9lMKs"},images:{}},{id:42033,sequence:10,track:"Paper",session:[42219],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415831"},authors:["Ken Nakagaki","Joanne Leong","Jordan L Tappa","Joao Wilbert","Hiroshi Ishii"],affiliations:["MIT Media Lab","MIT","Massachusetts Institute of Technology"],title:"HERMITS: Dynamically Reconfiguring the Interactivity of Self-propelled TUIs with Mechanical Shell Add-ons",keywords:[],abstract:"We introduce HERMITS, a modular interaction architecture for self-propelled Tangible User Interfaces (TUIs) that incorporates physical add-ons, referred to as mechanical shells. The mechanical shell add-ons are intended to be dynamically reconfigured by utilizing the locomotion capability of self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed a proofof-concept system that demonstrates this novel architecture using two-wheeled robots and a variety of mechanical shell examples. These mechanical shell add-ons are passive physical attatchments that extend the primitive interactivities (e.g. shape, motion and light) of the self-propelled robots. The paper proposes the architectural design, interactive functionality of HERMITS as well as design primitives for mechanical shells. The paper also introduces the prototype implementation that is based on an off-the-shelf robotic toy with a modified docking mechanism. A range of applications is demonstrated with the prototype to motivate the collective and dynamically reconfigurable capability of the modular architecture, such as an interactive mobility simulation, an adaptive home/desk environment, and a story-telling narrative. Lastly we discuss the future research opportunity of HERMITS to enrich the interactivity and adaptability of actuated and shape changing TUIs. ",qa:"discord://discordapp.com/channels/734723053036109904/764103695041429524",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=09e7kfT7nuY",Preview:"https://www.youtube.com/watch?v=Ts_f26Xxb_w","Short Talk":"https://www.youtube.com/watch?v=6yWFVRL0fLQ"},images:{}},{id:42034,sequence:11,track:"Paper",session:[42210],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415840"},authors:["Youngkyung Choi","Neung Ryu","Myung Jin Kim","Artem Dementyev","Andrea Bianchi"],affiliations:["KAIST","MIT"],title:"BodyPrinter: Fabricating Circuits Directly on the Skin at Arbitrary Locations Using a Wearable Compact Plotter",keywords:[],abstract:"On-body electronics and sensors offer the opportunity to seamlessly augment the human with computing power. Accordingly, numerous previous work investigated methods that exploit conductive materials and fexible substrates to fabricate circuits in the form of wearable devices, stretchable patches, and stickers that can be attached to the skin. For all these methods, the fabrication process involves several manual steps, such as designing the circuit in software, constructing conductive patches, and manually placing these physical patches on the body. In contrast, in this work, we propose to fabricate electronics directly on the skin. We present BodyPrinter, a wearable conductive-ink deposition machine, that prints fexible electronics directly on the body using skin-safe conductive ink. The paper describes our system in detail and, through a series of examples and a technical evaluation, we show how direct on-body fabrication of electronic circuits and sensors can further enhance the human body. ",qa:"discord://discordapp.com/channels/734723053036109904/764103689504817203",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=fkMPmDZeFsk","Short Talk":"https://www.youtube.com/watch?v=BcSXLao5bJ4"},images:{}},{id:42035,sequence:12,track:"Paper",session:[42198],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415855"},authors:["Danli Luo","Jianzhe Gu","Fang Qin","Guanyun Wang","Lining Yao"],affiliations:["Carnegie Mellon University"],title:"E-seed: Shape-Changing Interfaces that Self Drill",keywords:[],abstract:"As sensors and interactive devices become ubiquitous and transition outdoors and into the wild, we are met with the challenge of mass deployment and actuation. We present Eseed, a biomimetic platform that consumes little power to deploy, harvests energy from nature to install, and functions autonomously in the field. Each seed can individually selfdrill into a substrate by harvesting moisture fluctuations in its ambient environment. As such, E-seed acts as a shapechanging interface to autonomously embed functional devices and interfaces into the soil, with the potential of aerial deployment in hard-to-reach locations. Our system is constructed primarily from wood veneer, making it lightweight, inexpensive, and biodegradable. In this paper, we detail our fabrication process and showcase demos that leverage the E-seed platform as a self-drilling interface. We envision that possible applications include soil sensors, sampling, and environmental monitoring for agriculture and reforestation.",qa:"discord://discordapp.com/channels/734723053036109904/764103680441450547",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=6-BzTbYmuOk","Short Talk":"https://www.youtube.com/watch?v=AwwyUsoZqsc"},images:{}},{id:42036,sequence:13,track:"Paper",session:[42200],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415879"},authors:["Tuochao Chen","Benjamin Steeper","Kinan Alsheikh","Songyun Tao","Francois Guimbretiere","Cheng Zhang"],affiliations:["Cornell University","EECS"],title:"C-Face: Continuously reconstructing facial expressions by deep learning contours of the face with ear-mounted miniature cameras",keywords:[],abstract:"C-Face (Contour-Face) is an ear-mounted wearable sensing technology that uses two miniature cameras to continuously reconstruct facial expressions by deep learning contours of the face. When facial muscles move, the contours of the face change from the point of view of the ear-mounted cameras. These subtle changes are fed into a deep learning model which continuously outputs 42 facial feature points representing the shapes and positions of the mouth, eyes and eyebrows. To evaluate C-Face, we embedded our technology into headphones and earphones. We conducted a user study with nine participants. In this study, we compared the output of our system to the feature points outputted by a state of the art computer vision library (Dlib1) from a font facing camera. We found that the mean error of all 42 feature points was 0.77 mm for earphones and 0.74 mm for headphones. The mean error for 20 major feature points capturing the most active areas of the face was 1.43 mm for earphones and 1.39 mm for headphones.\r\nThe ability to continuously reconstruct facial expressions introduces new opportunities in a variety of applications. As a demonstration, we implemented and evaluated C-Face for two applications: facial expression detection (outputting emojis) and silent speech recognition. We further discuss the opportunities and challenges of deploying C-Face in real-world applications. ",qa:"discord://discordapp.com/channels/734723053036109904/764103681976172594",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=MPbac9rSfaw",Preview:"https://www.youtube.com/watch?v=c9xvFuQsHRE","Short Talk":"https://www.youtube.com/watch?v=J9d9gK4ND-E"},images:{}},{id:42037,sequence:14,track:"Paper",session:[42228],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415590"},authors:["Megan Hofmann","Jennifer Mankoff","Scott E Hudson"],affiliations:["Carnegie Mellon Universtiy","University of Washington","Carnegie Mellon University"],title:"KnitGIST: A Programming Synthesis Toolkit for Generating Functional Machine-Knitting Textures",keywords:[],abstract:"Automatic knitting machines are robust, digital fabrication devices that enable rapid and reliable production of attractive, functional objects by combining stitches to produce unique physical properties. However, no existing design tools support optimization for desirable physical and aesthetic knitted properties. We present KnitGIST (Generative Instantiation Synthesis Toolkit for knitting), a program synthesis pipeline and library for generating hand- and machine-knitting patterns by intuitively mapping objectives to tactics for texture design. KnitGIST generates a machine-knittable program in a domain-specific programming language.",qa:"discord://discordapp.com/channels/734723053036109904/764103700997865472",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=bDst8drCnLw",Preview:"https://www.youtube.com/watch?v=yIW8X92lxcM","Short Talk":"https://www.youtube.com/watch?v=LIkHkZ_t9ag"},images:{}},{id:42038,sequence:15,track:"Paper",session:[42225],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415817"},authors:["Alex Olwal","Kevin Balke","Dmitrii Votintcev","Thad Starner","Paula Conn","Bonnie Chinh","Benoit Corda"],affiliations:["Google Research"],title:"Wearable Subtitles: Augmenting Spoken Communication with Lightweight Eyewear for All-day Captioning",keywords:[],abstract:"Mobile solutions can help transform speech and sound into visual representations for people who are deaf or hard-ofhearing (DHH). However, where handheld phones present challenges, head-worn displays (HWDs) could further communication through privately transcribed text, handsfree use, improved mobility, and socially acceptable interactions. In this work, we introduce a lightweight HWD system specifically designed to augment communication through sound transcription for a full workday. Through a hybrid, low-power architecture, we enable up to 15 hours of continuous use. We describe a large survey (n=501) and three user studies with 24 deaf/hard-of-hearing participants which inform our development and help us refine our prototypes. Our studies and prior research identify critical challenges for the adoption of HWD systems which we address through extended battery life, lightweight and balanced mechanical design (54 g), fitting options, and form factors that are compatible with current social norms.",qa:"discord://discordapp.com/channels/734723053036109904/764103700234895360",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=slwTiW1mky4","Short Talk":"https://www.youtube.com/watch?v=jI9hIWQVjJA"},images:{}},{id:42039,sequence:16,track:"Paper",session:[42214],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415816"},authors:["Mark Richardson","Robert Wang"],affiliations:["Facebook Reality Labs"],title:"Decoding Surface Touch Typing from Hand-Tracking",keywords:[],abstract:"We propose a novel text decoding method that enables touch typing on an uninstrumented flat surface. Rather than relying on physical keyboards or capacitive touch, our method takes as input hand motion of the typist, obtained through hand-tracking, and decodes this motion directly into text. We use a temporal convolutional network to represent a motion model that maps the hand motion, represented as a sequence of hand pose features, into text characters. To enable touch typing without the haptic feedback of a physical keyboard, we had to address more erratic typing motion due to drift of the fingers. Thus, we incorporate a language model as a text prior and use beam search to efficiently combine our motion and language models to decode text from erratic or ambiguous hand motion. We collected a dataset of 20 touch typists and evaluated our model on several baselines, including contactbased text decoding and typing on a physical keyboard. Our proposed method is able to leverage continuous hand pose information to decode text more accurately than contact-based methods and an offline study shows parity (73 WPM, 2.38% UER) with typing on a physical keyboard. Our results show that hand-tracking has the potential to enable rapid text entry in mobile environments.",qa:"discord://discordapp.com/channels/734723053036109904/764103691979849729",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42040,sequence:17,track:"Paper",session:[42220],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415593"},authors:["Thomas Langerak","Juan José Zárate","Velko Vechev","Daniele Panozzo","Otmar Hilliges","David Lindlbauer"],affiliations:["ETH Zurich","ETH","New York University"],title:"Optimal Control for Electromagnetic Haptic Guidance Systems",keywords:[],abstract:"We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this fexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches. ",qa:"discord://discordapp.com/channels/734723053036109904/764103696476536863",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=EWilsPZsJ04",Preview:"https://www.youtube.com/watch?v=kWug-YdLwwc","Short Talk":"https://www.youtube.com/watch?v=KJxiGcKHjU0"},images:{}},{id:42041,sequence:18,track:"Paper",session:[42229],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415812"},authors:["Nianlong Li","Han-Jong Kim","Luyao Shen","Feng Tian","Teng Han","Xing-Dong Yang","Tek-Jin Nam"],affiliations:["Institute of Software, Chinese Academy of Sciences","University of Chinese Academy of Sciences","KAIST","Dartmouth College"],title:"HapLinkage: Prototyping Haptic Proxies for Virtual Hand Tools Using Linkage Mechanism",keywords:[],abstract:"Haptic simulation of hand tools like wrenches, pliers, scissors and syringes are beneficial for finely detailed skill training in VR, but designing for numerous hand tools usually requires an expert-level knowledge of specific mechanism and protocol. This paper presents HapLinkage, a prototyping framework based on linkage mechanism, that provides typical motion templates and haptic renderers to facilitate proxy design of virtual hand tools. The mechanical structures can be easily modified, for example, to scale the size, or to change the range of motion by selectively changing linkage lengths. Resistant, stop, release, and restoration force feedback are generated by an actuating module as part of the structure. Additional vibration feedback can be generated with a linear actuator. HapLinkage enables easy and quick prototypting of hand tools for diverse VR scenarios, that embody both of their kinetic and haptic properties. Based on interviews with expert designers, it was confirmed that HapLinkage is expressive in designing haptic proxy of hand tools to enhance VR experiences. It also identified potentials and future development of the framework.",qa:"discord://discordapp.com/channels/734723053036109904/764103703199612955",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{Preview:"https://www.youtube.com/watch?v=11zDk2A883E","Short Talk":"https://www.youtube.com/watch?v=3Hvr54iOArU"},images:{}},{id:42042,sequence:19,track:"Paper",session:[42206],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415824"},authors:["Lei Zhang","Steve Oney"],affiliations:["University of Michigan"],title:"FlowMatic: An Immersive Authoring Tool for Creating Interactive Scenes in Virtual Reality",keywords:[],abstract:"Immersive authoring is a paradigm that makes Virtual Reality (VR) application development easier by allowing programmers to create VR content while immersed in the virtual environment. In this paradigm, programmers manipulate programming primitives through direct manipulation and get immediate feedback on their program’s state and output. However, existing immersive authoring tools have a low ceiling; their programming primitives are intuitive but can only express a limited set of static relationships between elements in a scene. In this paper, we introduce FlowMatic, an immersive authoring tool that raises the ceiling of expressiveness by allowing programmers to specify reactive behaviors— behaviors that react to discrete events such as user actions, system timers, or collisions. FlowMatic also introduces primitives for programmatically creating and destroying new objects, for abstracting and re-using functionality, and for importing three-dimensional (3D) models. Importantly, FlowMatic uses novel visual representations to allow these primitives to be represented directly in VR. We also describe the results of a user study that illustrates the usability advantages of FlowMatic relative to a two-dimensional (2D) authoring tool and we demonstrate its expressiveness through several example applications that would be impossible to implement with existing immersive authoring tools. By combining a visual program representation with expressive programming primitives and a natural User Interface (UI) for authoring programs, FlowMatic shows how programmers can build fully interactive virtual experiences with immersive authoring.",qa:"discord://discordapp.com/channels/734723053036109904/764103686623985664",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=-m1QVWlLNDs","Short Talk":"https://www.youtube.com/watch?v=Yq0-nzfoDnU"},images:{}},{id:42043,sequence:20,track:"Paper",session:[42208],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415901"},authors:["Jun Gong","Aakar Gupta","Hrvoje Benko"],affiliations:["Facebook","Dartmouth College","Facebook Inc"],title:"Acustico: Surface Tap Detection and Localization using Wrist-based Acoustic TDOA Sensing",keywords:[],abstract:"In this paper, we present Acustico, a passive acoustic sensing approach that enables tap detection and 2D tap localization on uninstrumented surfaces using a wristworn device. Our technique uses a novel application of acoustic time differences of arrival (TDOA) analysis. We adopt a sensor fusion approach by taking both “surface waves” (i.e., vibrations through surface) and “sound waves” (i.e., vibrations through air) into analysis to improve sensing resolution. We carefully design a sensor configuration to meet the constraints of a wristband form factor. We built a wristband prototype with four acoustic sensors, two accelerometers and two microphones. Through a 20-participant study, we evaluated the performance of our proposed sensing technique for tap detection and localization. Results show that our system reliably detects taps with an F1-score of 0.9987 across different environmental noises and yields high localization accuracies with root-mean-square-errors of 7.6mm (Xaxis) and 4.6mm (Y-axis) across different surfaces and tapping techniques.",qa:"discord://discordapp.com/channels/734723053036109904/764103685848432670",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=ZyXd3KjN07A","Short Talk":"https://www.youtube.com/watch?v=HwgTqExTqB0"},images:{}},{id:42044,sequence:21,track:"Paper",session:[42212],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415834"},authors:["Sorin Lerner"],affiliations:["UC San Diego"],title:"Focused Live Programming with Loop Seeds",keywords:[],abstract:"Live programming is a paradigm in which the programmer can visualize the runtime values of the program each time the program changes. The promise of live programming depends on using test cases to run the program and thereby provide these runtime values. In this paper we show that in some situations test cases are insuffcient in a fundamental way, in that there are no test inputs that can drive certain incomplete loops to produce useful data, a problem we call the loop-datavoid problem. The problem stems from the fact that useful data inside the loop might only be produced after the loop has been fully written. To solve this problem, we propose a paradigm called Focused Live Programming with Loop Seeds, in which the programmer provides hypothetical values to start a loop iteration, and then the programming environment focuses the live visualization on this hypothetical loop iteration. We introduce the loop-datavoid problem, present our proposed solution, explain it in detail, and then present the results of a user study.",qa:"discord://discordapp.com/channels/734723053036109904/764103688939372574",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42045,sequence:22,track:"Paper",session:[42226],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415873"},authors:["Ryo Takahashi","Masaaki Fukumoto","Changyo Han","Takuya Sasatani","Yoshiaki Narusue","Yoshihiro Kawahara"],affiliations:["The University of Tokyo","Microsoft Research"],title:"TelemetRing: A Batteryless and Wireless Ring-shaped Keyboard using Passive Inductive Telemetry",keywords:[],abstract:"TelemetRing is a batteryless and wireless ring-shaped keyboard that supports command and text entry in daily lives by detecting finger typing on various surfaces. The proposed inductive telemetry approach eliminates bulky batteries or capacitors from the ring part. Each ring consists of a sensor coil (the ring part itself), 1-DoF piezoelectric accelerometer, and varactor diode; moreover, it has different resonant frequencies. Typing shocks slightly shift the resonant frequency, and these are detected by a wrist-mounted readout coil. 5-bit chord keyboard is realized by attaching five sensor rings on five fingers. Our evaluation shows that the prototype achieved the tiny (6 g, 3.5 cm3) ring sensor and 89.7% of typing detection ratio.",qa:"discord://discordapp.com/channels/734723053036109904/764103702764060682",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=8xgMMyThk2M",Preview:"https://www.youtube.com/watch?v=pD4FRuVnmJg","Short Talk":"https://www.youtube.com/watch?v=vRJu2T9xtE8"},images:{}},{id:42046,sequence:23,track:"Paper",session:[42219],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415894"},authors:["Ryuma Niiyama","Hiroki Sato","kazzmasa TSUJIMURA","Koya Narumi","Young ah Seong","Ryosuke Yamamura","Yasuaki Kakehi","Yoshihiro Kawahara"],affiliations:["The University of Tokyo","mercari R4D"],title:"poimo: Portable and Inflatable Mobility Devices Customizable for Personal Physical Characteristics",keywords:[],abstract:"Despite the recent growth in popularity of personal mobility devices (e.g., e-scooters and e-skateboards), they still suffer from limited safety and narrow design form factors, due to their rigid structures. On the other hand, inflatable interfaces studied in human-computer interaction can achieve large volume change by simple inflation/deflation. Inflatable structure also offers soft and safe interaction owing to material compliance and diverse fabrication methods that lead to a wide range of forms and aesthetics. In this paper, we propose poimo, a new family of POrtable and Inflatable MObility devices, which consists of inflatable frames, inflatable wheels, and inflatable steering mechanisms made of a mass-manufacturable material called drop-stitch fabric. First, we defined the basic material properties of a drop-stitch inflatable structure that is sufficiently strong to carry a person while simultaneously allowing soft deformation and deflation for storage and portability. We then implemented an interactive design system that can scan the user’s desired riding posture to generate a customized personal mobility device and can add the user’s shape and color preferences. To demonstrate the custom-design capability and mobility, we designed several 3D models using our system and built physical samples for two basic templates: a motorcycle and a wheelchair. Finally, we conducted an online user study to examine the usability of the design system and share lessons learned for further improvements in the design and fabrication of poimo. ",qa:"discord://discordapp.com/channels/734723053036109904/764103695041429524",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=Xgailzg-jSw","Short Talk":"https://www.youtube.com/watch?v=2mn283Yn9_c"},images:{}},{id:42047,sequence:24,track:"Paper",session:[42198],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415867"},authors:["Humphrey Yang","Kuanren Qian","Haolin Liu","Yuxuan Yu","Jianzhe Gu","Matthew McGehee","Yongjie Jessica Zhang","Lining Yao"],affiliations:["Carnegie Mellon University"],title:"SimuLearn: Fast and Accurate Simulator to Support  Morphing Materials Design and Workflows",keywords:[],abstract:"Morphing materials allow us to create new modalities of interaction and fabrication by leveraging the materials’ dynamic behaviors. Yet, despite the ongoing rapid growth of computational tools within this realm, current developments are bottlenecked by the lack of an effective simulation method. As a result, existing design tools must trade-off between speed and accuracy to support a real-time interactive design scenario. In response, we introduce SimuLearn, a data-driven method that combines finite element analysis and machine learning to create real-time (0.61 seconds) and truthful (97% accuracy) morphing material simulators. We use mesh-like 4D printed structures to contextualize this method and prototype design tools to exemplify the design workflows and spaces enabled by a fast and accurate simulation method. Situating this work among existing literature, we believe SimuLearn is a timely addition to the HCI CAD toolbox that can enable the proliferation of morphing materials.",qa:"discord://discordapp.com/channels/734723053036109904/764103680441450547",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=Eh5lKTPO0e0","Short Talk":"https://www.youtube.com/watch?v=rR4QApYJW2Q"},images:{}},{id:42048,sequence:25,track:"Paper",session:[42203],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415859"},authors:["YAN YIXIAN","Kazuki Takashima","Anthony Tang","Takayuki Tanno","Kazuyuki Fujita","Yoshifumi Kitamura"],affiliations:["Tohoku University","University of Toronto"],title:"ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World",keywords:[],abstract:"We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls—autonomous robotic encounter-type haptic wall-shaped props—that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user’s movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.",qa:"discord://discordapp.com/channels/734723053036109904/764103682840592394",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=t6J65a1S1y0","Short Talk":"https://www.youtube.com/watch?v=EUrgqFPI4G0"},images:{}},{id:42049,sequence:26,track:"Paper",session:[42208],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415823"},authors:["Artem Dementyev","Alex Olwal","Richard F Lyon"],affiliations:["Google Research","Google"],title:"Haptics with Input: Back-EMF in Linear Resonant Actuators to Enable Touch, Pressure and Environmental Awareness",keywords:[],abstract:"Today’s wearable and mobile devices typically use separate hardware components for sensing and actuation. In this work, we introduce new opportunities for the Linear Resonant Actuator (LRA), which is ubiquitous in such devices due to its capability for providing rich haptic feedback. By leveraging strategies to enable active and passive sensing capabilities with LRAs, we demonstrate their benefits and potential as self-contained I/O devices. Specifically, we use the back-EMF voltage to classify if the LRA is tapped, touched, as well as how much pressure is being applied. The back-EMF sensing is already integrated into many motor and LRA drivers. We developed a passive low-power tap sensing method that uses just 37.7 µA. Furthermore, we developed active touch and pressure sensing, which is low-power, quiet (2 dB), and minimizes vibration. The sensing method works with many types of LRAs. We show applications, such as pressure-sensing side-buttons on a mobile phone. We have also implemented our technique directly on an existing mobile phone’s LRA to detect if the phone is handheld or placed on a soft or hard surface. Finally, we show that this method can be used for haptic devices to determine if the LRA makes good contact with the skin. Our approach can add rich sensing capabilities to the ubiquitous LRA actuators without requiring additional sensors or hardware.",qa:"discord://discordapp.com/channels/734723053036109904/764103685848432670",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=f4mckHLWBd4","Short Talk":"https://www.youtube.com/watch?v=tAacK3yAK9I"},images:{}},{id:42050,sequence:27,track:"Paper",session:[42201],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415851"},authors:["Yifan Wu","Joseph Hellerstein","Arvind Satyanarayan"],affiliations:["UC Berkeley","MIT"],title:"B2: Bridging Code and Interactive Visualization in Computational Notebooks",keywords:[],abstract:"Data scientists have embraced computational notebooks to author analysis code and accompanying visualizations within a single document. Currently, although these media may be interleaved, they remain siloed: interactive visualizations must be manually specified as they are divorced from the analysis provenance expressed via dataframes, while code cells have no access to users’ interactions with visualizations, and hence no way to operate on the results of interaction. To bridge this divide, we present B2, a set of techniques grounded in treating data queries as a shared representation between the code and interactive visualizations. B2 instruments data frames to track the queries expressed in code and synthesize corresponding visualizations. These visualizations are displayed in a dashboard to facilitate interactive analysis. When an interaction occurs, B2 reifies it as a data query and generates a history log in a new code cell. Subsequent cells can use this log to further analyze interaction results and, when marked as reactive, to ensure that code is automatically recomputed when new interaction occurs. In an evaluative study with data scientists, we find that B2 promotes a tighter feedback loop between coding and interacting with visualizations. All participants frequently moved from code to visualization and vice-versa, which facilitated their exploratory data analysis in the notebook. ",qa:"discord://discordapp.com/channels/734723053036109904/764103679694995526",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=YmNXp8D50ts",Preview:"https://www.youtube.com/watch?v=UGqbFQbNW2c","Short Talk":"https://www.youtube.com/watch?v=cc1mDmqhL7Q"},images:{}},{id:42051,sequence:28,track:"Paper",session:[42220],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415850"},authors:["Youngbo Aram Shim","Keunwoo Park","Sangyoon Lee","Jeongmin Son","Taeyun Woo","Geehyuk Lee"],affiliations:["KAIST"],title:"FS-Pad: Video Game Interactions with a Force Feedback Gamepad",keywords:[],abstract:"Force feedback has not been fully explored in modern gaming environments where a gamepad is the main interface. We developed various game interaction scenarios where force feedback through the thumbstick of the gamepad can be effective, and categorized them into five themes. We built a haptic device and control system that can support all presented interactions. The resulting device, FS-Pad, has sufficient fidelity to be used as a haptic game interaction design tool. To verify the presented interactions and effectiveness of the FS-Pad, we conducted a user study with game players, developers, and designers. The subjects used an FS-Pad while playing a demo game and were then interviewed. Their feedback revealed the actual needs for the presented interactions as well as insight into the potential design of game interactions when applying FS-Pad. ",qa:"discord://discordapp.com/channels/734723053036109904/764103696476536863",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=DW4P4TP_ETA",Preview:"https://www.youtube.com/watch?v=CaOFOnD6ADk","Short Talk":"https://www.youtube.com/watch?v=BEKy-ieeBgE"},images:{}},{id:42052,sequence:29,track:"Paper",session:[42206],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415815"},authors:["Tianyi Wang","Xun Qian","Fengming He","Xiyun Hu","Ke Huo","Yuanzhi Cao","Karthik Ramani"],affiliations:["Purdue University"],title:"CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications",keywords:[],abstract:"Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user’s daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.",qa:"discord://discordapp.com/channels/734723053036109904/764103686623985664",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=-hOuwIzyIUw","Short Talk":"https://www.youtube.com/watch?v=bcHp6netQ9U"},images:{}},{id:42053,sequence:30,track:"Paper",session:[42212],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415869"},authors:["Kasra Ferdowsifard","Allen Ordookhanians","Hila Peleg","Sorin Lerner","Nadia Polikarpova"],affiliations:["UC San Diego"],title:"Small-Step Live Programming by Example",keywords:[],abstract:"Live programming is a paradigm in which the programming environment continually displays runtime values. Program synthesis is a technique that can generate programs or program snippets from examples. Previous works that combine the two have taken a holistic approach to the way examples describe the behavior of functions and programs. This paper presents a new programming paradigm called Small-Step Live Programming by Example that lets the user apply Programming by Example locally. When using Small-Step Live Programming by Example, programmers can change the runtime values displayed by the live visualization to generate local program snippets. We implemented this new paradigm in a tool called SNIPPY, and performed a user study on 13 programmers. Our study fnds that Small-Step Live Programming by Example with SNIPPY helps users solve harder problems faster, and that for certain types of queries, users prefer it to searching the web. Additionally, we identify the user-synthesizer gap, in which users’ mental models of the tool do not match its ability, and needs to be taken into account in the design of future synthesis tools. ",qa:"discord://discordapp.com/channels/734723053036109904/764103688939372574",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=ees02s2OttY","Short Talk":"https://www.youtube.com/watch?v=6xmCPznThN8"},images:{}},{id:42054,sequence:31,track:"Paper",session:[42211],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415589"},authors:["Thomas Langerak","Juan José Zárate","David Lindlbauer","Christian Holz","Otmar Hilliges"],affiliations:["ETH Zurich","ETH"],title:"Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback",keywords:[],abstract:"We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios. ",qa:"discord://discordapp.com/channels/734723053036109904/764103691124080641",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=q7PoZ-jGN3I",Preview:"https://www.youtube.com/watch?v=N9RPUSfnBac","Short Talk":"https://www.youtube.com/watch?v=ISR7eQgD5x8"},images:{}},{id:42055,sequence:32,track:"Paper",session:[42215],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415864"},authors:["Amy Pavel","Gabriel Reyes","Jeffrey P. Bigham"],affiliations:["Apple","Apple Inc."],title:"Rescribe: Authoring and Automatically Editing Audio Descriptions",keywords:[],abstract:"Audio descriptions make videos accessible to those who cannot see them by describing visual content in audio. Producing audio descriptions is challenging due to the synchronous nature of the audio description that must fit into gaps of other video content. An experienced audio description author will produce content that fits narration necessary to understand, enjoy, or experience the video content into the time available. This can be especially tricky for novices to do well. In this paper, we introduce a tool, Rescribe, that helps authors create and refine their audio descriptions. Using Rescribe, authors first create a draft of all the content they would like to include in the audio description. Rescribe then uses a dynamic programming approach to optimize between the length of the audio description, available automatic shortening approaches, and source track lengthening approaches. Authors can iteratively visualize and refine the audio descriptions produced by Rescribe, working in concert with the tool. We evaluate the effectiveness of Rescribe through interviews with blind and visually impaired audio description users who give feedback on Rescribe results. In addition, we invite novice users to create audio descriptions with Rescribe and another tool, finding that users produce audio descriptions with fewer placement errors using Rescribe.",qa:"discord://discordapp.com/channels/734723053036109904/764103692781355018",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=4DHkMsbsdec",Preview:"https://www.youtube.com/watch?v=9knqCzCX0zc","Short Talk":"https://www.youtube.com/watch?v=4OA7XaMdxKk"},images:{}},{id:42056,sequence:33,track:"Paper",session:[42203],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415891"},authors:["Elodie Bouzbib","Gilles Bailly","Sinan D Haliyo","Pascal Frey"],affiliations:["Sorbonne Université, CNRS, ISIR","ISCD","Sorbonne Université"],title:"CoVR: A Large-Scale Force-Feedback Robotic Interface for Non-Deterministic Scenarios in VR",keywords:[],abstract:"We present CoVR, a novel robotic interface providing strong kinesthetic feedback (100 N) in a room-scale VR arena. It consists of a physical column mounted on a 2D Cartesian ceiling robot (XY displacements) with the capacity of (1) resisting to body-scaled users’ actions such as pushing or leaning; (2) acting on the users by pulling or transporting them as well as (3) carrying multiple potentially heavy objects (up to 80kg) that users can freely manipulate or make interact with each other. We describe its implementation and define a trajectory generation algorithm based on a novel user intention model to support non-deterministic scenarios, where the users are free to interact with any virtual object of interest with no regards to the scenarios’ progress. A technical evaluation and a user study demonstrate the feasibility and usability of CoVR, as well as the relevance of whole-body interactions involving strong forces, such as being pulled through or transported.",qa:"discord://discordapp.com/channels/734723053036109904/764103682840592394",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=fPCXdxVtpQQ",Preview:"https://www.youtube.com/watch?v=BCufHVCjB_4","Short Talk":"https://www.youtube.com/watch?v=TwR4qQidYSo"},images:{}},{id:42057,sequence:34,track:"Paper",session:[42211],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415875"},authors:["Jun Nishida","Soichiro Matsuda","Hiroshi Matsui","Shan-Yuan Teng","ziwei Liu","Kenji Suzuki","Pedro Lopes"],affiliations:["University of Chicago","Tsukuba University","Ruhr-University Bochum","The University of Chicago","University of Tsukuba"],title:"HandMorph: a Passive Exoskeleton that Miniaturizes Grasp",keywords:[],abstract:"We engineered an exoskeleton, which we call HandMorph, that approximates the experience of having a smaller grasping range. It uses mechanical links to transmit motion from the wearer’s fingers to a smaller hand with five anatomically correct fingers. The result is that HandMorph miniaturizes a wearer’s grasping range while transmitting haptic feedback. Unlike other size-illusions based on virtual reality, HandMorph achieves this in the user’s real environment, preserving the user’s physical and social contexts. As such, our device can be integrated into the user’s workflow, e.g., to allow product designers to momentarily change their grasping range into that of a child while evaluating a toy prototype. In our first user study, we found that participants perceived objects as larger when wearing HandMorph, which suggests that their size perception was successfully transformed. In our second user study, we assessed the experience of using HandMorph in designing a simple toy trumpet for children. We found that participants felt more confident in their toy design when using HandMorph to validate its ergonomics. ",qa:"discord://discordapp.com/channels/734723053036109904/764103691124080641",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{Preview:"https://www.youtube.com/watch?v=BJTXSVIMjrg","Short Talk":"https://www.youtube.com/watch?v=t9J37oVFLO4"},images:{}},{id:42058,sequence:35,track:"Paper",session:[42226],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415818"},authors:["Keunwoo Park","Sunbum Kim","Youngwoo Yoon","Tae-Kyun Kim","Geehyuk Lee"],affiliations:["KAIST","ETRI","Imperial College London","School of Computing, KAIST"],title:"DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera",keywords:[],abstract:"Near-surface multi-finger tracking (NMFT) technology expands the input space of touchscreens by enabling novel interactions such as mid-air and finger-aware interactions. We present DeepFisheye, a practical NMFT solution for mobile devices, that utilizes a fisheye camera attached at the bottom of a touchscreen. DeepFisheye acquires the image of an interacting hand positioned above the touchscreen using the camera and employs deep learning to estimate the 3D position of each fingertip. We created two new hand pose datasets comprising fisheye images, on which our network was trained. We evaluated DeepFisheye’s performance for three device sizes. DeepFisheye showed average errors with approximate value of 20 mm for fingertip tracking across the different device sizes. Additionally, we created simple rule-based classifiers that estimate the contact finger and hand posture from DeepFisheye’s output. The contact finger and hand posture classifiers showed accuracy of approximately 83 and 90%, respectively, across the device sizes. ",qa:"discord://discordapp.com/channels/734723053036109904/764103702764060682",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=8W3OpxIXpQg",Preview:"https://www.youtube.com/watch?v=Vds_rvnU6j8","Short Talk":"https://www.youtube.com/watch?v=5gbf8Vztjzc"},images:{}},{id:42059,sequence:36,track:"Paper",session:[42208],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415902"},authors:["Md. Farhan Tasnim Oshim","Julian Killingback","Dave Follette","Huaishu Peng","Tauhidur Rahman"],affiliations:["University of Massachusetts Amherst","University of Maryland"],title:"MechanoBeat: Monitoring Interactions with Everyday Objects using 3D Printed Harmonic Oscillators and Ultra-Wideband Radar",keywords:[],abstract:"In this paper we present MechanoBeat, a 3D printed mechanical tag that oscillates at a unique frequency upon user interaction. With the help of an ultra-wideband (UWB) radar array, MechanoBeat can unobtrusively monitor interactions with both stationary and mobile objects. MechanoBeat consists of small, scalable, and easy-to-install tags that do not require any batteries, silicon chips, or electronic components. Tags can be produced using commodity desktop 3D printers with cheap materials. We develop an effcient signal processing and deep learning method to locate and identify tags using only the signals refected from the tag vibrations. MechanoBeat is capable of detecting simultaneous interactions with high accuracy, even in noisy environments. We leverage UWB radar signals’ high penetration property to sense interactions behind walls in a non-line-of-sight (NLOS) scenario. A number of applications using MechanoBeat have been explored and the results have been presented in the paper. ",qa:"discord://discordapp.com/channels/734723053036109904/764103685848432670",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=YX9APqyShL0","Short Talk":"https://www.youtube.com/watch?v=KqYUhP4UoF8"},images:{}},{id:43340,sequence:37,track:"Paper",session:[42229],links:{DOI:"https://dl.acm.org/doi/10.1145/3380959"},authors:["Marc McGill","Aidan Kehoe","Euan Freeman","Stephen Brewster"],affiliations:["University of Glasgow"],title:"Expanding the Bounds of Seated Virtual Workspaces",keywords:["rotational gain","virtual displays","Virtual reality","displays","productivity","virtual desktops","multi-monitor","display space","mixed reality","augmented reality","workspaces"],abstract:"Mixed Reality (MR), Augmented Reality (AR) and Virtual Reality (VR) headsets can improve upon existing physical multi-display environments by rendering large, ergonomic virtual display spaces whenever and wherever they are needed. However, given the physical and ergonomic limitations of neck movement, users may need assistance to view these display spaces comfortably. Through two studies, we developed new ways of minimising the physical effort and discomfort of viewing such display spaces. We first explored how the mapping between gaze angle and display position could be manipulated, helping users view wider display spaces than currently possible within an acceptable and comfortable range of neck movement. We then compared our implicit control of display position based on head orientation against explicit user control, finding significant benefits in terms of user preference, workload and comfort for implicit control. Our novel techniques create new opportunities for productive work by leveraging MR headsets to create interactive wide virtual workspaces with improved comfort and usability. These workspaces are flexible and can be used on-the-go, e.g., to improve remote working or make better use of commuter journeys.",qa:"discord://discordapp.com/channels/734723053036109904/764103703199612955",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42060,sequence:38,track:"Paper",session:[42219],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415826"},authors:["Jiahao Li","Meilin Cui","Jeeeun Kim","Xiang 'Anthony' Chen"],affiliations:["UCLA","University of California, Los Angeles","Texas A&M University"],title:"Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities",keywords:[],abstract:"Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair’s armrest, a piggybank can reach out an arm to ’steal’ coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such ‘transformables’ using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo—a design tool for creating transformables embedded into a 3D model to robotically augment the object’s default functionalities. Romeo allows users to express at a high level, (1) which part of the object to be transformed, (2) how it moves following motion points in space, and (3) the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. We validated Romeo with a design session where 8 participants design and create custom transformables using 3D objects of their own choice.",qa:"discord://discordapp.com/channels/734723053036109904/764103695041429524",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=9juk3HjLKHc","Short Talk":"https://www.youtube.com/watch?v=C3yMg1hZB9Y"},images:{}},{id:43341,sequence:39,track:"Paper",session:[42221],links:{DOI:"https://dx.doi.org/10.1145/3418196"},authors:["Blaine Levis","Daniel Vogel"],affiliations:["University of Waterloo"],title:"Longer Delays in Rehearsal-based Interfaces Increase Expert Use",keywords:["Learning","Novice Mode","Expert Mode","Rehearsal","Menus"],abstract:"Rehearsal-based interfaces are designed to encourage a transition from novice to expert, but many users fail to make this transition. Most of these interfaces activate novice mode after a short delay, between 150 and 500ms. We investigate the impact of delay time on expert usage and learning in three crowdsourced experiments. The first experiment examines an 8-item marking menu with delay times from 200ms to 2~seconds. Results show longer delays increase successful expert selections. The second and third experiments generalise this result to a different rehearsal-based menu, a desktop clone of FastTap with 8-items and 15-items. Together, our results show that expert use correlates positively with increased delay time, but can increase errors since users are less risk averse. We also find imperceptible delays of 200ms can harm long term retention of menu items. Designers should consider longer delays in rehearsal-based interfaces to encourage a transition to expert usage.",qa:"discord://discordapp.com/channels/734723053036109904/764103697260347402",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42061,sequence:40,track:"Paper",session:[42213],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415888"},authors:["Thomas Preindl","Cedric Honnet","Andreas Pointner","Roland Aigner","Joe Paradiso","Michael Haller"],affiliations:["University of Applied Sciences Upper Austria","Media Lab","MIT Media Lab"],title:"Sonoflex: Embroidered Speakers Without Permanent Magnets",keywords:[],abstract:"We present Sonoflex, a thin-form, embroidered dynamic speaker made without using a permanent magnet. Our design consists of two flat spiral coils, stacked on top of each other, and is based on an isolated, thin (0.15 mm) enameled copper wire. Our approach allows for thin, lightweight, and textile speakers and does not require high voltage as in electrostatic speakers. We show how the speaker can be designed and fabricated and evaluate its acoustic properties as a function of manufacturing parameters (size, turn counts, turn spacing, and substrate materials). The experiment results revealed that we can produce audible sound with a broad frequency range (1.5 kHz - 20 kHz) with the embroidered speaker with a diameter of 50 mm. We conclude the paper by presenting several applications such as audible notifications and near-ultrasound communication.",qa:"discord://discordapp.com/channels/734723053036109904/764103690373562368",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=5coB7Ekreac","Short Talk":"https://www.youtube.com/watch?v=mZrntLy5E0c"},images:{}},{id:42062,sequence:41,track:"Paper",session:[42224],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415844"},authors:["Matthias R. Hohmann","Lisa Konieczny","Michelle Hackl","Brian Wirth","Talha Zaman","Raffi Enficiaud","Moritz Grosse-Wentrup","Bernhard Schölkopf"],affiliations:["Max Planck Institute for Intelligent Systems","University of Vienna"],title:"MYND: Unsupervised Evaluation of Novel BCI Control Strategies on Consumer Hardware",keywords:[],abstract:"Neurophysiological laboratory studies are often constraint to immediate geographical surroundings and access to equipment may be temporally restricted. Limitations of ecological validity, scalability, and generalizability of findings pose a significant challenge for the development of brain-computer interfaces (BCIs), which ultimately need to function in any context, on consumer-grade hardware. We introduce MYND: An open-source framework that couples consumer-grade recording hardware with an easy-to-use application for the unsupervised evaluation of BCI control strategies. Subjects are guided through experiment selection, hardware fitting, recording, and data upload in order to self-administer multi-day studies that include neurophysiological recordings and questionnaires at home. As a use case, thirty subjects evaluated two BCI control strategies (“Positive memories” and “Music imagery”) by using a four-channel electroencephalogram (EEG) with MYND. Neural activity in both control strategies could be decoded with an average offline accuracy of 68.5% and 64.0% across all days. ",qa:"discord://discordapp.com/channels/734723053036109904/764103697998675988",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{Preview:"https://www.youtube.com/watch?v=KxnbNAP2DDk","Short Talk":"https://www.youtube.com/watch?v=MEBZ2lwOTrk"},images:{}},{id:42063,sequence:42,track:"Paper",session:[42210],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415898"},authors:["Junyi Zhu","Yunyi Zhu","Jiaming Cui","Leon Cheng","Jackson C Snowden","Mark Chounlakone","Michael Wessely","Stefanie Mueller"],affiliations:["MIT CSAIL"],title:"MorphSensor: A 3D Electronic Design Tool for Reforming Sensor Modules",keywords:[],abstract:"MorphSensor is a 3D electronic design tool that enables designersto morph existing sensor modules of pre-defined twodimensional shape into free-form electronic component arrangements that better integrate with the three-dimensional shape of a physical prototype. \r\nMorphSensor builds onto existing sensor module schematics that already define the electronic components and the wiring required to build the sensor. Since MorphSensor maintains the wire connections throughout the editing process, the sensor remains fully functional even when designers change the electronic component layout on the prototype geometry. \r\nWe detail the MorphSensor editor that supports designers in re-arranging the electronic components, and discuss a fabrication pipeline based on customized PCB footprints for making the resulting freeform sensor. We then demonstrate the capabilities of our system by morphing a range of sensor modules of different complexity and provide a technical evaluation of the quality of the resulting free-form sensors.",qa:"discord://discordapp.com/channels/734723053036109904/764103689504817203",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=B93djg_u9_c","Short Talk":"https://www.youtube.com/watch?v=plwXSmprcKw"},images:{}},{id:42064,sequence:43,track:"Paper",session:[42223],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415862"},authors:["Neung Ryu","Woojin Lee","Myung Jin Kim","Andrea Bianchi"],affiliations:["KAIST"],title:"ElaStick: A Handheld Variable Stiffness Display for Rendering Dynamic Haptic Response of Flexible Object",keywords:[],abstract:"Haptic controllers have an important role in providing rich and immersive Virtual Reality (VR) experiences. While previous works have succeeded in creating handheld devices that simulate dynamic properties of rigid objects, such as weight, shape, and movement, recreating the behavior of fexible objects with different stiffness using ungrounded controllers remains an open challenge. In this paper we present ElaStick, a variablestiffness controller that simulates the dynamic response resulting from shaking or swinging fexible virtual objects. This is achieved by dynamically changing the stiffness of four custom elastic tendons along a joint that effectively increase and reduce the overall stiffness of a perceived object in 2-DoF. We show that with the proposed mechanism, we can render stiffness with high precision and granularity in a continuous range between 10.8 and 71.5Nmm/◦. We estimate the threshold of the human perception of stiffness with a just-noticeable difference (JND) study and investigate the levels of immersion, realism and enjoyment using a VR application. ",qa:"discord://discordapp.com/channels/734723053036109904/764103701932933150",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{Preview:"https://www.youtube.com/watch?v=j9XP0jniAYo","Short Talk":"https://www.youtube.com/watch?v=kkDMDuDpv9s"},images:{}},{id:42065,sequence:44,track:"Paper",session:[42229],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415893"},authors:["Thibault Louis","Jocelyne Troccaz","Amélie Rochet-Capellan","François Bérard"],affiliations:["Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG","CNRS - Univ. Grenoble Alpes","CNRS","University of Grenoble Alpes"],title:"GyroSuite: General-Purpose Interactions for Handheld Perspective Corrected Displays",keywords:[],abstract:"Handheld Perspective-Corrected Displays (HPCDs) are physical objects that have a notable volume and that display a virtual 3D scene on their entire surface. Being handheld, they create the illusion of holding the scene in a physical container (the display). This has strong benefits for the intuitiveness of 3D interaction: manipulating objects of the virtual scene amounts to physical manipulations of the display. HPCDs have been limited so far to technical demonstrators and experimental tools to assess their merits. However, they show great potential as interactive systems for actual 3D applications. This requires that novel interactions be created to go beyond object manipulation and to offer general-purpose services such as menu command selection and continuous parameter control.\r\n\r\nWorking with a two-handed spherical HPCD, we report on the design and informal evaluations of various interaction techniques for distant object selection, scene scaling, menu interaction and continuous parameter control. In particular, our design leverages the efficient two-handed control of the rotations of the display. We demonstrate how some of these techniques can be assemble in a self-contained anatomy learning application. Novice participants used the application in a qualitative user experiment. Most participants used the application effortlessly without any training or explanations.",qa:"discord://discordapp.com/channels/734723053036109904/764103703199612955",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=y-qfJeRKR5E",Preview:"https://www.youtube.com/watch?v=NtTkd_Ci_X0","Short Talk":"https://www.youtube.com/watch?v=RjQQk3HUix0"},images:{}},{id:42066,sequence:45,track:"Paper",session:[42230],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415849"},authors:["Jeremy Hartmann","Yen-Ting Yeh","Daniel Vogel"],affiliations:["University of Waterloo"],title:"AAR: Augmenting a Wearable Augmented Reality Display with an Actuated Head-Mounted Projector ",keywords:[],abstract:"Current wearable AR devices create an isolated experience with a limited feld of view, vergence-accommodation conficts, and diffculty communicating the virtual environment to observers. To address these issues and enable new ways to visualize, manipulate, and share virtual content, we introduce Augmented Augmented Reality (AAR) by combining a wearable AR display with a wearable spatial augmented reality projector. To explore this idea, a system is constructed to combine a head-mounted actuated pico projector with a Hololens AR headset. Projector calibration uses a modifed structure from motion pipeline to reconstruct the geometric structure of the pan-tilt actuator axes and offsets. A toolkit encapsulates a set of high-level functionality to manage content placement relative to each augmented display and the physical environment. Demonstrations showcase ways to utilize the projected and head-mounted displays together, such as expanding feld of view, distributing content across depth surfaces, and enabling bystander collaboration. ",qa:"discord://discordapp.com/channels/734723053036109904/764103687504396288",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=RaiQtPd8TjU","Short Talk":"https://www.youtube.com/watch?v=UBEpEyHtz6k"},images:{}},{id:42067,sequence:46,track:"Paper",session:[42224],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415852"},authors:["Homei Miyashita"],affiliations:["Meiji University"],title:"Taste Display that Reproduces Tastes Measured by a Taste Sensor",keywords:[],abstract:"Aiming for the creation and development of taste media, a taste display was developed in this study that can reproduce tastes measured using taste sensors. By performing iontophoresis on five gels, which contain dissolved electrolytes that reproduce the five basic tastes, the quantity of ions that contact the tongue was controlled. A tasteless gel was added, so that the sum of the currents flowing in the six gels could be kept constant, ensuring a uniform amount of stimulation on the tongue. The measured tastes could be successfully reproduced through calibration, in which the indicated taste levels were matched with the taste-sensor measurements. Furthermore, video-editing software was adapted to edit taste information as well as recorded audio and video. In addition, effector and equalizer prototypes were built that can not only reproduce the recorded tastes in their original states but also adjust the tastes to match individual preferences.",qa:"discord://discordapp.com/channels/734723053036109904/764103697998675988",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{Preview:"https://www.youtube.com/watch?v=x5qrAULtPgQ","Short Talk":"https://www.youtube.com/watch?v=jmbeyTgkK4Y"},images:{}},{id:42068,sequence:47,track:"Paper",session:[42213],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415886"},authors:["Tony Wu","Shiho Fukuhara","Nicholas Gillian","Kishore Sundara-Rajan","Ivan Poupyrev"],affiliations:["Google"],title:"ZebraSense: A Double-sided Textile Touch Sensor for Smart Apparel",keywords:[],abstract:'ZebraSense is a novel dual-sided woven touch sensor that can recognize and differentiate interactions on the top and bottom surfaces of the sensor. ZebraSense is based on an industrial multi-layer textile weaving technique, yet it enbales a novel capacitive sensing paradigm, where each sensing element contributes to touch detection on both surfaces of the sensor simultaneously. Unlike the common "sensor sandwich" approach used in previous work, ZebraSense inherently minimizes the number of sensing elements, which drastically simplifies both sensor construction and its integration into soft goods, while preserving maximum sensor resolution. The experimental evaluation confirmed the validity of our approach and demonstrated that ZebraSense is a reliable, efficient, and accurate solution for detecting user gestures in various dual-sided interaction scenarios, allowing for new use cases in smart apparel, home decoration, toys, and other textile objects. ',qa:"discord://discordapp.com/channels/734723053036109904/764103690373562368",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=FYlAoqQ7NZM","Short Talk":"https://www.youtube.com/watch?v=EUrgqFPI4G0"},images:{}},{id:42069,sequence:48,track:"Paper",session:[42221],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415592"},authors:["C. Ailie Fraser","Julia M. Markel","N. James Basa","Mira Dontcheva","Scott Klemmer"],affiliations:["UC San Diego","Adobe Research"],title:"ReMap: Lowering the Barrier to Help-Seeking with Multimodal Search",keywords:[],abstract:"People often seek help online while using complex software. Currently, information search takes users’ attention away from the task at hand by creating a separate search task. This paper investigates how multimodal interaction can make in-task helpseeking easier and faster. We introduce ReMap, a multimodal search interface that helps users find video assistance while using desktop and web applications. Users can speak search queries, add application-specific terms deictically (e.g., “how to erase this”), and navigate search results via speech, all without taking their hands (or mouse) off their current task. Thirteen participants who used ReMap in the lab found that it helped them stay focused on their task while simultaneously searching for and using learning videos. Users’ experiences with ReMap also raised a number of important challenges with implementing system-wide context-aware multimodal assistance. ",qa:"discord://discordapp.com/channels/734723053036109904/764103697260347402",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=uZKy0akdFVY",Preview:"https://www.youtube.com/watch?v=WblSz53znl8","Short Talk":"https://www.youtube.com/watch?v=d2uGrgO2A1A"},images:{}},{id:42070,sequence:49,track:"Paper",session:[42220],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415837"},authors:["Chaeyong Park","Jinhyuk Yoon","Seungjae Oh","Seungmoon Choi"],affiliations:["Pohang University of Science and Technology","POSTECH","Pohang University of Science and Technology (POSTECH)"],title:"Augmenting Physical Buttons with Vibrotactile Feedback for Programmable Feels",keywords:[],abstract:"Physical buttons provide clear haptic feedback when pressed and released, but their responses are unvarying. Physical buttons can be powered by force actuators to produce unlimited click sensations, but the cost is substantial. An alternative can be augmenting physical buttons with simple and inexpensive vibration actuators. When pushed, an augmented button generates a vibration overlayed on the button’s original kinesthetic response, under the general framework of haptic augmented reality. We explore the design space of augmented buttons while changing vibration frequency, amplitude, duration, and envelope. We then visualize the perceptual structure of augmented buttons by estimating a perceptual space for 7 physical buttons and 40 augmented buttons. Their sensations are also assessed against adjectives, and results are mapped into the perceptual space to identify meaningful perceptual dimensions. Our results contribute to understanding the benefits and limitations of programmable vibration-augmented physical buttons with emphasis on their feels.",qa:"discord://discordapp.com/channels/734723053036109904/764103696476536863",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=TZnR2OuxyAY",Preview:"https://www.youtube.com/watch?v=GlMEwPrSew0","Short Talk":"https://www.youtube.com/watch?v=c6BUkA2r2Mw"},images:{}},{id:42071,sequence:50,track:"Paper",session:[42203],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415870"},authors:["Eric J Gonzalez","Parastoo Abtahi","Sean Follmer"],affiliations:["Stanford University"],title:"REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR",keywords:[],abstract:"Encountered-type haptic devices (EHDs) face a number of challenges when physically embodying content in a virtual environment, including workspace limits and device latency. To address these issues, we propose REACH+, a framework for dynamic visuo-haptic redirection to improve the perceived performance of EHDs during physical interaction in VR. Using this approach, we estimate the user’s arrival time to their intended target and redirect their hand to a point within the EHD’s spatio-temporally reachable space. We present an evaluation of this framework implemented with a desktop mobile robot in a 2D target selection task, tested at four robot speeds (20, 25, 30 and 35 cm/s). Results suggest that REACH+ can improve the performance of lower-speed EHDs, increasing their rate of on-time arrival to the point of contact by up to 25% and improving users’ self-reported sense of realism. ",qa:"discord://discordapp.com/channels/734723053036109904/764103682840592394",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42072,sequence:51,track:"Paper",session:[42226],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415897"},authors:["Erwin Wu","Ye Yuan","Hui-Shyong Yeo","Aaron J Quigley","Hideki Koike","Kris Kitani"],affiliations:["Carnegie Mellon University","Tokyo Institute of Technology","University of St Andrews"],title:"Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-worn Camera via Dorsum Deformation Network",keywords:[],abstract:"The automatic recognition of how people use their hands and fingers in natural settings – without instrumenting the fngers – can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fngers scarcely visible. To address this, a special network that observes deformations on the back of the hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress fnger joint angles from spatio-temporal features of the dorsal hand region (the movement of bones, muscle, and tendons). This work is the frst vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81° for user-specifc models and 9.77° for a general model. Further evaluation shows that our system outperforms previous work with an average of 20% higher accuracy in recognizing dynamic gestures, and achieves a 75% accuracy of detecting 11 different grasp types. We also demonstrate 3 applications which employ our system as a control device, an input device, and a grasped object recognizer. ",qa:"discord://discordapp.com/channels/734723053036109904/764103702764060682",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=fby54OeJrP0",Preview:"https://www.youtube.com/watch?v=pb3R4su2Fok","Short Talk":"https://www.youtube.com/watch?v=CD7j9ZX25Lo"},images:{}},{id:42073,sequence:52,track:"Paper",session:[42204],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415814"},authors:["Peggy Chi","Zheng Sun","Katrina Panovich","Irfan Essa"],affiliations:["Google Research","Google"],title:"Automatic Video Creation From a Web Page",keywords:[],abstract:"Creating marketing videos from scratch can be challenging, especially when designing for multiple platforms with different viewing criteria. We present URL2Video, an automatic approach that converts a web page into a short video given temporal and visual constraints. URL2Video captures quality materials and design styles extracted from a web page, including fonts, colors, and layouts. Using constraint programming, URL2Video’s design engine organizes the visual assets into a sequence of shots and renders to a video with user-specified aspect ratio and duration. Creators can review the video composition, modify constraints, and generate video variation through a user interface. We learned the design process from designers and compared our automatically generated results with their creation through interviews and an online survey. The evaluation shows that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process.",qa:"discord://discordapp.com/channels/734723053036109904/764103683565813790",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{Preview:"https://www.youtube.com/watch?v=HFcS5ByNShk","Short Talk":"https://www.youtube.com/watch?v=BezEjP4TCD4"},images:{}},{id:42074,sequence:53,track:"Paper",session:[42209],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415861"},authors:["Hugo Romat","Christopher Collins","Nathalie Henry Riche","Michel Pahud","Christian Holz","Adam Riddle","William Buxton","Ken Hinckley"],affiliations:["Microsoft Research","ETH Zurich","Ontario Tech University","Microsoft Expat"],title:"Tilt-Responsive Techniques for Digital Drawing Boards",keywords:[],abstract:"Drawing boards offer a self-stable work surface that is continuously adjustable. On digital displays, such as the Microsoft Surface Studio, these properties open up a class of\r\ntechniques that sense and respond to tilt adjustments. Each display posture—whether angled high, low, or somewhere in-between—affords some activities, but not others. Because what is appropriate also depends on the application and task, we explore a range of app-specific transitions between reading vs. writing (annotation), public vs. personal, shared person-space vs. task-space, and other nuances of input and\r\nfeedback, contingent on display angle. Continuous responses provide interactive transitions tailored to each use-case. We show how a variety of knowledge work scenarios can use sensed display adjustments to drive context-appropriate transitions, as well as technical software details of how to best realize these concepts. A preliminary remote user study suggests that techniques must balance effort required to adjust tilt, versus the potential benefits of a sensed transition.",qa:"discord://discordapp.com/channels/734723053036109904/764103687907835925",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=Km27fKpGpVo","Short Talk":"https://www.youtube.com/watch?v=Hk2OA05M4Do"},images:{}},{id:42075,sequence:54,track:"Paper",session:[42205],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415885"},authors:["Danny Leen","Nadya Peek","Raf Ramakers"],affiliations:["KULeuven","University of Washington","Flanders Make - Expertise Centre for Digital Media"],title:"LamiFold: Fabricating Objects with Integrated Mechanisms Using a Laser cutter Lamination Workflow",keywords:[],abstract:"We present LamiFold, a novel design and fabrication workflow for making functional mechanical objects using a laser cutter. Objects fabricated with LamiFold embed advanced rotary, linear, and chained mechanisms, including linkages that support fine-tuning and locking position. Laser cutting such mechanisms without LamiFold requires designing for and embedding off-the-shelf parts such as springs, bolts, and axles for gears. The key to laser cutting our functional mechanisms is the selective cutting and gluing of stacks of sheet material. Designing mechanisms for this workflow is non-trivial, therefore we contribute a set of mechanical primitives that are compatible with our lamination workflow and can be combined to realize advanced mechanical systems. Our software design environment facilitates the process of inserting and composing our mechanical primitives and realizing functional laser-cut objects.",qa:"discord://discordapp.com/channels/734723053036109904/764103685126881321",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=IRaqAzstVqE","Short Talk":"https://www.youtube.com/watch?v=z98b2c3znZA"},images:{}},{id:42076,sequence:55,track:"Paper",session:[42227],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415839"},authors:["Thomas Ball","Shannon Kao","Richard Knoll","Daryl Zuniga"],affiliations:["Microsoft"],title:"Creation of Video Games on Gaming Handhelds",keywords:[],abstract:"We present TileCode, a video game creation environment that runs on battery-powered microcontroller-based gaming handhelds. Our work is motivated by the popularity of retro video games, the availability of low-cost gaming handhelds loaded with many such games, and the concomitant lack of a means to create games on the same handhelds. With TileCode, we seek to close the gap between the consumers and creators of video games and to motivate more individuals to participate in the design and creation of their own games. The TileCode programming model is based on tile maps and provides a visual means for specifying the context around a sprite, how a sprite should move based on that context, and what should happen upon sprite collisions. We demonstrate that a variety of popular video games can be programmed with TileCode using 10-15 visual rules and compare/contrast with block-based versions of the same games implemented using MakeCode Arcade.",qa:"discord://discordapp.com/channels/734723053036109904/764103698750111794",broadcast:"https://zoom.us/j/96611244915?pwd=NU5UVjhvak1abDF6MlI1bVgvVXlWUT09",videos:{Preview:"https://www.youtube.com/watch?v=GpWb847x3BQ","Short Talk":"https://www.youtube.com/watch?v=R0K-3yR6Hn4"},images:{}},{id:42077,sequence:56,track:"Paper",session:[42218],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415896"},authors:["Shota Yamanaka","Hiroki Usuba","Haruki Takahashi","Homei Miyashita"],affiliations:["Yahoo Japan Corporation","Meiji University"],title:"Servo-Gaussian Model to Predict Success Rates in Manual Tracking: Path Steering and Pursuit of 1D Moving Target",keywords:[],abstract:"We propose a Servo-Gaussian model to predict success rates in continuous manual tracking tasks. Two tasks were conducted to validate this model: path steering and pursuit of a 1D moving target. We hypothesized that (1) hand movements follow the servo-mechanism model, (2) submovement endpoints form a bivariate Gaussian distribution, thus enabling us to predict the success rate at which a submovement endpoint falls inside the tolerance, and (3) the success rate for a whole trial can be predicted if the number of submovements is known. The cross-validation showed R^2 > 0.92 and MAE < 4.9% for steering and R^2 > 0.95 and MAE < 6.5% for pursuit tasks. These results demonstrate that our proposed model delivers high prediction accuracy even for unknown datasets.",qa:"discord://discordapp.com/channels/734723053036109904/764103693175357461",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=7VkZDV64cFk","Short Talk":"https://www.youtube.com/watch?v=ZlWctxBMgn4"},images:{}},{id:42078,sequence:57,track:"Paper",session:[42210],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415860"},authors:["Richard Lin","Rohit Ramesh","Connie Chi","Nikhil Jain","Ryan Nuqui","Prabal Dutta","Bjoern Hartmann"],affiliations:["University of California, Berkeley","UC Berkeley"],title:"Polymorphic Blocks: unifying high-level specification and low-level control for circuit board design",keywords:[],abstract:"Mainstream board-level circuit design tools work at the lowest level of design — schematics and individual components. While novel tools are experimenting with higher levels of design, abstraction often comes at the expense of the fine-grained control afforded by low-level tools. In this work, we propose a hardware description language (HDL) approach that supports users at multiple levels of abstraction from broad system architecture to subcircuits and component selection. We extend the familiar hierarchical block diagram with polymorphism to include abstract-typed blocks (e.g., generic resistor supertype) and electronics modeling (i.e., currents and voltages). Such an approach brings the advantages of reusability and encapsulation from object-oriented programming, while addressing the unique needs of electronics designers such as physical correctness verification. We discuss the system design, including fundamental abstractions, the block diagram construction HDL, and user interfaces to inspect and fine-tune the design; demonstrate example designs built with our system; and present feedback from intermediate-level engineers who have worked with our system.",qa:"discord://discordapp.com/channels/734723053036109904/764103689504817203",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=RdG9UkWaCYM","Short Talk":"https://www.youtube.com/watch?v=rimuZNuejmc"},images:{}},{id:42079,sequence:58,track:"Paper",session:[42217],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415881"},authors:["Ruofei Du","Eric Lee Turner","Max Dzitsiuk","Luca Prasso","Ivo Ferrao Duarte","Jason Dourgarian","Joao Afonso","Jose Pascoal","Josh Gladstone","Nuno Cruces","Shahram Izadi","Adarsh Kowdle","Konstantine Tsotsos","David Kim"],affiliations:["Google","Google Inc."],title:"DepthLab: Real-time 3D Interaction with Depth Maps for Mobile Augmented Reality",keywords:[],abstract:"Mobile devices with passive depth sensing capabilities are ubiquitous, and recently active depth sensors have become available on some tablets and AR/VR devices. Although realtime depth data is accessible, its rich value to mainstream AR applications has been sorely under-explored. Adoption of depth-based UX has been impeded by the complexity of performing even simple operations with raw depth data, such as detecting intersections or constructing meshes. In this paper, we introduce DepthLab, a software library that encapsulates a variety of depth-based UI/UX paradigms, including geometry-aware rendering (occlusion, shadows), surface interaction behaviors (physics-based collisions, avatar path planning), and visual effects (relighting, 3D-anchored focus and aperture effects). We break down the usage of depth into localized depth, surface depth, and dense depth, and describe our real-time algorithms for interaction and rendering tasks. We present the design process, system, and components of DepthLab to streamline and centralize the development of interactive depth features. We have open-sourced our software at https://github.com/googlesamples/arcore-depth-lab to external developers, conducted performance evaluation, and discussed how DepthLab can accelerate the workflow of mobile AR designers and developers. With DepthLab we aim to help mobile developers to effortlessly integrate depth into their AR experiences and amplify the expression of their creative vision.",qa:"discord://discordapp.com/channels/734723053036109904/764103695754592256",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42080,sequence:59,track:"Paper",session:[42209],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415857"},authors:["Wenzhe Cui","Suwen Zhu","Mingrui Ray Zhang","Andrew Schwartz","Jacob O. Wobbrock","Xiaojun Bi"],affiliations:["Stony Brook University","University of Washington"],title:"JustCorrect: Intelligent Post Hoc Text Correction Techniques on Smartphones",keywords:[],abstract:'Correcting errors in entered text is a common task but usually difficult to perform on mobile devices due to tedious cursor navigation steps. In this paper, we present JustCorrect, an intelligent post hoc text correction technique for smartphones. To make a correction, the user simply types the correct text at the end of their current input, and JustCorrect will automatically detect the error and apply the correction in the form of an insertion or a substitution. In this way, manual navigation steps are bypassed, and the correction can be committed with a single tap. We solved two critical problems to support JustCorrect: (1) Correction Algorithm: we propose an algorithm that infers the user’s correction intention from the last typed word. (2) Input Modalities: our study revealed that both tap and gesture were suitable input modalities for performing JustCorrect. Based on our findings, we integrated JustCorrect into a soft keyboard. Our user studies show that using JustCorrect reduces the text correction time by 12.8% over the stock Android keyboard and by 9.7% over the "Type, then Correct" text correction technique by Zhang et al. (2019). Overall, JustCorrect complements existing post hoc text correction techniques, making error correction more automatic and intelligent.',qa:"discord://discordapp.com/channels/734723053036109904/764103687907835925",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=x1OH-uTT524","Short Talk":"https://www.youtube.com/watch?v=gdCryupTzpg"},images:{}},{id:42081,sequence:60,track:"Paper",session:[42214],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415835"},authors:["Xiao TANG","Xiaowei Hu","Chi-Wing Fu","Daniel CohenOr"],affiliations:["The Chinese University of Hong Kong","Tel Aviv University"],title:"GrabAR: Occlusion-aware Grabbing Virtual Objects in AR",keywords:[],abstract:"Existing augmented reality (AR) applications often ignore the occlusion between real hands and virtual objects when incorporating virtual objects in user’s views. The challenges come from the lack of accurate depth and mismatch between real and virtual depth. This paper presents GrabAR1, a new approach that directly predicts the real-and-virtual occlusion and bypasses the depth acquisition and inference. Our goal is to enhance AR applications with interactions between hand (real) and grabbable objects (virtual). With paired images of hand and object as inputs, we formulate a compact deep neural network that learns to generate the occlusion mask. To train the network, we compile a large dataset, including synthetic data and real data. We then embed the trained network in a prototyping AR system to support real-time grabbing of virtual objects. Further, we demonstrate the performance of our method on various virtual objects, compare our method with others through two user studies, and showcase a rich variety of interaction scenarios, in which we can use bare hand to grab virtual objects and directly manipulate them.",qa:"discord://discordapp.com/channels/734723053036109904/764103691979849729",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{Preview:"https://www.youtube.com/watch?v=TZgepi4ssP0","Short Talk":"https://www.youtube.com/watch?v=2c1MsioTC9o"},images:{}},{id:42082,sequence:61,track:"Paper",session:[42197],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415841"},authors:["Jackie (Junrui) Yang","Monica Lam","James A. Landay"],affiliations:["Stanford University"],title:"DoThisHere: Using multi-modal interaction to support cross-application tasks on mobile devices",keywords:[],abstract:"Many computing tasks, such as comparison shopping, twofactor authentication, and checking movie reviews, require using multiple apps together. On large screens, “windows, icons, menus, pointer” (WIMP) graphical user interfaces (GUIs) support easy sharing of content and context between multiple apps. So, it is easy to see the content from one application and write something relevant in another application, such as looking at the map around a place and typing walking instructions into an email. However, although today’s smartphones also use GUIs, they have small screens and limited windowing support, making it hard to switch contexts and exchange data between apps.\r\n\r\nWe introduce DoThisHere, a multimodal interaction technique that streamlines cross-app tasks and reduces the burden thesetasks impose on users. Users can use voice to refer to information or app features that are off-screen and touch to specify where the relevant information should be inserted or is displayed. With DoThisHere, users can access information from or carry information to other apps with less context switching.\r\n\r\nWe conducted a survey to find out what cross-app tasks people are performing or wish to perform on their smartphones. Among the 125 tasks that we collected from 75 participants, we found that 59 of these tasks are not well supported currently. DoThisHere is helpful in completing 95% of these unsupported tasks. A user study, where users are shown the list of supported voice commands when performing a representative sample of such tasks, suggests that DoThisHere may reduce expert users’ cognitive load; the Query action, in particular, can help users reduce task completion time.",qa:"discord://discordapp.com/channels/734723053036109904/764103679011192884",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42083,sequence:62,track:"Paper",session:[42225],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415588"},authors:["Karan Ahuja","Andy Kong","Mayank Goel","Chris Harrison"],affiliations:["Carnegie Mellon University"],title:"Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Device Ecosystems",keywords:[],abstract:"Future homes and offices will feature increasingly dense ecosystems of IoT devices, such as smart lighting, speakers, and domestic appliances. Voice input is a natural candidate for interacting with out-of-reach and often small devices that lack full-sized physical interfaces. However, at present, voice agents generally require wake-words and device names in order to specify the target of a spoken command (e.g., “Hey Alexa, kitchen lights to full brightness”). In this research, we explore whether speech alone can be used as a directional communication channel, in much the same way visual gaze specifies a focus. Instead of a device’s microphones simply receiving and processing spoken commands, we suggest they also infer the Direction of Voice (DoV). Our approach innately enables voice commands with addressability (i.e., devices know if a command was directed at them) in a natural and rapid manner. We quantify the accuracy of our implementation across users, rooms, spoken phrases, and other key factors that affect performance and usability. Taken together, we believe our DoV approach demonstrates feasibility and the promise of making distributed voice interactions much more intuitive and fluid.",qa:"discord://discordapp.com/channels/734723053036109904/764103700234895360",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=JWndA1AlLE4","Short Talk":"https://www.youtube.com/watch?v=tWL6YGYTj8I"},images:{}},{id:42339,sequence:1,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416189",Document:"http://uist.acm.org/uist2020/data/posters/1064.pdf"},authors:["Jonas Auda","Roman Heger","Thomas Kosch","Uwe Gruenefeld","Stefan Schneegass"],affiliations:["University of Duisburg-Essen","TU Darmstadt"],title:"EasyEG: A 3D-printable Brain-Computer Interface ",keywords:[],abstract:"Brain-Computer Interfaces (BCIs) are progressively adopted by the consumer market, making them available for a variety of use-cases. However, off-the-shelf BCIs are limited in their adjustments towards individual head shapes, evaluation of scalp-electrode contact, and extension through additional sensors. This work presents EasyEG, a BCI headset that is adaptable to individual head shapes and offers adjustable electrode-scalp contact to improve measuring quality. EasyEG consists of 3D-printed and low-cost components that can be extended by additional sensing hardware, hence expanding the application domain of current BCIs. We conclude with use-cases that demonstrate the potentials of our EasyEG headset.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104010931503144",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1064.jpg"}},{id:42084,sequence:63,track:"Paper",session:[42223],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415836"},authors:["Tzu-Yun Wei","Hsin-Ruey Tsai","Yu-So Liao","Chieh Tsai","Yi Shan Chen","Chi Wang","Bing-Yu Chen"],affiliations:["National Taiwan University","National Chengchi University"],title:"ElastiLinks: Force Feedback between VR Controllers with Dynamic Points of Application of Force",keywords:[],abstract:"Force feedback is commonly used to enhance realism in virtual reality (VR). However, current works mainly focus on providing different force types or patterns, but do not investigate how a proper point of application of force (PAF), which means where the resultant force is applied to, affects users’ experience. For example, users perceive resistive force without torque when pulling a virtual bow, but with torque when pulling a virtual slingshot. Therefore, we propose a set of handheld controllers, ElastiLinks, to provide force feedback between controllers with dynamic PAFs. A rotatable track on each controller provides a dynamic PAF, and two common types of force feedback, resistive force and impact, are produced by two links, respectively. We performed a force perception study to ascertain users’ resistive and impact force level distinguishability between controllers. Based on the results, we conducted another perception study to understand users’ distinguishability of PAF offset and rotation differences. Finally, we performed a VR experience study to prove that force feedback with dynamic PAFs enhances VR experience.",qa:"discord://discordapp.com/channels/734723053036109904/764103701932933150",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{Preview:"https://www.youtube.com/watch?v=D4DNGd4j_Ec","Short Talk":"https://www.youtube.com/watch?v=VsMw4QpSgiA"},images:{}},{id:42340,sequence:2,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416195",Document:"http://uist.acm.org/uist2020/data/posters/1085.pdf"},authors:["Tim Rietz","Peyman Toreini","Alexander Maedche"],affiliations:["Karlsruhe Institute of Technology (KIT)"],title:"Cody: An Interactive Machine Learning System for Qualitative Coding",keywords:[],abstract:"Qualitative coding, the process of assigning labels to text as part of qualitative analysis, is time-consuming and repetitive, especially for large datasets. While available QDAS sometimes allows the semi-automated extension of annotations to unseen data, recent user studies revealed critical issues. In particular, the integration of automated code suggestions into the coding process is not transparent and interactive. In this work, we present Cody, a system for semi-automated qualitative coding that suggests codes based on human-defined coding rules and supervised machine learning (ML). Suggestions and rules can be revised iteratively by users in a lean interface that provides explanations for code suggestions. In a preliminary evaluation, 42% of all documents could be coded automatically based on code rules. Cody is the first coding system to allow users to define query-style code rules in combination with supervised ML. Thereby, users can extend manual annotations to unseen data to improve coding speed and quality. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764104014031749150",videos:{"Short Talk":"https://www.youtube.com/watch?v=PdRiyKDuoHA"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1085.jpg"}},{id:42085,sequence:64,track:"Paper",session:[42200],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415889"},authors:["Daehwa Kim","Keunwoo Park","Geehyuk Lee"],affiliations:["KAIST","School of Computing, KAIST"],title:"OddEyeCam: A Sensing Technique for Body-Centric Peephole Interaction using WFoV RGB and NFoV Depth Cameras",keywords:[],abstract:"The space around the body not only expands the interaction space of a mobile device beyond its small screen, but also enables users to utilize their kinesthetic sense. Therefore, body-centric peephole interaction has gained considerable attention. To support its practical implementation, we propose OddEyeCam, which is a vision-based method that tracks the 3D location of a mobile device in an absolute, wide, and continuous manner with respect to the body of a user in both static and mobile environments. OddEyeCam tracks the body of a user using a wide-view RGB camera and obtains precise depth information using a narrow-view depth camera from a smartphone close to the body. We quantitatively evaluated OddEyeCam through an accuracy test and two user studies. The accuracy test showed the average tracking accuracy of OddEyeCam was 4.17 and 4.47cm in 3D space when a participant is standing and walking, respectively. In the frst user study, we implemented various interaction scenarios and observed that OddEyeCam was well received by the participants. In the second user study, we observed that the peephole target acquisition task performed using our system followed Fitts’ law. We also analyzed the performance of OddEyeCam using the obtained measurements and observed that the participants completed the tasks with suffcient speed and accuracy. ",qa:"discord://discordapp.com/channels/734723053036109904/764103681976172594",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=u6xvk0dM8fo",Preview:"https://www.youtube.com/watch?v=R56iuEuZyo0","Short Talk":"https://www.youtube.com/watch?v=rsiCohRoFYI"},images:{}},{id:42341,sequence:3,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416174",Document:"http://uist.acm.org/uist2020/data/posters/1022.pdf"},authors:["Zhenjie Zhao","Xiaojuan Ma"],affiliations:["Hong Kong University of Science and Technology"],title:"Situated Learning of Soft Skills with an Interactive Agent in Virtual Reality via Multimodal Feedback",keywords:[],abstract:"Nowadays, customer virtual reality (VR) devices can offer high-quality presence experiences in a relatively low cost, which opens opportunities for popularizing situated learning. We investigate situated learning of soft skills, in particular, elevator pitches, through simulating an interactive agent with multimodal feedback in VR. A virtual agent acts as a coach and offers both verbal and nonverbal feedback to users. We developed a prototype system and conducted a quantitative experiment to investigate the potential of situated learning in VR, especially on how users perceive the virtual agent. We summarize design considerations on developing interactive agents with multimodal feedback in VR to facilitate situated learning of soft skills.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104017466359808",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1022.jpg"}},{id:42086,sequence:65,track:"Paper",session:[42204],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415866"},authors:["Evan Shimizu","Matthew David Fisher","Sylvain Paris","James McCann","Kayvon Fatahalian"],affiliations:["Carnegie Mellon University","Adobe Systems","Adobe","Stanford University"],title:"Design Adjectives: A Framework for Interactive Model-Guided Exploration of Parameterized Design Spaces",keywords:[],abstract:"Many digital design tasks require a user to set a large number of parameters. Gallery-based interfaces provide a way to quickly evaluate examples and explore the space of potential designs, but require systems to predict which designs from a high-dimensional space are the right ones to present to the user. In this paper we present the design adjectives framework for building parameterized design tools in high dimensional design spaces. The framework allows users to create and edit design adjectives, machine-learned models of user intent, to guide exploration through high-dimensional design spaces. We provide a domain-agnostic implementation of the design adjectives framework based on Gaussian process regression, which is able to rapidly learn user intent from only a few examples. Learning and sampling of the design adjective occurs at interactive rates, making the system suitable for iterative design workflows. We demonstrate use of the design adjectives framework to create design tools for three domains: materials, fonts, and particle systems. We evaluate these tools in a user study showing that participants were able to easily explore the design space and find designs that they liked, and in professional case studies that demonstrate the framework’s ability to support professional design concepting workflows.",qa:"discord://discordapp.com/channels/734723053036109904/764103683565813790",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=F80c8atWFBw",Preview:"https://www.youtube.com/watch?v=ZR2r6T5JC7Y","Short Talk":"https://www.youtube.com/watch?v=3W3wUUpkUZI"},images:{}},{id:42342,sequence:4,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416190",Document:"http://uist.acm.org/uist2020/data/posters/1066.pdf"},authors:["Mayra Donaji Barrera Machuca","Alvaro Cassinelli","Christian Sandor"],affiliations:["City University of Hong Kong","City University, Hong Kong"],title:"Context-Based 3D Grids for Augmented Reality User Interfaces",keywords:[],abstract:"Accurate 3D registration of real and virtual objects is a crucial step in AR, especially when manipulating those objects in space. Previous work simplifies mid-air 3D manipulations by removing one or more degrees of freedom by constraining motion using automatic algorithms. However, when designing objects, limiting the user’s actions can affect their creativity. To solve this problem, we present a new system called Context-based 3D Grids that allows users to do precise mid-air 3D manipulations without constraining their actions. Our system creates 3D grids for each object in the scene that change depending on the object pose. Users can display additional reference frames inside the virtual environment using natural hand gestures that are commonly used when designing an object. Our goal is to help users visualize more clearly the spatial relation and the differences in pose and size of the objects.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104019891191849",videos:{Preview:"https://www.youtube.com/watch?v=mcxLE1-7Ycs","Short Talk":"https://www.youtube.com/watch?v=3j0EMr3lVLI"},images:{}},{id:42087,sequence:66,track:"Paper",session:[42216],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415877"},authors:["Rinat Abdrashitov","Fanny Chevalier","Karan Singh"],affiliations:["University of Toronto"],title:"Interactive Exploration and Refinement of Facial Expression using Manifold Learning",keywords:[],abstract:"Posing expressive 3D faces is extremely challenging. Typical facial rigs have upwards of 30 controllable parameters, that while anatomically meaningful, are hard to use due to redundancy of expression, unrealistic configurations, and many semantic and stylistic correlations between the parameters. We propose a novel interface for rapid exploration and refinement of static facial expressions, based on a data-driven face manifold of “natural” expressions. Rapidly explored face configurations are interactively projected onto this manifold of meaningful expressions. These expressions can then be refined using a 2D embedding of nearby faces, both on and off the manifold. Our validation is fourfold: we show expressive face creation using various devices; we verify that our learnt manifold transcends its training face, to expressively control very different faces; we perform a crowd-sourced study to evaluate the quality of manifold face expressions; and we report on a usability study that shows our approach is an effective interactive tool to author facial expression. ",qa:"discord://discordapp.com/channels/734723053036109904/764103694244642856",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=1aPoCi83IBI","Short Talk":"https://www.youtube.com/watch?v=pGQhffnazLQ"},images:{}},{id:42343,sequence:5,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416184",Document:"http://uist.acm.org/uist2020/data/posters/1043.pdf"},authors:["HIROO YAMAMURA","Holger Baldauf","Kai Kunze"],affiliations:["Media design","Keio University"],title:"Pleasant Locomotion -- Towards Reducing Cybersickness using fNIRS during Walking Events in VR",keywords:[],abstract:"Moving in virtual reality without causing cybersickness is still an unsolved and difficult problem, especially if the virtual space is much larger than the real space and the virtual reality environment asks for quick movements. Many methods to reduce cybersickness are proposed but most of them also reduce immersion. In this paper, we explore the use of fNIRS as an additional modality to detect the level of cybersickness for movement events in VR. We try to mitigate the sickness an individual feels by narrowing the field of vision based on the sickness level detected via measuring increased deoxygenated hemoglobin (Hb) with fNIRS. Our overall goal is to reduce cybersickness in virtual reality applications using physiological signals and appropriate adjustments with little to no impact on immersion.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104022927605781",videos:{Preview:"https://www.youtube.com/watch?v=Dl6gSzWsZWY"},images:{}},{id:42088,sequence:67,track:"Paper",session:[42205],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415899"},authors:["Maria Katarina Larsson","Hironori Yoshida","Nobuyuki Umetani","Takeo Igarashi"],affiliations:["University of Tokyo","Creative Informatics","Preferred Networks Inc.","The University of Tokyo"],title:"Tsugite: Interactive Design and Fabrication of Wood Joints",keywords:[],abstract:"We present Tsugite—an interactive system for designing and fabricating wood joints for frame structures. To design and manually craft such joints is difficult and time consuming. Our system facilitates the creation of custom joints by a modeling interface combined with computer numerical control (CNC) fabrication. The design space is a 3D grid of voxels that enables efficient geometrical analysis and combinatorial search. The interface has two modes: manual editing and gallery. In the manual editing mode, the user edits a joint while receiving real-time graphical feedback and suggestions provided based on performance metrics including slidability, fabricability, and durability with regard to the direction of fiber. In the gallery mode, the user views and selects feasible joints that have been pre-calculated. When a joint design is finalized, it can be manufactured with a 3-axis CNC milling machine using a specialized path planning algorithm that ensures joint assemblability by corner rounding. This system was evaluated via a user study and by designing and fabricating joint samples and functional furniture.",qa:"discord://discordapp.com/channels/734723053036109904/764103685126881321",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=zmJaQU_fsr4",Preview:"https://www.youtube.com/watch?v=a4jNpHJ9lp0","Short Talk":"https://www.youtube.com/watch?v=mXVQBXBcAAk"},images:{}},{id:42344,sequence:6,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416173",Document:"http://uist.acm.org/uist2020/data/posters/1021.pdf"},authors:["Suzanne Kobeisse","Lars Erik Holmquist"],affiliations:["Northumbria University"],title:"ARcheoBox: Engaging with Historical Artefacts Through Augmented Reality and Tangible Interactions",keywords:[],abstract:"Museum visitors can often be distanced from directly engaging with historical artefacts due to their fragile nature. ARcheoBox is a novel interface that lets users physically pick up a digitized version of a real artefact and inspect it closely from all angles through a screen. The system combines augmented reality and tangible interactions to remove barriers between visitors and artefacts. We piloted our prototype with eight archaeological experts at a local exhibition centre. The results indicate that this is a promising way to remove barriers between humans and historical artefacts, and that it allows expert users to inspect, understand and interact with these artefacts in a way not previously possible.\r\n",broadcast:"discord://discordapp.com/channels/734723053036109904/764104026731053086",videos:{Preview:"https://www.youtube.com/watch?v=tuhgZ6qook4"},images:{}},{id:42089,sequence:68,track:"Paper",session:[42205],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415895"},authors:["Thijs Roumen","Ingo Apel","Jotaro Shigeyama","Muhammad Abdullah","Patrick Baudisch"],affiliations:["Hasso Plattner Institute","Kyung Hee University"],title:"Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate across Different Laser Cutters",keywords:[],abstract:"Getting laser-cut mechanisms, such as those in microscopes, robots, vehicles, etc., to work, requires all their components to be dimensioned precisely. This precision, however, tends to be lost when fabricating on a different laser cutter, as it is likely to remove more or less material (aka “kerf”). We address this with what we call kerf-canceling mechanisms. Kerf-canceling mechanisms replace laser-cut bearings, sliders, gear pairs, etc. Unlike their traditional counterparts, however, they keep working when manufactured on a different laser cutter and/or with different kerf. Kerf-canceling mechanisms achieve this by adding an additional wedge element per mechanism. We have created a software tool KerfCanceler that locates traditional mechanisms in cutting plans and replaces them with their kerf-canceling counterparts. We evaluated our tool by converting 17 models found online to kerf-invariant models; we evaluated kerf-canceling bearings by testing with kerf values ranging from 0mm and 0.5mm and find that they perform reliably independent of this kerf.",qa:"discord://discordapp.com/channels/734723053036109904/764103685126881321",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=Xf8NMxlpRXk",Preview:"https://www.youtube.com/watch?v=lxlXXPj0LJs","Short Talk":"https://www.youtube.com/watch?v=FkWNcHWhrhs"},images:{}},{id:42345,sequence:7,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416196",Document:"http://uist.acm.org/uist2020/data/posters/1007.pdf"},authors:["Lingyun Sun","Jiaji Li","Yu Chen","Yue Yang","Jianzhe Gu","Ye Tao","Lining Yao","Guanyun Wang"],affiliations:["Zhejiang University","Carnegie Mellon University"],title:"WireTruss: A Fast-Modifiable Prototyping Method  Through 3D Printing",keywords:[],abstract:"Digital fabrication allows users to produce physical objects from digital models. However, conventional fabrication processes are mostly irreversible: once an object is fabricated, it is detached from its original virtual model and cannot be physically changed. In this work, we propose WireTruss, a novel approach for rapid prototyping truss mesh structures, which can be rapidly fabricated, easily assembled and manually modified to support an intuitive design iteration. We developed a parametric design tool that first simplifies the object into a truss graph composed of multi-way joints, and then calculates the route of the wire which is inherently a Euler path. Furthermore, WireTruss can be demonstrated its practical usability through a series of application cases.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104029739155476",videos:{Preview:"https://www.youtube.com/watch?v=f_BbNgsK5g4","Short Talk":"https://www.youtube.com/watch?v=C3H9Em21pNY"},images:{}},{id:42090,sequence:69,track:"Paper",session:[42207],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415838"},authors:["Thomas Martin Prinz","Linda Gräfe","Jan Plötner"],affiliations:["Friedrich Schiller University Jena","Friedrich-Schiller-University Jena"],title:"Learning from the Past - Do Historical Data Help to Improve Progress Indicators in Web Surveys?",keywords:[],abstract:"As a typical part of the interface of web surveys, progress indicators show the degree of completion for participants. These progress indicators influence the dropout and response behavior as various studies suggest. For this reason, the indicator should be chosen carefully. However, calculating the progress in adaptive surveys with many branches is often difficult. Recently related work has provided algorithms for such surveys based on different prediction strategies and has identified the Root Mean Squared Error as a valuable measure to compare different strategies. However, all previously mentioned strategies have shown poor predictions in some cases. In this paper, we present a new strategy which learns from historical data. A simulation study with 10k randomly generated surveys shows its benefits and its limits. As an example of application, we confirm our results of the simulation by comparing different prediction strategies for two large real-world surveys.",qa:"discord://discordapp.com/channels/734723053036109904/764103684367056897",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{Preview:"https://www.youtube.com/watch?v=-5Sp62DQ0bI","Short Talk":"https://www.youtube.com/watch?v=AsxU7Ww64ng"},images:{}},{id:42346,sequence:8,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416176",Document:"http://uist.acm.org/uist2020/data/posters/1024.pdf"},authors:["Tai WANG","Conghui He","Zhe Wang","Jianping Shi","Dahua Lin"],affiliations:["The Chinese University of Hong Kong","Sensetime Inc","SenseTime"],title:"FLAVA: Find, Localize, Adjust and Verify to Annotate LiDAR-based Point Clouds",keywords:[],abstract:"Recent years have witnessed the rapid progress of perception algorithms on top of LiDAR, a widely adopted sensor for autonomous driving systems. These LiDAR-based solutions are typically data hungry, requiring a large amount of data to be labeled for training and evaluation. However, annotating this kind of data is very challenging due to the sparsity and irregularity of point clouds and more complex interaction involved in this procedure. To tackle this problem, we propose FLAVA, a systematic approach to minimizing human interaction in the annotation process. Specifically, we divide the annotation pipeline into four parts: find, localize, adjust and verify. In addition, we carefully design the UI for different stages of the annotation procedure, thus keeping the annotators to focus on the aspects that are most important to each stage. Furthermore, our system also greatly reduces the amount of interaction by introducing a lightweight yet effective mechanism to propagate the annotation results. Experimental results show that our method can remarkably accelerate the procedure and improve the annotation quality.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104032309739520",videos:{"Short Talk":"https://www.youtube.com/watch?v=hri54dzPxnI"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1024.jpg"}},{id:42091,sequence:70,track:"Paper",session:[42228],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415876"},authors:["Jack Forman","Mustafa Doga Dogan","Hamilton Forsythe","Hiroshi Ishii"],affiliations:["Massachusetts Institute of Technology","MIT CSAIL","MIT"],title:"DefeXtiles: 3D Printing Quasi-Woven Fabric via Under-Extrusion",keywords:[],abstract:"We present DefeXtiles, a rapid and low-cost technique to produce tulle-like fabrics on unmodified fused deposition modeling (FDM) printers. The under-extrusion of filament is a common cause of print failure, resulting in objects with periodic gap defects. In this paper, we demonstrate that these defects can be finely controlled to quickly print thinner, more flexible textiles than previous approaches allow. Our approach allows hierarchical control from micrometer structure to decameter form and is compatible with all common 3D printing materials.\r\nIn this paper, we introduce the mechanism of DefeXtiles, establish the design space through a set of primitives with detailed workflows, and characterize the mechanical properties of DefeXtiles printed with multiple materials and parameters. Finally, we demonstrate the interactive features and new use cases of our approach through a variety of applications, such as fashion design prototyping, interactive objects, aesthetic patterning, and single-print actuators. ",qa:"discord://discordapp.com/channels/734723053036109904/764103700997865472",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=_o7Duvtbd5M","Short Talk":"https://www.youtube.com/watch?v=RRLgx0-DjEQ"},images:{}},{id:42347,sequence:9,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416175",Document:"http://uist.acm.org/uist2020/data/posters/1023.pdf"},authors:["Tzu-Sheng Kuo","Eric Rawn"],affiliations:["Stanford University"],title:"Let It Rip! Using Velcro for Acoustic Labeling",keywords:[],abstract:"We present an early stage prototype of an acoustic labeling system using Velcro, a two-sided household adhesive product. We create labels by varying the shape of Velcro pieces to produce distinct sounds when the two sides are separated, and we use an automatic audio classification pipeline to detect and classify small sets of labels. We evaluate our classifier on four sets of three simple Velcro labels, present a demo highlighting potential use cases of these labels, and discuss future applications.\r\n",broadcast:"discord://discordapp.com/channels/734723053036109904/764104035263184896",videos:{"Short Talk":"https://www.youtube.com/watch?v=Nf_-AdIBb8Y"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1023.jpg"}},{id:42092,sequence:71,track:"Paper",session:[42202],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415827"},authors:["Balasaravanan Thoravi Kumaravel","Cuong Nguyen","Stephen DiVerdi","Bjoern Hartmann"],affiliations:["UC Berkeley","Adobe Research","Adobe"],title:"TransceiVR: Bridging Asymmetrical Communication Between External and VR Users",keywords:[],abstract:"Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user’s gestures, and the external user cannot see VR scene elements outside of the VR user’s view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.",qa:"discord://discordapp.com/channels/734723053036109904/764103681238237204",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=U9kOvVOx2sw","Short Talk":"https://www.youtube.com/watch?v=vtY_gbHx9hE"},images:{}},{id:42348,sequence:10,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416156",Document:"http://uist.acm.org/uist2020/data/posters/1089.pdf"},authors:["Md Tahsin Tausif","RJ Weaver","Sang Won Lee"],affiliations:["Virginia Tech","Virginia Polytechnic Institute and State University"],title:"Towards Enabling Eye Contact and Perspective Control in Video Conference",keywords:[],abstract:"Eye contact is an important part of in-person communication. \r\n  However, in modern remote communication - video conference calls, non-verbal cues through eye contact are lost. \r\n  This is because it is not possible to make eye contact in a video call; to make it look like a person is making an eye contact, the user has to look at the camera directly, but then this means that the user is not looking at the visual of the interlocutor on the screen.  \r\n  In our research, we aim to build a hardware and software system that helps the users to make eye contact in video conference and change their perspective, placing a moving camera behind a semi-transparent screen. \r\n  The system keeps the position of camera right behind the remote user's eye position on screen tracked by a computer vision algorithm, so users will be able to make eye contact during the video call.\r\n  We believe the system will lead to better user experience in remote conference calls.\r\n  In this work, we share our motivation, design choices, the current progress, and a plan to evaluate the system.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104038068781097",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1089.jpg"}},{id:42093,sequence:72,track:"Paper",session:[42223],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415854"},authors:["Robert Kovacs","Eyal Ofek","Mar Gonzalez-Franco","Alexa Fay Siu","Sebastian Marwecki","Christian Holz","Mike Sinclair"],affiliations:["Hasso Plattner Institute","Microsoft Research","Stanford University","ETH Zurich","Microsoft"],title:"Haptic PIVOT: On-Demand Handhelds in VR",keywords:[],abstract:"We present PIVOT, a wrist-worn haptic device that renders virtual objects into the user’s hand on demand. Its simple design comprises a single actuated joint that pivots a haptic handle into and out of the user’s hand, rendering the haptic sensations of grasping, catching, or throwing an object – anywhere in space. Unlike existing hand-held haptic devices and haptic gloves, PIVOT leaves the user’s palm free when not in use, allowing users to make unencumbered use of their hand. PIVOT also enables rendering forces acting on the held virtual objects, such as gravity, inertia, or air-drag, by actively driving its motor while the user is firmly holding the handle. When wearing a PIVOT device on both hands, they can add haptic feedback to bimanual interaction, such as lifting larger objects. In our user study, participants (n=12) evaluated the realism of grabbing and releasing objects of different shape and size with mean score 5.19 on a scale from 1 to 7, rated the ability to catch and throw balls in different directions with different velocities (mean=5.5), and verified the ability to render the comparative weight of held objects with 87% accuracy for ~100g increments. ",qa:"discord://discordapp.com/channels/734723053036109904/764103701932933150",broadcast:"https://acm-org.zoom.us/j/93807490097?pwd=NWZRY1lUMnpEdmplbVR3MmJManBFdz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=WkCkfCwE0DI",Preview:"https://www.youtube.com/watch?v=uOhxQ5nKO6U","Short Talk":"https://www.youtube.com/watch?v=qnnWRhwxRSk"},images:{}},{id:42349,sequence:11,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416185",Document:"http://uist.acm.org/uist2020/data/posters/1047.pdf"},authors:["Reiya Horii","Yurike Chandra","Kai Kunze","Kouta Minamizawa"],affiliations:["Keio University","Keio University Graduate School of Media Design"],title:"Bubble Visualization Overlay in Online Communication for Increased Speed Awareness and Better Turn Taking",keywords:[],abstract:"In this paper, we explore the use of a real-time speech visualization overlay to help native English speakers (NES) reflect on their speech speed and allow them to understand how English as a Foreign Language speakers (EFLS) perceive their speech. Our visual system generates a sequence of bubbles based on the speaking speed overlaid close to the user's mouth; the faster the speaking speed, the denser the bubble sequence, and vice versa. The results suggest that the presence of the speech visualization helps NES to understand their speech speed, and subsequently, it helps EFLS to feel comfortable speaking during the online group discussion.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104040736489482",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1089.jpg"}},{id:42094,sequence:73,track:"Paper",session:[42222],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415880"},authors:["Yunwoo Jeong","Han-Jong Kim","Gyeongwon Yun","Tek-Jin Nam"],affiliations:["KAIST"],title:"WIKA: A Projected Augmented Reality Workbench for Interactive Kinetic Art",keywords:[],abstract:"Iterative artistic exploration, mechanism building, and interaction programming are essential processes of prototyping interactive kinetic art (IKA). However, scattered tools and interwoven workfows across digital and physical worlds make the task diffcult. We present WIKA, an integrated environment supporting the whole creation process of IKA in the form of a layered picture frame in a single workspace. A projected AR system with a mobile device effciently makes an interactive tabletop. The projected information connected with physical components (e.g. sensors and motors) enables the programming and simulation on the workspace. Physical components are applied from the initial phase of prototyping using an AR plate, and this supports the iterative trial-and-error process by bridging the workfow. A user study shows that WIKA enabled non-experts to create diverse IKA with their ideas. A tangible interaction and projected information enable the iterative and rapid creation. The method that integrates the hardware and software in the physical environment can be applied to other prototyping tools that support the creation of interactive and kinetic elements. ",qa:"discord://discordapp.com/channels/734723053036109904/764103699140050955",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=N5C0a4D6PXk","Short Talk":"https://www.youtube.com/watch?v=ohhl9o9_cIU"},images:{}},{id:42350,sequence:12,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416166",Document:"http://uist.acm.org/uist2020/data/posters/1003.pdf"},authors:["André Zenner","Antonio Krüger"],affiliations:["German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus"],title:"Shifting & Warping: A Case for the Combined Use of Dynamic Passive Haptics and Haptic Retargeting in VR",keywords:[],abstract:"Passive haptic feedback for virtual reality can provide immersive sensations but suffers from scalability issues. To tackle these, two independent concepts have been proposed previously: Dynamic Passive Haptic Feedback (DPHF), leveraging actuated props that change their physical state, and Haptic Retargeting, redirecting the user's hand during interaction. While past research on both techniques reported promising results, up to now, these concepts remained isolated. This paper advocates the combined use of DPHF and Haptic Retargeting. We introduce two thought experiments showcasing that the combination of both techniques in an example scenario is beneficial to solve two central challenges of prop-based VR haptics: haptic similarity and co-location of proxies and virtual objects.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104043546804225",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1003.jpg"}},{id:42095,sequence:74,track:"Paper",session:[42211],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415828"},authors:["Paul Strohmeier","Seref Güngör","Luis Herres","Dennis Gudea","Bruno Fruchard","Jürgen Steimle"],affiliations:["Saarland Informatics Campus","Saarland University, Saarland Informatics Campus"],title:"bARefoot: Generating Virtual Materials using Motion Coupled Vibration in Shoes",keywords:[],abstract:"Many features of materials can be experienced through tactile cues, even using one’s feet. For example, one can easily distinguish between moss and stone without looking at the ground. However, this type of material experience is largely not supported in AR and VR applications. We present bARefoot, a prototype shoe providing tactile impulses tightly coupled to motor actions. This enables generating virtual material experiences such as compliance, elasticity, or friction. To explore the parameter space of such sensorimotor coupled vibrations, we present a design tool enabling rapid design of virtual materials. We report initial explorations to increase understanding of how parameters can be optimized for generating compliance, and to examine the effect of dynamic parameters on material experiences. Finally, we present a series of use cases that demonstrate the potential of bARefoot for VR and AR.",qa:"discord://discordapp.com/channels/734723053036109904/764103691124080641",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{Preview:"https://www.youtube.com/watch?v=JwvXwGOvtpE","Short Talk":"https://www.youtube.com/watch?v=22R1RkG6wNU"},images:{}},{id:42351,sequence:13,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416191",Document:"http://uist.acm.org/uist2020/data/posters/1069.pdf"},authors:["Priyan Vaithilingam","Julia M. Markel","Philip Guo"],affiliations:["Harvard University","UC San Diego"],title:"Papercode: Generating Paper-Based User Interfaces for Code Review, Annotation, and Teaching",keywords:[],abstract:"Paper can be a powerful and flexible user interface that lets programmers read through large amounts of code. Using off-the-shelf equipment, how can we generate a paper-based UI that supports code review, annotation, and teaching? To address this question, we ran formative studies and developed Papercode, a system that formats source code for printing on standard paper. Users can interact with that code on paper, make freehand annotations, then transfer annotations back to the computer by taking photos with a normal phone camera. Papercode optimizes source code for on-paper readability with tunable heuristics such as code-aware line wraps and page breaks, quick references to function and global definitions, moving comments and short function calls into margins, and topologically sorting functions in dependency order.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104047342256158",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1069.jpg"}},{id:42096,sequence:75,track:"Paper",session:[42216],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415822"},authors:["Jingyuan Liu","Hongbo Fu","Chiew-Lan Tai"],affiliations:["Hong Kong University of Science and Technology","City University of Hong Kong","Hong Kong Univeristy of Science and Technology"],title:"PoseTween: Pose-driven Tween Animation",keywords:[],abstract:"Augmenting human action videos with visual effects often requires professional tools and skills. To make this more accessible by novice users, existing attempts have focused on automatically adding visual effects to faces and hands, or let virtual objects strictly track certain body parts, resulting in rigid-looking effects. We present PoseTween, an interactive system that allows novice users to easily add vivid virtual objects with their movement interacting with a moving subject in an input video. Our key idea is to leverage the motion of the subject to create pose-driven tween animations of virtual objects. With our tool, a user only needs to edit the properties of a virtual object with respect to the subject’s movement at keyframes, and the object is associated with certain body parts automatically. The properties of the object at intermediate frames are then determined by both the body movement and the interpolated object keyframe properties, producing natural object movements and interactions with the subject. We design a user interface to facilitate editing of keyframes and previewing animation results. Our user study shows that PoseTween significantly requires less editing time and fewer keyframes than using the traditional tween animation in making pose-driven tween animations for novice users.",qa:"discord://discordapp.com/channels/734723053036109904/764103694244642856",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=qrac2O2y7A4",Preview:"https://www.youtube.com/watch?v=hKPgOZgFGZQ","Short Talk":"https://www.youtube.com/watch?v=XfU0xU6n1WI"},images:{}},{id:42352,sequence:14,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416193",Document:"http://uist.acm.org/uist2020/data/posters/1082.pdf"},authors:["Andrew Kurauchi","Wenxin Feng","Ajjen Joshi","Carlos H Morimoto","Margrit Betke"],affiliations:["Insper Institute of Education and Research","Boston University","Affectiva","University of São Paulo"],title:"Swipe&Switch: Text Entry Using Gaze Paths and Context Switching",keywords:[],abstract:"Swipe-based methods for text entry by gaze allow users to swipe through the letters of a word by gaze, analogous to how they can swipe with a finger on a touchscreen keyboard. Two challenges for these methods are: (1) gaze paths do not possess clear start and end positions, and (2) it is difficult to design text editing features. We introduce Swipe&Switch, a text-entry interface that uses swiping and switching to improve gaze-based interaction. The interface contains three context regions, and detects the start/end of a gesture and emits text editing commands (e.g., word insertion, deletion) when a user switches focus between these regions. A user study showed that Swipe&Switch provides a better user experience and higher text entry rate over a baseline, EyeSwipe.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104050412093451",videos:{"Short Talk":"https://www.youtube.com/watch?v=FQIPfwKOnoY"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1082.jpg"}},{id:42097,sequence:76,track:"Paper",session:[42230],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415847"},authors:["Chiu-Hsuan Wang","Hsin-Yu Chen","Yuan-Syun Ye","Seraphina Yong","Liwei Chan"],affiliations:["Nation Chiao Tung University","Institute of Multimedia Engineering","National Chiao Tung University","National Taiwan University","Computer Science"],title:"HMD Light: Sharing In-VR Experience via Head-Mounted Projector for Asymmetric Interaction",keywords:[],abstract:"We present HMD Light, a proof-of-concept Head-Mounted Display (HMD) implementation that reveals the Virtual Reality (VR) user’s experience in the physical environment to facilitate communication between VR and external users in a mobile VR context. While previous work externalized the VR user’s experience through an on-HMD display, HMD Light places the display into the physical environment to enable larger display and interaction area. This work explores the interaction design space of HMD Light and presents four applications to demonstrate its versatility. Our exploratory user study observed participant pairs experience applications with HMD Light and evaluated usability, accessibility and social presence between users. From the results, we distill design insights for HMD Light and asymmetric VR collaboration. ",qa:"discord://discordapp.com/channels/734723053036109904/764103687504396288",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=bJ4tLogNWDI","Short Talk":"https://www.youtube.com/watch?v=B4KW92QKqXQ"},images:{}},{id:42353,sequence:15,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416188",Document:"http://uist.acm.org/uist2020/data/posters/1060.pdf"},authors:["Martin Feick","Niko Kleer","Anthony Tang","Antonio Krüger"],affiliations:["Saarland Informatics Campus","University of Toronto","DFKI, Saarland Informatics Campus"],title:"The Virtual Reality Questionnaire Toolkit",keywords:[],abstract:"In this work, we present the VRQuestionnaireToolkit, which enables the research community to easily collect subjective measures within virtual reality (VR). We contribute a highly customizable and reusable open-source toolkit which can be integrated in existing VR projects rapidly. The toolkit comes with a pre-installed set of standard questionnaires such as NASA TLX, SSQ and SUS Presence questionnaire. Our system aims to lower the entry barrier to use questionnaires in VR and to significantly reduce development time and cost needed to run pre-, in between- and post-study questionnaires.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104053902540820",videos:{Preview:"https://www.youtube.com/watch?v=ctzbobrgcdE","Short Talk":"https://www.youtube.com/watch?v=h_vZ6y4ZG9E"},images:{}},{id:42098,sequence:77,track:"Paper",session:[42202],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415591"},authors:["Christopher Clarke","Doga Cavdir","Patrick Chiu","Laurent Denoue","Don Kimber"],affiliations:["Lancaster University","FXPAL","Stanford Universtiy","FX Palo Alto Laboratory"],title:"Reactive Video: Adaptive Video Playback Based on User Motion for Supporting Physical Activity",keywords:[],abstract:"Videos are a convenient platform to begin, maintain, or improve a ftness program or physical activity. Traditional video systems allow users to manipulate videos through specifc user interface actions such as button clicks or mouse drags, but have no model of what the user is doing and are unable to adapt in useful ways. We present adaptive video playback, which seamlessly synchronises video playback with the user’s movements, building upon the principle of direct manipulation video navigation. We implement adaptive video playback in Reactive Video, a vision-based system which supports users learning or practising a physical skill. The use of pre-existing videos removes the need to create bespoke content or specially authored videos, and the system can provide real-time guidance and feedback to better support users when learning new movements. Adaptive video playback using a discrete Bayes and particle flter are evaluated on a data set collected of participants performing tai chi and radio exercises. Results show that both approaches can accurately adapt to the user’s movements, however reversing playback can be problematic. ",qa:"discord://discordapp.com/channels/734723053036109904/764103681238237204",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{"Long Talk":"https://www.youtube.com/watch?v=pF4z3_th3fA",Preview:"https://www.youtube.com/watch?v=i3XdLcR1e9c","Short Talk":"https://www.youtube.com/watch?v=1Ux1XegnodM"},images:{}},{id:42354,sequence:16,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416194",Document:"http://uist.acm.org/uist2020/data/posters/1083.pdf"},authors:["Amber Aftab","Ruipu Hu","Sang Won Lee"],affiliations:["Virginia Polytechnic Institute and State University"],title:"Remo: Generating Interactive Tutorials by Demonstration for Online Tasks",keywords:[],abstract:"People with limited digital literacy struggle to keep up with our increasing dependence on websites for everyday tasks like paying bills or booking flight tickets online. They often get in-person assistance from their friends and family but such help may not always be possible. Remote assistance from peers such as phone calls, or written instructions via email or text messages can be useful. However, remote methods of assistance may lead to communication issues between the helper and the help-seeker, due to a lack of shared visual context; a helper cannot see the help-seeker’s screen. Moreover, help-seekers are often unacquainted with the terminology associated with web navigation. In order to bridge the gap between in-person support and remote help, we develop Remo, a web browser extension, which will allow helpers to create interactive tutorials by demonstration. These tutorials will be embedded within a web page and will make use of visual cues to direct users to specific parts of the page. Remo aims to provide opportunities for people with limited digital literacy to complete online tasks by following the step-by-step and task specific tutorials generated by their peers. Using Remo, we anticipate that the target population will be able to get personalized assistance, similar to in-person support, and eventually learn how to complete broader online tasks independently.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104056997937192",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1083.jpg"}},{id:42099,sequence:78,track:"Paper",session:[42217],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415878"},authors:["Devamardeep Hayatpur","Haijun Xia","Daniel Wigdor"],affiliations:["University of Toronto","University of California, San Diego"],title:"DataHop: Spatial Data Exploration in Virtual Reality",keywords:[],abstract:"Virtual reality has recently been adopted for use within the domain of visual analytics because it can provide users with an endless workspace within which they can be actively engaged and use their spatial reasoning skills for data analysis. However, virtual worlds need to utilize layouts and organizational schemes that are meaningful to the user and beneficial for data analysis. This paper presents DataHop, a novel visualization system that enables users to lay out their data analysis steps in a virtual environment. With a Filter, a user can specify the modification they wish to perform on one or more input data panels (i.e., containers of points), along with where output data panels should be placed in the virtual environment. Using this simple tool, highly intricate and useful visualizations may be generated and traversed by harnessing a user’s spatial abilities. An exploratory study conducted with six virtual reality users evaluated the usability, affordances, and performance of DataHop for data analysis tasks, and found that spatially mapping one’s workflow can be beneficial when exploring multidimensional datasets. ",qa:"discord://discordapp.com/channels/734723053036109904/764103695754592256",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{Preview:"https://www.youtube.com/watch?v=k8b2VjXM-jQ","Short Talk":"https://www.youtube.com/watch?v=AdA9IIb9Q_Y"},images:{}},{id:42355,sequence:17,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416172",Document:"http://uist.acm.org/uist2020/data/posters/1017.pdf"},authors:["Ayaka Ishii","Kaori Ikematsu","Itiro Siio"],affiliations:["Ochanomizu University","Yahoo Japan Corporation"],title:"Electrolysis Ion Display on Wet Surfaces",keywords:[],abstract:"We present a novel method to render color patterns using electrolysis applied onto open wet surfaces. By implementing electrodes within a wet object and electrifying them, electrolysis can occur and generate ions. Using color indicators reacting to such ions, we can create a color-forming display. By applying common techniques, such as a printed circuit board, arbitrary patterns can be displayed by computer control. By reversing the polarity of electrodes and varying the number of ions, it is possible to fade the existing pattern and display a contrasting color. The proposed method can be used to create a color-forming display using various objects more common in everyday life than those used in conventional substantial displays. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764104061095510046",videos:{Preview:"https://www.youtube.com/watch?v=jdXxW88KEZg"},images:{}},{id:42100,sequence:79,track:"Paper",session:[42202],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415892"},authors:["Ryo Suzuki","Rubaiat Habib Kazi","Li-yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],affiliations:["University of Colorado Boulder","Adobe Research","Adobe","University of Colorado, Boulder"],title:"RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching",keywords:[],abstract:"We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.",qa:"discord://discordapp.com/channels/734723053036109904/764103681238237204",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=S5ouToPJyeM","Short Talk":"https://www.youtube.com/watch?v=o4WCgwbNYwo"},images:{}},{id:42356,sequence:18,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416183",Document:"http://uist.acm.org/uist2020/data/posters/1038.pdf"},authors:["Ryusei Uramune","Hiroki Ishizuka","Takefumi Hiraki","Yoshihiro Kawahara","Sei Ikeda","Osamu Oshiro"],affiliations:["Osaka Univeristy","Osaka University","The University of Tokyo"],title:"HaPouch: Soft and Wearable Haptic Display Devices using Liquid-to-gas Phase Change Actuator",keywords:[],abstract:"Conventional pneumatic haptic displays need a complex and weight system because air is fed through a tube from an air compressor. To address this problem, we propose a haptic display that uses a liquid-to-gas phase change actuator and Peltier device. The actuator is composed of a thin plastic pouch containing a low boiling point liquid. The temperature of the liquid is controlled by a Peltier device in close contact with an actuator. This approach allows soft actuators to be inflated without the use of a compressor and tubes, allowing us to realize a small and light haptic display. We implemented the haptic display that has a pouch of 10 mm square and evaluated the characteristics of it. We confirmed that the maximum output force reached almost 1.5 N, which is a similar level to off-the-shelf wearable haptic display.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104063888392212",videos:{"Short Talk":"https://www.youtube.com/watch?v=lO_klxYEKPo"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1038.jpg"}},{id:42101,sequence:80,track:"Paper",session:[42214],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415832"},authors:["Jess McIntosh","Hubert Zajac","Andreea Stefan","Joanna Bergström","Kasper Hornbæk"],affiliations:["University of Copenhagen"],title:"Iteratively Adapting Avatars using Task-Integrated Optimisation",keywords:[],abstract:"Virtual Reality allows users to embody avatars that do not match their real bodies. Earlier work has selected changes to the avatar arbitrarily and it therefore remains unclear how to change avatars to improve users’ performance. We propose a systematic approach for iteratively adapting the avatar to perform better for a given task based on users’ performance. The approach is evaluated in a target selection task, where the forearms of the avatar are scaled to improve performance. A comparison between the optimised and real arm lengths shows a significant reduction in average tapping time by 18.7%, for forearms multiplied in length by 5.6. Additionally, with the adapted avatar, participants moved their real body and arms significantly less, and subjective measures show reduced physical demand and frustration. In a second study, we modify finger lengths for a linear tapping task to achieve a better performing avatar, which demonstrates the generalisability of the approach.",qa:"discord://discordapp.com/channels/734723053036109904/764103691979849729",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{Preview:"https://www.youtube.com/watch?v=gw1ZYQS04KE","Short Talk":"https://www.youtube.com/watch?v=5cy2TFyExDE"},images:{}},{id:42357,sequence:19,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416171",Document:"http://uist.acm.org/uist2020/data/posters/1016.pdf"},authors:["Shoki Kishida","Sho Sakurai","Koichi Hirota","Takuya Nojima"],affiliations:["The University of Electro-Communications","University of Electro-Communications"],title:"Ambre: Augmented Metaphor Based Representation System for Electricity",keywords:[],abstract:"With the rise of the Maker Movement and STEM education, toolkit research for beginners in electronics has become popular. Most of them are focused on the efficient creation and debugging of electrical circuits. Therefore, users were implicitly required to understanding of the nature of electricity and some prerequisites knowledge. In this paper, we propose tangible interface to express electrical properties such as voltage and resistance. This system aims to bring a deep interest and understanding of the nature of electricity to the beginners explicitly. In this research, we discuss the interaction between the laws of electricity and another several physical laws, present an example of a prototype implementation, and describe the results of a preliminary user questionnaire.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104066757427210",videos:{Preview:"https://www.youtube.com/watch?v=huijjgx44Ik","Short Talk":"https://www.youtube.com/watch?v=NPuQsvB4Bhk"},images:{}},{id:42102,sequence:81,track:"Paper",session:[42215],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415882"},authors:["Haijun Xia","Jennifer Jacobs","Maneesh Agrawala"],affiliations:["University of California, San Diego","University of California Santa Barbara","Stanford University"],title:"Crosscast: Adding Visuals to Audio Travel Podcasts",keywords:[],abstract:"Audio travel podcasts are a valuable source of information for travelers. Yet, travel is, in many ways, a visual experience and the lack of visuals in travel podcasts can make it difficult for listeners to fully understand the places being discussed. We present Crosscast: a system for automatically adding visuals to audio travel podcasts. Given an audio travel podcast as input, Crosscast uses natural language processing and text mining to identify geographic locations and descriptive keywords within the podcast transcript. Crosscast then uses these locations and keywords to automatically select relevant photos from online repositories and synchronizes their display to align with the audio narration. In a user evaluation, we find that 85.7% of the participants preferred Crosscast generated audio-visual travel podcasts compared to audio-only travel podcasts.",qa:"discord://discordapp.com/channels/734723053036109904/764103692781355018",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42358,sequence:20,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416192",Document:"http://uist.acm.org/uist2020/data/posters/1074.pdf"},authors:["Cristian Sorescu","Yogesh Kumar Meena","Deepak Ranjan Sahoo"],affiliations:["Swansea University"],title:"PViMat: A Self-Powered Portable and Rollable Large Area Gestural Interface Using Indoor Light",keywords:[],abstract:"We present PViMat, a portable photovoltaic mat that can recognise six unique useful dynamic hover gestures up to 30 cmabove the surface over a large area with more than 97% accuracy. We utilised an off-the-shelf portable and rollable outdoor solar tape and employed it to harvest indoor natural andartificial light energy to trickle charge a LiPo battery for self-powering. We demonstrate a low-power operation techniqueof continuous charging with DC photocurrent and simultane-ous event-driven gesture recognition with AC photocurrent.The PViMat prototype harvests 30 mW in a general livingroom light level of 300 lux and consumes 0.8 mW per gesture. Lastly, we propose applications of PViMat with its large-area,flexible and rollable form-factors and the gesture set.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104071359365141",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1074.jpg"}},{id:42103,sequence:82,track:"Paper",session:[42222],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415819"},authors:["Masahiro Yamaguchi","Shohei Mori","Peter Mohr","Markus Tatzgern","Ana Stanescu","Hideo Saito","Denis Kalkofen"],affiliations:["Keio University","Graz University of Technology","Salzburg University of Applied Sciences"],title:"Video-Annotated Augmented Reality Assembly Tutorials",keywords:[],abstract:"We present a system for generating and visualizing interactive 3D Augmented Reality tutorials based on 2D video input, which allows viewpoint control at runtime. Inspired by assembly planning, we analyze the input video using a 3D CAD model of the object to determine an assembly graph that encodes blocking relationships between parts. Using an assembly graph enables us to detect assembly steps that are otherwise difficult to extract from the video, and generally improves object detection and tracking by providing prior knowledge about movable parts. To avoid information loss, we combine the 3D animation with relevant parts of the 2D video so that we can show detailed manipulations and tool usage that cannot be easily extracted from the video. To further support user orientation, we visually align the 3D animation with the realworld object by using texture information from the input video. We developed a presentation system that uses commonly available hardware to make our results accessible for home use and demonstrate the effectiveness of our approach by comparing it to traditional video tutorials.",qa:"discord://discordapp.com/channels/734723053036109904/764103699140050955",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=Tid4qacqrQU","Short Talk":"https://www.youtube.com/watch?v=m8uUDR8DERY"},images:{}},{id:42359,sequence:21,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416178",Document:"http://uist.acm.org/uist2020/data/posters/1033.pdf"},authors:["Ashrith Shetty","Ebrima Jarjue","Huaishu Peng"],affiliations:["University of Maryland, College Park","University of Maryland"],title:"Tangible Web Layout Design for Blind and Visually Impaired People: An Initial Investigation",keywords:[],abstract:"In this poster, we explore the potential of using a tangible user interface (TUI) to enable blind and visually impaired (BVI) developers to design web layouts without assistance. We conducted a semi-structured interview and a co-design session with a blind participant to elicit insights that guided the design of a TUI prototype named Sparsha. Our initial prototype contains 3D printed tactile beads that represent HTML elements, and a 3D printed base that represents the web page layout. BVI users can add an HTML element to the layout by placing tactile beads on top of the base. The base senses the type and location of the beads and renders the HTML element in the corresponding location on the client browser. The poster concludes with a discussion and enumerates future work.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104074446241822",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1033.jpg"}},{id:42104,sequence:83,track:"Paper",session:[42197],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415830"},authors:["Julien Gori","Han L. Han","Michel Beaudouin-Lafon"],affiliations:["Laboratoire de Recherche en Informatique, Université Paris-Saclay, CNRS, Inria","Université Paris-Saclay, CNRS, Inria","Univ. Paris-Sud, CNRS, Inria, Université Paris-Saclay"],title:"FileWeaver: Flexible File Management with Automatic Dependency Tracking",keywords:[],abstract:"Knowledge management and sharing involves a variety of specialized but isolated software tools, tied together by the files that these tools use and produce. We interviewed 23 scientists and found that they all had difficulties using the file system to keep track of, re-find and maintain consistency among related but distributed information. We introduce FileWeaver, a system that automatically detects dependencies among files without explicit user action, tracks their history, and lets users interact directly with the graphs representing these dependencies and version history. Changes to a file can trigger recipes, either automatically or under user control, to keep the file consistent with its dependants. Users can merge variants of a file, e.g. different output formats, into a polymorphic file, or morph, and automate the management of these variants. By making dependencies among files explicit and visible, FileWeaver facilitates the automation of workflows by scientists and other users who rely on the file system to manage their data.",qa:"discord://discordapp.com/channels/734723053036109904/764103679011192884",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=zN2csHYgo5U",Preview:"https://www.youtube.com/watch?v=Q1q9cfytfsI","Short Talk":"https://www.youtube.com/watch?v=lxjCQrLMG5A"},images:{}},{id:42360,sequence:22,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416187",Document:"http://uist.acm.org/uist2020/data/posters/1054.pdf"},authors:["Minori Narita","Nolwenn Maudet","Yi Lu","Takeo Igarashi"],affiliations:["University of Massachusetts, Amherst","Université de Strasbourg","The University of Tokyo"],title:"FlashAttention: Data-centric Interaction for Data Transformation Using Programming-by-Example",keywords:[],abstract:"Programming-by-example (PBE) can be a powerful tool for reducing manual work in repetitive data transformation tasks. However, having few examples often leaves ambiguity and may cause undesirable data transformation by the system. This ambiguity can be resolved by allowing the user to directly edit the synthesized programs, but this is difficult for non-programmers. Here, we present a novel approach: data-centric interaction for data transformation, where users resolve the ambiguity in data transformation by examining and modifying the output rather than the program. The key idea is focusing on the given set of data the user wants to transform instead of pursuing the synthesized program's completeness. Our system provides interactive visualization that allows users to efficiently examine and fix the transformed outputs, which is much simpler than understanding and modifying the program itself. Our experiment shows that the number of candidates is much smaller than the number of synthesized programs, which implies the effectiveness of the proposed method.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104077373997066",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1054.jpg"}},{id:42105,sequence:84,track:"Paper",session:[42207],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415865"},authors:["Joseph Chee Chang","Nathan Hahn","Aniket Kittur"],affiliations:["HCII and LTI","Carnegie Mellon University"],title:"Mesh: Scaffolding Comparison Tables for Online Decision Making",keywords:[],abstract:"While there is an enormous amount of information online for making decisions such as choosing a product, restaurant, or school, it can be costly for users to synthesize that information into confident decisions. Information for users’ many different criteria needs to be gathered from many different sources into a structure where they can be compared and contrasted. The usefulness of each criterion for differentiating potential options can be opaque to users, and evidence such as reviews may be subjective and conflicting, requiring users to interpret each under their personal context. We introduce Mesh, which scaffolds users in iteratively building up a better understanding of both their criteria and options by evaluating evidence gathered across sources in the context of consumer decision making. Mesh bridges the gap between decision support systems that typically have rigid structures and the fluid and dynamic process of exploratory search, changing the cost structure to provide increasing payoffs with greater user investment. Our lab and field deployment studies found evidence that Mesh significantly reduces the costs of gathering and evaluating evidence and scaffolds decision-making through personalized criteria enabling users to gain deeper insights from data. ",qa:"discord://discordapp.com/channels/734723053036109904/764103684367056897",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=90dpujMsAQg"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42361,sequence:23,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416180",Document:"http://uist.acm.org/uist2020/data/posters/1035.pdf"},authors:["Songlin Xu","Zhiyuan Wu","ShunHong Wang","Rui Fan","Nan Lin"],affiliations:["University of Science and Technology of China"],title:"Hydrauio: Extending Interaction Space on the Pen through Hydraulic Sensing and Haptic Output",keywords:[],abstract:"We have explored a fluid-based interface(Hydrauio) on the pen body to extend interaction space of human-pen interaction. Users could perform finger gestures on the pen for input and also receive haptic feedback of different profiles from the fluid surface. The user studies showed that Hydrauio could achieve an accuracy of more than 92% for finger gesture recognition and users could distinguish different haptic profiles with an accuracy of more than 95%. Finally, we present application scenarios to demonstrate the potential of Hydrauio to extend interaction space of human-pen interaction.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104080401367050",videos:{Preview:"https://www.youtube.com/watch?v=9WX8qxf5KC0","Short Talk":"https://www.youtube.com/watch?v=r0emDn6AUyQ"},images:{}},{id:42106,sequence:85,track:"Paper",session:[42217],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415868"},authors:["Chiu-Hsuan Wang","Chia-En Tsai","Seraphina Yong","Liwei Chan"],affiliations:["Nation Chiao Tung University","Institute of Multimedia Engineering","National Taiwan University","Computer Science"],title:"Slice of Light: Transparent and Integrative Transition Among Realities in a Multi-HMD-User Environment",keywords:[],abstract:"This work presents Slice of Light, a visualization design created to enhance transparency and integrative transition between realities of Head-Mounted Display (HMD) users sharing the same physical environment. Targeted at reality-guests, Slice of Light’s design enables guests to view other HMD users’ interactions contextualized in their own virtual environments while allowing the guests to navigate among these virtual environments. In this paper, we detail our visualization design and the implementation. We demonstrate Slice of Light with a block-world construction scenario that involves a multi-HMD-user environment. VR developer and HCI expert participants were recruited to evaluate the scenario, and responded positively to Slice of Light. We discuss their feedback, our design insights, and the limitations of this work.",qa:"discord://discordapp.com/channels/734723053036109904/764103695754592256",broadcast:"https://acm-org.zoom.us/j/93954539577?pwd=RHB6U2paUkRLaExzak8xcUxaZXZDQT09",videos:{Preview:"https://www.youtube.com/watch?v=di4M6IAhgl4","Short Talk":"https://www.youtube.com/watch?v=z_UUTvqgCsc"},images:{}},{id:42362,sequence:24,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416179",Document:"http://uist.acm.org/uist2020/data/posters/1034.pdf"},authors:["Patrick Coe","Grigori Evreinov","Hasse Sinivaara","Arto Hippula","Roope Raisamo"],affiliations:["Tampere University","TactoTek Group"],title:"Actuation Plate for Multi-Layered in-Vehicle Control Panel",keywords:[],abstract:"High-fidelity localized feedback has the potential of providing new and unique levels of interaction with a given device. Achieving this in a cost-effective reproducible manner has been a challenge in modern technology. Past experiments have shown that by using the principles of constructive wave interference introduced by time offsets it is possible to achieve a position of increased vibration displacement at any given location. As new interface form factors increasingly incorporate curved surfaces, we now show that these same techniques can successfully be applied and mechanically coupled with a universal actuation plate.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104083430572072",videos:{Preview:"https://www.youtube.com/watch?v=3rAcgdKPfM8"},images:{}},{id:42107,sequence:86,track:"Paper",session:[42215],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415845"},authors:["Haijun Xia"],affiliations:["University of California, San Diego"],title:"Crosspower: Bridging Graphics and Linguistics",keywords:[],abstract:"Despite the ubiquity of direct manipulation techniques available in computer-aided design applications, creating digital content remains a tedious and indirect task. This is because applications require users to perform numerous low-level editing operations rather than allowing them to directly indicate high-level design goals. Yet, the creation of graphic content, such as videos, animations, and presentations often begins with a description of design goals in natural language, such as screenplays, scripts, outlines. Therefore, there is an opportunity for language-oriented authoring, i.e., leveraging the information found in the structure of a language to facilitate the creation of graphic content. We present a systematic exploration of the identification, graphic description, and interaction with various linguistic structures to assist in the creation of visual content. The prototype system, Crosspower, and its proposed interaction techniques, enables content creators to indicate and customize their desired visual content in a flexible and direct manner.",qa:"discord://discordapp.com/channels/734723053036109904/764103692781355018",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42363,sequence:25,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3418578",Document:"http://uist.acm.org/uist2020/data/posters/1012.pdf"},authors:["Hee-Seung Moon","Jiwon Seo"],affiliations:["Yonsei University"],title:"Dynamic Difficulty Adjustment via Fast User Adaptation",keywords:[],abstract:"Dynamic difficulty adjustment (DDA) is a technology that adapts a game's challenge to match the player's skill. It is a key element in game development that provides continuous motivation and immersion to the player. However, conventional DDA methods require tuning in-game parameters to generate the levels for various players. Recent DDA approaches based on deep learning can shorten the time-consuming tuning process, but require sufficient user demo data for adaptation. In this paper, we present a fast user adaptation method that can adjust the difficulty of the game for various players using only a small amount of demo data by applying a meta-learning algorithm. In the video game environment user test (n=9), our proposed DDA method outperformed a typical deep learning-based baseline method.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104089188696074",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1012.jpg"}},{id:42108,sequence:87,track:"Paper",session:[42201],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415890"},authors:["Geoffrey X. Yu","Tovi Grossman","Gennady Pekhimenko"],affiliations:["University of Toronto","Vector Institute"],title:"Skyline: Interactive In-Editor Computational Performance Profiling for Deep Neural Network Training",keywords:[],abstract:"Training a state-of-the-art deep neural network (DNNs) is a computationally-expensive and time-consuming process, which incentivizes deep learning developers to debug their DNNs for computational performance. However, effectively performing this debugging requires intimate knowledge about the underlying software and hardware systems—something that the typical deep learning developer may not have. To help bridge this gap, we present SKYLINE: a new interactive tool for DNN training that supports in-editor computational performance profiling, visualization, and debugging. SKYLINE’s key contribution is that it leverages special computational properties of DNN training to provide (i) interactive performance predictions and visualizations, and (ii) directly manipulatable visualizations that, when dragged, mutate the batch size in the code. As an in-editor tool, SKYLINE allows users to leverage these diagnostic features to debug the performance of their DNNs during development. An exploratory qualitative user study of SKYLINE produced promising results; all the participants found SKYLINE to be useful and easy to use.",qa:"discord://discordapp.com/channels/734723053036109904/764103679694995526",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=XujtsfR37z0",Preview:"https://www.youtube.com/watch?v=4GReNYxxIAo","Short Talk":"https://www.youtube.com/watch?v=roixf-i5_PU"},images:{}},{id:42364,sequence:26,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416182",Document:"http://uist.acm.org/uist2020/data/posters/1037.pdf"},authors:["Ahmed Farooq","Hong Z Tan","Roope Raisamo"],affiliations:["Tampere University","Purdue University"],title:"Enhancing Vibrotactile Signal Propagation using Sub-Surface 3D-Printed Waveguides",keywords:[],abstract:"The mediation of haptic feedback largely depends on physical properties of the surface of interaction and its internal structure. Most mobile devices today are assembled using a wide range of components with varied physical properties that can limit the propagation of tactile signals. For that reason, it is important to understand how common materials used in assembling mobile devices relay vibration signals and develop techniques of effectively mediating haptic feedback uniformly throughout the entire device. This research compares three off-the-shelf waveguide materials and one custom-designed 3D-printed ABS structure for creating feedback signals on flat interaction surfaces. Preliminary results indicate that by altering the internal structure of the interaction surface we can reduce attenuation and integration of the applied signal and improving the overall haptic experience. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764104092414509066",videos:{Preview:"https://www.youtube.com/watch?v=SB6VdgBwFNc","Short Talk":"https://www.youtube.com/watch?v=_ut2-5mkiS4"},images:{}},{id:42109,sequence:88,track:"Paper",session:[42228],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415863"},authors:["Haruki Takahashi","Parinya Punpongsanon","Jeeeun Kim"],affiliations:["Meiji University","Osaka University","Texas A&M University"],title:"Programmable Filament: Printed Filaments for Multi-material 3D Printing",keywords:[],abstract:"From full-color objects to functional capacitive artifacts, 3D printing multi-materials became essential to broaden the application areas of digital fabrication. We present Programmable Filament, a novel technique that enables multi-material printing using a commodity FDM 3D printer, requiring no hardware upgrades. Our technique builds upon an existing printing technique in which multiple filament segments are printed and spliced into a single threaded filament. We propose an end-toend pipeline for 3D printing an object in multi-materials, with an introduction of the design systems for end-users. Optimized for low-cost, single-nozzle FDM 3D printers, the system is built upon our computational analysis and experiments to enhance its validity over various printers and materials to design and produce a programmable filament. Finally, we discuss application examples and speculate the future with its potential, such as custom filament manufacturing on-demand.",qa:"discord://discordapp.com/channels/734723053036109904/764103700997865472",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=ZnCh6JK0RZs","Short Talk":"https://www.youtube.com/watch?v=XkZ5qyRDW4Y"},images:{}},{id:42365,sequence:27,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416181",Document:"http://uist.acm.org/uist2020/data/posters/1036.pdf"},authors:["Donghan Hu","Sang Won Lee"],affiliations:["Virginia Tech","Virginia Polytechnic Institute and State University"],title:"Scrapbook: Screenshot-based Bookmark for EffectiveCuration of Digital Resources",keywords:[],abstract:"Modern users typically open multiple software, websites, and documents for daily tasks.\r\nRetrieving previously used digital resources, such as web pages, documents, local files, and software, for knowledge workers is inevitable, but can be time-consuming. People may fail to remember where the resource is. In addition, sometimes, users need to retrieve digital resources across multiple applications to resume a computing task.\r\nHowever, the current methods - bookmarks, file systems, and recent history - for curating such resources are limited in their capacity and dispersed over multiple applications.  \r\nTo address the above problems, we test the idea of curating digital resources by developing Scrapbook, a software that allows users to curate digital resources with screenshots. Scrapbook allows users to take a screenshot of their computer screen and stores metadata of captured applications in the screenshot. Later, users can utilize multimodal (visual and textual) information to retrieve information that they want to recall or to restore the working context of a certain task. In this poster, we acknowledge previous relevant research, introduce ongoing efforts of developing Scrapbook, and share our plan to validate the idea via a user study.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104095534415872",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1036.jpg"}},{id:42110,sequence:89,track:"Paper",session:[42222],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415887"},authors:["Yoonji Kim","Hyein Lee","Ramkrishna Prasad","Seungwoo Je","Youngkyung Choi","Daniel Ashbrook","Ian Oakley","Andrea Bianchi"],affiliations:["KAIST","University of Copenhagen","UNIST"],title:"SchemaBoard: Supporting Correct Assembly of Schematic Circuits using Dynamic In-Situ Visualization",keywords:[],abstract:"Assembling circuits on breadboards using reference designs is a common activity among makers. While tools like Fritzing offer a simplified visualization of how components and wires are connected, such pictorial depictions of circuits are rare in formal educational materials and the vast bulk of online technical documentation. Electronic schematics are more common but are perceived as challenging and confusing by novice makers. To improve access to schematics, we propose SchemaBoard, a system for assisting makers in assembling and inspecting circuits on breadboards from schematic source materials. SchemaBoard uses an LED matrix integrated underneath a working breadboard to visualize via light patterns where and how components should be placed, or to highlight elements of circuit topology such as electrical nets and connected pins. This paper presents a formative study with 16 makers, the SchemaBoard system, and a summative evaluation with an additional 16 users. Results indicate that SchemaBoard is effective in reducing both the time and the number of errors associated with building a circuit based on a reference schematic, and for inspecting the circuit for correctness after its assembly. ",qa:"discord://discordapp.com/channels/734723053036109904/764103699140050955",broadcast:"https://acm-org.zoom.us/j/91803844044?pwd=b0dsQk4ycE5pNXBhSUNmY1dwTHY0dz09",videos:{Preview:"https://www.youtube.com/watch?v=vpDQu0r1I70","Short Talk":"https://www.youtube.com/watch?v=1mRkc4oC-mw"},images:{}},{id:42366,sequence:28,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416198",Document:"http://uist.acm.org/uist2020/data/posters/1091.pdf"},authors:["Lingyun Sun","Ziqian Shao","Danli Luo","Jianzhe Gu","Ye Tao","Lining Yao","Guanyun Wang"],affiliations:["Zhejiang University","Carnegie Mellon University"],title:"FabricFit: Transforming Form-Fitting Fabrics",keywords:[],abstract:"Textiles encompass a wide variety and rich characteristics that ranges from soft to tough, foldable to rigid, cuttable to stiff, elastic to plastic. On the other hand, 3D printing thermoplastic (e.g., PLA) materials exhibit controlled deformation at a certain transition temperature. In this paper, we present a method of fabricating form-fitting composite textiles by printing PLA on different fabric substrates, designing different structures and transforming the fabric-PLA composite to form different textures.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104098563096598",videos:{Preview:"https://www.youtube.com/watch?v=MmkQtO_uKsM","Short Talk":"https://www.youtube.com/watch?v=u0XkaB8zTKQ"},images:{}},{id:42111,sequence:90,track:"Paper",session:[42198],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415853"},authors:["Yuta Noma","Koya Narumi","Fuminori Okuya","Yoshihiro Kawahara"],affiliations:["The University of Tokyo"],title:"Pop-up Print: Rapidly 3D Printing Mechanically Reversible Objects in the Folded State",keywords:[],abstract:"Despite recent advancements in 3D printing technology, which allows users to rapidly produce 3D objects, printing tall and/or large objects still consumes more time and large amount of support material. In order to address these problems, we propose Pop-up Print, a method to 3D print an object in a compact “folded” state and then unfold it after printing to achieve the final artifact. Using this method, we can reduce the object’s print height and volume, which directly affects the printing time and support material consumption. In addition, thanks to the reversibility of folding/unfolding, we can reversibly minimize the printed object’s volume when unused for storage or transportation, and expand it only in use. To achieve Pop-up Print, we first conducted an experiment using selected printed sample objects with several parameters, in order to determine suitable crease patterns that make both the unfolded and folded state mechanically stable. Based on this result, we developed an interactive design tool to convert 3D models – such as a Stanford Bunny or a Huffman’s cone – to the folded shape. Our design tool allows users to decide non-intuitive parameters that may affect the form’s mechanical stability, while maintaining both functional crease patterns and the object’s original form factor. Finally, we demonstrate the feasibility of our method through several examples of folded objects. ",qa:"discord://discordapp.com/channels/734723053036109904/764103680441450547",broadcast:"https://acm-org.zoom.us/j/99151407318?pwd=cUlLaE5JMzNadHRjYjE0Y2owNDlPQT09",videos:{Preview:"https://www.youtube.com/watch?v=-KcUMDJ9XXI","Short Talk":"https://www.youtube.com/watch?v=fPVKm3DzzKc"},images:{}},{id:42367,sequence:29,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416186",Document:"http://uist.acm.org/uist2020/data/posters/1051.pdf"},authors:["Florin-Timotei Ghiurãu","Mehmet Aydın Baytaş","Casper Wickman"],affiliations:["Volvo Cars","Chalmers University of Technology"],title:"ARCAR: On-Road Driving in Mixed Reality by Volvo Cars",keywords:[],abstract:"ARCAR is a proof-of-concept headset-based mixed reality experience, for use by a driver in full control of a moving vehicle. The functional prototype has been implemented using a high end video pass-through mixed reality headset, and tested on roads (closed to external traffic) at conventional driving speeds. Our implementation embodies acceptable solutions to a number of current challenges that have been in the way of in-car XR applications, and enables a wealth of research. ARCAR is intended to serve as a research platform in the near future, enabling investigations on topics which include safety, design, perceived quality, and consumer applications. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764104102094438480",videos:{"Short Talk":"https://www.youtube.com/watch?v=vYjk8VXdwyU"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1051.jpg"}},{id:42112,sequence:91,track:"Paper",session:[42218],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415871"},authors:["Yu-Jung Ko","HANG ZHAO","Yoonsang Kim","IV Ramakrishnan","Shumin Zhai","Xiaojun Bi"],affiliations:["Stony Brook University","Google"],title:"Modeling Two Dimensional Touch Pointing",keywords:[],abstract:"Modeling touch pointing is essential to touchscreen interface development and research, as pointing is one of the most basic and common touch actions users perform on touchscreen devices. Finger-Fitts Law [4] revised the conventional Fitts’ law into a 1D (one-dimensional) pointing model for finger touch by explicitly accounting for the fat finger ambiguity (absolute error) problem which was unaccounted for in the original Fitts’ law. We generalize Finger-Fitts law to 2D touch pointing by solving two critical problems. First, we extend two of the most successful 2D Fitts law forms to accommodate finger ambiguity. Second, we discovered that using nominal target width and height is a conceptually simple yet effective approach for defining amplitude and directional constraints for 2D touch pointing across different movement directions. The evaluation shows our derived 2D Finger-Fitts law models can be both principled and powerful. Specifically, they outperformed the existing 2D Fitts’ laws, as measured by the regression coefficient and model selection information criteria (e.g., Akaike Information Criterion) considering the number of parameters. Finally, 2D Finger-Fitts laws also advance our understanding of touch pointing and thereby serve as the basis for touch interface designs.",qa:"discord://discordapp.com/channels/734723053036109904/764103693175357461",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=K84bW0GYgBo","Short Talk":"https://www.youtube.com/watch?v=ahbihNNsoGE"},images:{}},{id:42368,sequence:30,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416177",Document:"http://uist.acm.org/uist2020/data/posters/1028.pdf"},authors:["Miles Priebe","Esther W Foo","Brad Holschuh"],affiliations:["University of Minnesota"],title:"Shape Memory Alloy Haptic Compression Garment for Media Augmentation in Virtual Reality Environment",keywords:[],abstract:"Compression-based haptics instrumented into soft, wearable systems have the potential to enhance/augment gaming and entertainment experiences. Here, we present a computer-mediated Shape Memory Alloy (SMA) garment capable of delivering spatially and temporally dynamic compression and thermal stimuli for the purposes of enhancing a movie-watching affective experience based on 4 different emotions: sadness, love, gratitude, fear. Preliminary findings, still pending publication, were obtained from an MTurk survey (n=53) that seek to gather how different emotions can/should be represented physically through a haptic garment. A proposed study in a virtual reality (VR) environment is also introduced as a next step to evaluate how the presence/absence of haptics changes a user’s video-watching experience, including immersion, enjoyment, presence, and felt emotional intensity, evaluated through objective (biometrics) and subjective (self-report questionnaire and qualitative interview) measures. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764104105064661082",videos:{"Short Talk":"https://www.youtube.com/watch?v=m3hQC0tOuCM"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/posters/preview-pics/1028.jpg"}},{id:42113,sequence:92,track:"Paper",session:[42201],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415842"},authors:["Mary Beth Kery","Donghao Ren","Fred Hohman","Dominik Moritz","Kanit Wongsuphasawat","Kayur Patel"],affiliations:["Carnegie Mellon University","Apple Inc.","Georgia Institute of Technology","Apple Inc"],title:"mage: Fluid Moves Between Code and Graphical Work in Computational Notebooks",keywords:[],abstract:"We aim to increase the flexibility at which a data worker can choose the right tool for the job, regardless of whether the tool is a code library or an interactive graphical user interface (GUI). To achieve this flexibility, we extend computational notebooks with a new API mage, which supports tools that can represent themselves as both code and GUI as needed. We discuss the design of mage as well as design opportunities in the space of flexible code/GUI tools for data work. To understand tooling needs, we conduct a study with nine professional practitioners and elicit their feedback on mage and potential areas for flexible code/GUI tooling. We then implement six client tools for mage that illustrate the main themes of our study findings. Finally, we discuss open challenges in providing flexible code/GUI interactions for data workers.",qa:"discord://discordapp.com/channels/734723053036109904/764103679694995526",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{Preview:"https://www.youtube.com/watch?v=uA0ZwCqSgFo","Short Talk":"https://www.youtube.com/watch?v=AlBCScTDYKs"},images:{}},{id:42369,sequence:31,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416179",Document:"http://uist.acm.org/uist2020/data/posters/1006.pdf"},authors:["Audrey Labrie","Jinghui Cheng"],affiliations:["Polytechnique Montréal","Polytechnique Montreal"],title:"Adapting Usability Heuristics to the Context of Mobile Augmented Reality",keywords:[],abstract:"Augmented reality (AR) is an emerging technology in mobile app design during recent years. However, usability challenges in these apps are prominent. There are currently no established guidelines for designing and evaluating interactions in AR as there are in traditional user interfaces. In this work, we aimed to examine the usability of current mobile AR applications and interpreting classic usability heuristics in the context of mobile AR. Particularly, we focused on AR home design apps because of their popularity and ability to incorporate important mobile AR interaction schemas. Our findings indicated that it is important for the designers to consider the unfamiliarity of AR technology to the vast users and to take technological limitations into consideration when designing mobile AR apps. Our work serves as a first step for establishing more general heuristics and guidelines for mobile AR.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104107946410004",videos:{Preview:"https://www.youtube.com/watch?v=3rAcgdKPfM8"},images:{}},{id:42114,sequence:93,track:"Paper",session:[42197],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415874"},authors:["Frederik Brudy","David Ledo","Michel Pahud","Nathalie Henry Riche","Christian Holz","Anand Waghmare","Hemant Surale","Marcus Peinado","Xiaokuan Zhang","Shannon Joyner","Badrish Chandramouli","Umar Farooq Minhas","Jonathan Goldstein","William Buxton","Ken Hinckley"],affiliations:["Microsoft Research","University College London","University of Calgary","ETH Zurich","University of Washington","University of Waterloo","Ohio State University","Cornell University"],title:"SurfaceFleet: Exploring Distributed Interactions Unbounded from Device, Application, User, and Time",keywords:[],abstract:"Knowledge work increasingly spans multiple computing surfaces. Yet in status quo user experiences, content as well as tools, behaviors, and workflows are largely bound to the current device—running the current application, for the current user, and at the current moment in time. SurfaceFleet is a system and toolkit that uses resilient distributed programming techniques to explore cross-device interactions that are unbounded in these four dimensions of device, application, user, and time. As a reference implementation, we describe an interface built using Surface Fleet that employs lightweight, semi-transparent UI elements known as Applets. Applets appear always-on-top of the operating system, application windows, and (conceptually) above the device itself. But all connections and synchronized data are virtualized and made resilient through the cloud. For example, a sharing Applet known as a Portfolio allows a user to drag and drop unbound Interaction Promises into a document. Such promises can then be fulfilled with content asynchronously, at a later time (or multiple times), from another device, and by the same or a different user. ",qa:"discord://discordapp.com/channels/734723053036109904/764103679011192884",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{Preview:"https://www.youtube.com/watch?v=vHyxU4sBzdo","Short Talk":"https://www.youtube.com/watch?v=PUddLWil8do"},images:{}},{id:42370,sequence:32,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416169",Document:"http://uist.acm.org/uist2020/data/posters/1008.pdf"},authors:["Kaori Ikematsu","Kunihiro Kato","Yoshihiro Kawahara"],affiliations:["Yahoo Japan Corporation","Tokyo University of Technology","The university of Tokyo"],title:"LightTouch:  Passive Gadgets for Extending Interactions on Capacitive Touchscreens by Automating Touch Inputs",keywords:[],abstract:"We present LightTouch, a passive gadget to enhance touch interactions on unmodified capacitive touchscreens. \r\nIt simulates finger operations such as tapping, swiping, or multi-touch gestures using conductive materials and photoresistors embedded inside the objects. \r\nThe touchscreen emits visible light and the photoresistor senses the level of this light, which changes its resistance value. \r\nBy controlling the screen brightness, it connects or disconnects the path between the GND and the touchscreen, thus allowing the touch inputs to be controlled. In contrast to conventional physical extensions for touchscreens, our technique does not require continuous finger contact on the conductive part nor the use of batteries. \r\nOur technique opens up new possibilities for touch interaction such as for enhancing the trackability of tangibles beyond the simple automation of touch inputs. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764104111133950033",videos:{Preview:"https://www.youtube.com/watch?v=3leCJ5DHuWE","Short Talk":"https://www.youtube.com/watch?v=7dI5UPXMvhA"},images:{}},{id:42115,sequence:94,track:"Paper",session:[42212],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415900"},authors:["Tianyi Zhang","London Lowmanstone","Xinyu Wang","Elena L. Glassman"],affiliations:["Harvard University","University of Michigan"],title:"Interactive Program Synthesis by Augmented Examples",keywords:[],abstract:"Programming-by-example (PBE) has become an increasingly popular component in software development tools, humanrobot interaction, and end-user programming. A long-standing challenge in PBE is the inherent ambiguity in user-provided examples. This paper presents an interaction model to disambiguate user intent and reduce the cognitive load of understanding and validating synthesized programs. Our model provides two types of augmentations to user-given examples: 1) semantic augmentation where a user can specify how different aspects of an example should be treated by a synthesizer via light-weight annotations, and 2) data augmentation where the synthesizer generates additional examples to help the user understand and validate synthesized programs. We implement and demonstrate this interaction model in the domain of regular expressions, which is a popular mechanism for text processing and data wrangling and is often considered hard to master even for experienced programmers. A within-subjects user study with twelve participants shows that, compared with only inspecting and annotating synthesized programs, interacting with augmented examples significantly increases the success rate of finishing a programming task with less time and increases users’ confidence of synthesized programs.",qa:"discord://discordapp.com/channels/734723053036109904/764103688939372574",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=ShVlU679i9s","Short Talk":"https://www.youtube.com/watch?v=vosQh_Duk-E"},images:{}},{id:42371,sequence:33,track:"Poster",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416156",Document:"http://uist.acm.org/uist2020/data/posters/1007.pdf"},authors:["Noushad Sojib","Syed Masum Billah","Mohammad Amin"],affiliations:["North East University Bangladesh","Pennsylvania State University","Fordham University"],title:"Towards Multi-Wheel Input Device for Non-Visual Interaction",keywords:[],abstract:"While sighted users leverage both keyboard and mouse input devices to interact with desktops, non-visual users, i.e., users who are blind, cannot use a mouse as it only provides feedback through a visual cursor. As a result, these users rely on keyboard-only interaction, which is often cumbersome, inefficient, and error-prone. Prior work has shown that using a small, rotary input device benefits blind users significantly, as it simulates mouse-like operations. In this paper, we extend this prior work by proposing \\emph{Wheeler}, a multi-wheel based input device that provides simultaneous access to UI elements at three different hierarchies, to facilitate rapid navigation and mouse-like interaction. We designed Wheeler from scratch in multiple iterations and assembled it using 3D printed models and commercially available electronics. A preliminary user study with six blind-folded sighted users revealed its potential to become an essential input device for blind users, as well as a training and learning tool.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104114224496661",videos:{Preview:"https://www.youtube.com/watch?v=RLdN1QWv6F0","Short Talk":"https://www.youtube.com/watch?v=qbWacmM7AYY"},images:{}},{id:42116,sequence:95,track:"Paper",session:[42207],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415858"},authors:["Amy X. Zhang","Grant Hugh","Michael S. Bernstein"],affiliations:["University of Washington","Stanford University"],title:"PolicyKit: Building Governance in Online Communities",keywords:[],abstract:"The software behind online community platforms encodes a governance model that represents a strikingly narrow set of governance possibilities focused on moderators and administrators. When online communities desire other forms of government, such as ones that take many members’ opinions into account or that distribute power in non-trivial ways, communities must resort to laborious manual effort. In this paper, we present PolicyKit, a software infrastructure that empowers online community members to concisely author a wide range of governance procedures and automatically carry out those procedures on their home platforms. We draw on political science theory to encode community governance into policies, or short imperative functions that specify a procedure for determining whether a user-initiated action can execute. Actions that can be governed by policies encompass everyday activities such as posting or moderating a message, but actions can also encompass changes to the policies themselves, enabling the evolution of governance over time. We demonstrate the expressivity of PolicyKit through implementations of governance models such as a random jury deliberation, a multi-stage caucus, a reputation system, and a promotion procedure inspired by Wikipedia’s Request for Adminship (RfA) process. ",qa:"discord://discordapp.com/channels/734723053036109904/764103684367056897",broadcast:"https://zoom.us/j/92340557415?pwd=YnR0bmppam1uZ3lvS3ZPVVdpS0Nidz09",videos:{"Long Talk":"https://www.youtube.com/watch?v=DnYLrMmyybg",Preview:"https://www.youtube.com/watch?v=hU1exy4Dd7M","Short Talk":"https://www.youtube.com/watch?v=UBRR2ZZSxIU"},images:{}},{id:42117,sequence:96,track:"Paper",session:[42200],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415856"},authors:["Dong-Hyun Hwang","Kohei Aso","Ye Yuan","Kris Kitani","Hideki Koike"],affiliations:["Tokyo Institute of Technology","Carnegie Mellon University"],title:"MonoEye: Multimodal Human Motion Capture System Using A Single Ultra-Wide Fisheye Camera",keywords:[],abstract:"We present MonoEye, a multimodal human motion capture system using a single RGB camera with an ultra-wide fisheye lens, mounted on the user’s chest. Existing optical motion capture systems use multiple cameras, which are synchronized and require camera calibration. These systems also have usability constraints that limit the user’s movement and operating space. Since the MonoEye system is based on a wearable single RGB camera, the wearer’s 3D body pose can be captured without space and environment limitations. The body pose, captured with our system, is aware of the camera orientation and therefore it is possible to recognize various motions that existing egocentric motion capture systems cannot recognize. Furthermore, the proposed system captures not only the wearer’s body motion but also their viewport using the head pose estimation and an ultra-wide image. To implement robust multimodal motion capture, we design three deep neural networks: BodyPoseNet, HeadPoseNet, and CameraPoseNet, that estimate 3D body pose, head pose, and camera pose in real-time, respectively. We train these networks with our new extensive synthetic dataset providing 680K frames of renderings of people with a wide range of body shapes, clothing, actions, backgrounds, and lighting conditions. To demonstrate the interactive potential of the MonoEye system, we present several application examples from common body gestural to context-aware interactions.",qa:"discord://discordapp.com/channels/734723053036109904/764103681976172594",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=t2TxY1QEnRI","Short Talk":"https://www.youtube.com/watch?v=XZA6awMwImg"},images:{}},{id:42118,sequence:97,track:"Paper",session:[42209],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415883"},authors:["Philippe Schmid","Sylvain Malacria","Andy Cockburn","Mathieu Nancel"],affiliations:["Inria Lille - Nord Europe","Inria","University of Canterbury"],title:"Interaction Interferences: Implications of Last-Instant System State Changes",keywords:[],abstract:"We study interaction interferences, situations where an unexpected change occurs in an interface immediately before the user performs an action, causing the corresponding input to be misinterpreted by the system. For example, a user tries to select an item in a list, but the list is automatically updated immediately before the click, causing the wrong item to be selected. First, we formally define interaction interferences and discuss their causes from behavioral and system-design perspectives. Then, we report the results of a survey examining users’ perceptions of the frequency, frustration, and severity of interaction interferences. We also report a controlled experiment, based on state-of-the-art experimental protocols from neuroscience, that explores the minimum time interval, before clicking, below which participants could not refrain from completing their action. Finally, we discuss our findings and their implications for system design, paving the way for future work.",qa:"discord://discordapp.com/channels/734723053036109904/764103687907835925",broadcast:"https://zoom.us/j/93681544268?pwd=UmVSS2ptNGZiL2F3ejFWUmZaTmZMZz09",videos:{Preview:"https://www.youtube.com/watch?v=shBE8awUi-U","Short Talk":"https://www.youtube.com/watch?v=9mNNyb6k-zs"},images:{}},{id:42119,sequence:98,track:"Paper",session:[42213],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415829"},authors:["Te-Yen Wu","Lu Tan","Yuji Zhang","Teddy Seyed","Xing-Dong Yang"],affiliations:["Dartmouth College","Wuhan University","Southeast University","Microsoft Research"],title:"Capacitivo: Contact-Based Object Recognition on Interactive Fabrics using Capacitive Sensing   ",keywords:[],abstract:"We present Capacitivo, a contact-based object recognition technique developed for interactive fabrics, using capacitive sensing. Unlike prior work that has focused on metallic objects, our technique recognizes non-metallic objects such as food, different types of fruits, liquids, and other types of objects that are often found around a home or in a workplace. To demonstrate our technique, we created a prototype composed of a 12 x 12 grid of electrodes, made from conductive fabric attached to a textile substrate. We designed the size and separation between the electrodes to maximize the sensing area and sensitivity. We then used a 10-person study to evaluate the performance of our sensing technique using 20 different objects, which yielded a 94.5% accuracy rate. We conclude this work by presenting several different application scenarios to demonstrate unique interactions that are enabled by our technique on fabrics.",qa:"discord://discordapp.com/channels/734723053036109904/764103690373562368",broadcast:"https://acm-org.zoom.us/j/95587008551?pwd=MTNJY2xVb2xCb2RQNHlKaERXOC9GQT09",videos:{Preview:"https://www.youtube.com/watch?v=ITYGILCULPo","Short Talk":"https://www.youtube.com/watch?v=rAgr1J3yoGY"},images:{}},{id:42120,sequence:99,track:"Paper",session:[42230],links:{DOI:"https://dl.acm.org/doi/10.1145/3379337.3415843"},authors:["Pascal Jansen","Fabian Fischbach","Jan Gugenheimer","Evgeny Stemasov","Julian Frommel","Enrico Rukzio"],affiliations:["Ulm University","Institute of Media Informatics, Ulm University","Institut Polytechnique de Paris","University of Saskatchewan","University of Ulm"],title:"ShARe: Enabling Co-Located Asymmetric Multi-User Interaction for Augmented Reality Head-Mounted Displays",keywords:[],abstract:"Head-Mounted Displays (HMDs) are the dominant form of enabling Virtual Reality (VR) and Augmented Reality (AR) for personal use. One of the biggest challenges of HMDs is the exclusion of people in the vicinity, such as friends or family. While recent research on asymmetric interaction for VR HMDs has contributed to solving this problem in the VR domain, AR HMDs come with similar but also different problems, such as conflicting information in visualization through the HMD and projection. In this work, we propose ShARe, a modified AR HMD combined with a projector that can display augmented content onto planar surfaces to include the outside users (non-HMD users). To combat the challenge of conflicting visualization between augmented and projected content, ShARe visually aligns the content presented through the AR HMD with the projected content using an internal calibration procedure and a servo motor. Using marker tracking, non-HMD users are able to interact with the projected content using touch and gestures. To further explore the arising design space, we implemented three types of applications (collaborative game, competitive game, and external visualization). ShARe is a proof-of-concept system that showcases how AR HMDs can facilitate interaction with outside users to combat exclusion and instead foster rich, enjoyable social interactions.",qa:"discord://discordapp.com/channels/734723053036109904/764103687504396288",broadcast:"https://acm-org.zoom.us/j/96609304280?pwd=dUg5bUNGQmdwMVFZMFppYzlRSzhyQT09",videos:{Preview:"https://www.youtube.com/watch?v=2waCYZUs0nA","Short Talk":"https://www.youtube.com/watch?v=6wuBt2Nv18w"},images:{}},{id:42164,sequence:1,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416184"},authors:["Yuta Noma","Koya Narumi","Fuminori Okuya","Yoshihiro Kawahara"],affiliations:["The University of Tokyo"],title:"Pop-up Print: Rapidly 3D Printing Mechanically Reversible Objects in the Folded State",keywords:[],abstract:"Despite recent advancements in 3D printing technology, which allows users to rapidly produce 3D objects, printing tall and/or large objects still consumes more time and large amount of support material. In order to address these problems, we propose Pop-up Print, a method to 3D print an object in a compact \"folded\" state and then unfold it after printing to achieve the final artifact. Using this method, we can reduce the object's print height and volume, which directly affects the printing time and support material consumption. In addition, thanks to the reversibility of folding/unfolding, we can reversibly minimize the printed object's volume when unused for storage or transportation, and expand it only in use. To achieve Pop-up Print, we first conducted an experiment using selected printed sample objects with several parameters, in order to determine suitable crease patterns that make both the unfolded and folded state mechanically stable. Based on this result, we developed an interactive design tool to convert 3D models – such as a Stanford Bunny or a Huffman's cone – to the folded shape. Our design tool allows users to decide non-intuitive parameters that may affect the form's mechanical stability, while maintaining both functional crease patterns and the object's original form factor. Finally, we demonstrate the feasibility of our method through several examples of folded objects.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103905453539348",videos:{Preview:"https://www.youtube.com/watch?v=Dl6gSzWsZWY"},images:{}},{id:42165,sequence:2,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416158","Deployed Application":"https://www.appliancizer.com/"},authors:["Jorge Garza","Devon J. Merrill","Steven Swanson"],affiliations:["University of California, San Diego"],title:"Appliancizer: Transforming web pages into electronic gadgets",keywords:["Web applications","electronics design automation","digital fabrication","computer-aided design","HTML","PCB layout"],abstract:"Making electronic gadgets that meet today's consumer standards has become a difficult task. Electronic gadgets are expected to have displays with visually appealing interfaces and, at the same time, be physically and screen-interactive, making the development process of these devices time-consuming and challenging. To address this problem, we have created Appliancizer, an online synthesis tool that can automatically generate sophisticated electronic devices from web pages. Appliancizer takes advantage of the similarities between software and physical interfaces to reduce development steps and allow the rapid development of electronic devices. By matching the interface of hardware components with the interface of graphical HTML elements found on web pages, our tool allows a designer to transform HTML elements from a digital to a tangible interface without changing the application source code. Finally, a modular design enables our tool to automatically combine the circuit design and low-level hardware code of selected hardware components into a complete design. Attendees can interact with our online tool and produce manufacturable PCBs from web pages.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103909656100864",videos:{Preview:"https://www.youtube.com/watch?v=oyg0P_sSvJs","Short Talk":"https://www.youtube.com/watch?v=KHufWXRw2rU"},images:{}},{id:42166,sequence:3,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416136","Deployed Application":"https://play.google.com/store/apps/details?id=com.google.ar.unity.arcore_depth_lab","Lab Website":"http://uist.acm.org/uist2020/","Project Website":"https://augmentedperception.github.io/depthlab/","Source Code":"https://github.com/googlesamples/arcore-depth-lab"},authors:["Ruofei Du","Eric Lee Turner","Max Dzitsiuk","Luca Prasso","Ivo Ferrao Duarte","Jason Dourgarian","Joao Afonso","Jose Pascoal","Josh Gladstone","Nuno Cruces","Shahram Izadi","Adarsh Kowdle","Konstantine Tsotsos","David Kim"],affiliations:["Google","Google, Inc.","Google Inc."],title:"Experiencing Real-time 3D Interaction with Depth Maps for Mobile Augmented Reality in DepthLab",keywords:["Depth map","interactive 3D graphics","real time","interaction","augmented reality","mobile AR","rendering","GPU","ARCore"],abstract:"We demonstrate DepthLab, a playground for interactive augmented reality experiences leveraging the shape and depth of the physical environment on a mobile phone. Based on the ARCore Depth API, DepthLab encapsulates a variety of depth-based UI/UX paradigms, including geometry-aware rendering (occlusion, shadows, texture decals), surface interaction behaviors (physics, collision detection, avatar path planning), and visual effects (relighting, 3D-anchored focus and aperture effects, 3D photos). We have open-sourced our software at https://github.com/googlesamples/arcore-depth-lab to facilitate future research and development in depth-aware mobile AR experiences. With DepthLab, we aim to help mobile developers to effortlessly integrate depth into their AR experiences and amplify the expression of their creative vision.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103912046329867",videos:{Preview:"https://www.youtube.com/watch?v=Do_sHXXsmos","Short Talk":"https://www.youtube.com/watch?v=wfVmq7d_v-I"},images:{}},{id:42167,sequence:4,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416137"},authors:["Hye Yeon Nam","Iyleah Hernandez","Brendan Harmon"],affiliations:["Louisiana State University","Dominican University"],title:"Unmasked",keywords:["COVID-19","Emotions","Expressive interface","Lip tracking","Mask","Physical computing"],abstract:"Due to the COVID-19 pandemic, wearing a mask to cover one's mouth is recommended in public spaces to prevent the spread of the virus. Wearing masks hinders our ability to express ourselves, as it is hard to read facial expressions much less lips behind a mask. We present Unmasked, an expressive interface using lip tracking to enhance communication while wearing a mask. Unmasked uses three different methods  - either accelerometers, LEDs with a camera tracking, or streaming video - to make speaking while wearing a mask more expressive. Unmasked aims to improve communication during conversations while wearing a mask. This device will help people express themselves while wearing a mask by tracking their mouth movements and displaying their facial expressions on an LCD mounted on the front of the mask. By enhancing communication while wearing a mask, this prototype makes social distancing less disruptive and more bearable, metaphorically closing some of the distance between us.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103914319773716",videos:{Preview:"https://www.youtube.com/watch?v=OZMj2LSRjbw"},images:{}},{id:42168,sequence:5,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416198"},authors:["Robert Kovacs","Eyal Ofek","Mar Gonzalez-Franco","Alexa Fay Siu","Sebastian Marwecki","Christian Holz","Mike Sinclair"],affiliations:["Hasso Plattner Institute","Microsoft Research","Stanford University","ETH Zürich","Microsoft"],title:"Haptic PIVOT: On-Demand Handhelds in VR",keywords:[],abstract:"We present PIVOT, a wrist-worn haptic device that renders virtual objects into the user’s hand on demand. Its simple design comprises a single actuated joint that pivots a haptic handle into and out of the user’s hand, rendering the haptic sensations of grasping, catching, or throwing an object – anywhere in space. Unlike existing hand-held haptic devices and haptic gloves, PIVOT leaves the user’s palm free when not in use, allowing users to make unencumbered use of their hand. PIVOT also enables rendering forces acting on the held virtual objects, such as gravity, inertia, or air-drag, by actively driving its motor while the user is firmly holding the handle. When wearing a PIVOT device on both hands, they can add haptic feedback to bimanual interaction, such as lifting larger objects. In our user study, participants (n=12) evaluated the realism of grabbing and releasing objects of different shape and size with mean score 5.19 on a scale from 1 to 7, rated the ability to catch and throw balls in different directions with different velocities (mean=5.5), and verified the ability to render the comparative weight of held objects with 87% accuracy for ~100g increments. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764103916660719616",videos:{Preview:"https://www.youtube.com/watch?v=uOhxQ5nKO6U"},images:{}},{id:42169,sequence:6,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416154","Lab Website":"http://hcil.kaist.ac.kr/","Project Website":"http://kwpark.io/deepfisheye","Source Code":"https://github.com/KAIST-HCIL/DeepFisheyeNet"},authors:["Keunwoo Park","Sunbum Kim","Youngwoo Yoon","Tae-Kyun Kim","Geehyuk Lee"],affiliations:["KAIST","ETRI","Imperial College London","School of Computing, KAIST"],title:"DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera",keywords:[],abstract:"Near-surface multi-finger tracking (NMFT) technology expands the input space of touchscreens by enabling novel interactions such as mid-air and finger-aware interactions. We present DeepFisheye, a practical NMFT solution for mobile devices, that utilizes a fisheye camera attached at the bottom of a touchscreen. DeepFisheye acquires the image of an interacting hand positioned above the touchscreen using the camera and employs deep learning to estimate the 3D position of each fingertip. We created two new hand pose datasets comprising fisheye images, on which our network was trained. We evaluated DeepFisheye’s performance for three device sizes. DeepFisheye showed average errors with approximate value of 20 mm for fingertip tracking across the different device sizes. Additionally, we created simple rule-based classifiers that estimate the contact finger and hand posture from DeepFisheye’s output. The contact finger and hand posture classifiers showed accuracy of approximately 83 and 90%, respectively, across the device sizes. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764103918984364052",videos:{Preview:"https://www.youtube.com/watch?v=Vds_rvnU6j8","Short Talk":"https://www.youtube.com/watch?v=thmtBKEazxw"},images:{}},{id:42170,sequence:7,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416163"},authors:["Waleed Hashmi","Moumena Chaqfeh","Lakshmi Subramanian","Yasir Zaki"],affiliations:["New York University Abu Dhabi","New York University Abu Dhabi (NYUAD)","New York University","NYUAD"],title:"PQual: Automating Web Pages Qualitative Evaluation",keywords:["Qualitative Evaluation","Web Pages","Similarity Score"],abstract:"The increasing complexity of web pages has brought a number of solutions to offer simpler or lighter versions of these pages. The qualitative evaluation of the new versions is commonly carried out relying on user studies. In addition to the associated costs, running user studies might become extremely challenging due to health, travel, and financial constraints. Moreover, user studies are prone to subjectivity, which makes it difficult to compare the results of different studies. In this paper, we propose PQual, a tool that enables the automation of the qualitative evaluation of web pages using computer vision. Results show that PQual computes comparable similarity scores to the scores provided by human raters. In addition, it can effectively evaluate all the functionality of a web page, whereas humans might skip many of the functional elements during the evaluation.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103921500291102",videos:{Preview:"https://www.youtube.com/watch?v=13qVW_SUP3s","Short Talk":"https://www.youtube.com/watch?v=Mj1lTXpSIUk"},images:{}},{id:42171,sequence:8,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416198"},authors:["Jun Nishida","Soichiro Matsuda","Hiroshi Matsui","Shan-Yuan Teng","ziwei Liu","Kenji Suzuki","Pedro Lopes"],affiliations:["University of Chicago","Tsukuba University","Ruhr-University Bochum","University of Tsukuba"],title:"HandMorph: a Passive Exoskeleton that Miniaturizes Grasp",keywords:["Exoskeleton","Haptics","Perception","Embodied Design"],abstract:"We engineered an exoskeleton, which we call HandMorph, that approximates the experience of having a smaller grasping range. It uses mechanical links to transmit motion from the wearer's fingers to a smaller hand with five anatomically correct fingers. The result is that HandMorph miniaturizes a wearer's grasping range while transmitting haptic feedback. \r\n\r\nUnlike other size-illusions based on virtual reality, HandMorph achieves this in the user's real environment, preserving the user's physical and social contexts. As such, our device can be integrated into the user's workflow, e.g., to allow product designers to momentarily change their grasping range into that of a child while evaluating a toy prototype.\r\n\r\nIn our first user study, we found that participants perceived objects as larger when wearing HandMorph, which suggests that their size perception was successfully transformed.\r\nIn our second user study, we assessed the experience of using HandMorph in designing a simple toy trumpet for children. We found that participants felt more confident in their toy design when using HandMorph to validate its ergonomics.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103924185563146",videos:{Preview:"https://www.youtube.com/watch?v=BJTXSVIMjrg","Short Talk":"https://www.youtube.com/watch?v=ms0mf5OwN-k"},images:{}},{id:42172,sequence:9,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416153"},authors:["Yevhen Romaniak","Anastasiia Smielova","Yevhenii Yakishyn","Valerii Dziubliuk","Mykhailo Zlotnyk","Oleksandr Viatchaninov"],affiliations:["Samsung R&D Institute Ukraine (SRK)"],title:"Nimble: Mobile Interface for a Visual Question Answering Augmented by Gestures",keywords:["Attention Mechanism","Visual Question Answering","Co-Attention","Pointing Gesture","Virtual Assistant"],abstract:"Virtual Assistants are becoming increasingly popular. How-\r\never voice-only systems providing limited functionality and\r\nminimal variability are often unusable because the exposed\r\nbehavior is either fully deterministic or essentially ambiguous\r\nfor supporting human-like dialogues. This paper introduces a\r\nNimble, solution that allows users to ask short questions and\r\nto get answers about objects selected from a scene by natural\r\npointing gestures. With a modified Visual Question Answering\r\nmodel we have shown how the integration of gestures to the\r\nattention mechanism can reduce questions’ ambiguity while\r\nsustaining the same accuracy level of the system. We per-\r\nformed this by modifying the model’s attention scores using\r\ngestures fused with linguistic information.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103926629924874",videos:{Preview:"https://www.youtube.com/watch?v=8Uz1WQDATdI"},images:{}},{id:42173,sequence:10,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416139"},authors:["Rishi Vanukuru","Amarnath Murugan","Jayesh S. Pillai"],affiliations:["Indian Institute of Technology Bombay","Indian Institute of Technology, Bombay"],title:"Dual Phone AR: Using a Second Phone as a Controller for Mobile Augmented Reality",keywords:["Augmented Reality","Mobile Interaction","Cross-Device Computing"],abstract:"Mobile Augmented Reality applications have become increasingly popular, however the possible interactions with AR content are largely limited to on-screen gestures and spatial movement. There has been a renewed interest in designing interaction methods for mobile AR that go beyond the screen. Mobile phones present a rich range of input, output, and tracking capabilities, and have been used as controllers for Virtual Reality and head-mounted Augmented Reality applications. In this project, we explore the use of a second phone as a controller for Mobile AR. We developed ARTWO, an application that showcases Handheld Dual Phone AR through a series of small demos in which a second phone can be used to perform basic tasks such as pointing, selecting, and drawing, in the context of real use cases. We believe that the Dual Phone AR approach can help address many of the issues faced when using conventional mobile AR interactions, and also serves as a stepping stone to the general use of phones with head-mounted AR systems in the near future.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103929268797470",videos:{"Short Talk":"https://www.youtube.com/watch?v=SoJ-USeNelE"},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42174,sequence:11,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416137"},authors:["Alex Olwal","Kevin Balke","Dmitrii Votintcev","Thad Starner","Paula Conn","Bonnie Chinh","Benoit Corda"],affiliations:["Google Research","Google, Inc.","Google"],title:"Wearable Subtitles: Augmenting Spoken Communication with Lightweight Eyewear for All-day Captioning",keywords:[],abstract:"Mobile solutions can help transform speech  and sound into visual representations for people who are deaf or hard-of-hearing (DHH). However, where handheld phones present challenges, head-worn displays (HWDs) could further communication through privately transcribed text, hands-free use, improved mobility, and socially acceptable interactions. In this work, we introduce a lightweight HWD system specifically designed to augment communication through sound transcription for a full workday. Through a hybrid, low-power architecture, we enable up to 15 hours of continuous use. We describe a large survey (n=501) and three user studies with 24 deaf/hard-of-hearing participants which inform our development and help us refine our prototypes. Our studies and prior research identify critical challenges for the adoption of HWD systems which we address through extended battery life, lightweight and balanced mechanical design (54 g), fitting options, and form factors that are compatible with current social norms.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103931773059092",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42175,sequence:12,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416163"},authors:["Vidya Setlur","Enamul Hoque","Dae Hyun Kim","Angel Xuan Chang"],affiliations:["Tableau Research","York University","Stanford University","Simon Fraser University"],title:"Sneak Pique: Exploring Autocompletion as a Data Discovery Scaffold for Supporting Visual Analysis",keywords:[],abstract:"Natural language interaction has evolved as a useful modality to help users explore and interact with their data during visual analysis. Little work has been done to explore how\r\nautocompletion can help with data discovery while helping users formulate analytical questions. We developed a system called Sneak Pique as a design probe to better understand the usefulness of autocompletion for visual analysis. We ran three Mechanical Turk studies to evaluate user preferences for various text- and visualization widget-based autocompletion design variants for helping with partial search queries. Our findings indicate that users found data previews to be useful in the suggestions. Widgets were preferred for previewing temporal, geospatial, and numerical data while text autocompletion was preferred for categorical and hierarchical data. We conducted an exploratory analysis of our system implementing this specific subset of preferred autocompletion variants. Our insights regarding the efficacy of these autocompletion\r\nsuggestions can inform the future design of natural language\r\ninterfaces supporting visual analysis.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103934569611304",videos:{Preview:"https://www.youtube.com/watch?v=A4YFk_yBW5k"},images:{}},{id:42176,sequence:13,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416153","Short Talk":"https://youtu.be/xavNhLH_3_s"},authors:["Daehwa Kim","Keunwoo Park","Geehyuk Lee"],affiliations:["KAIST","School of Computing, KAIST"],title:"OddEyeCam: A Sensing Technique for Body-Centric Peephole Interaction using WFoV RGB and NFoV Depth Cameras",keywords:[],abstract:"The space around the body not only expands the interaction space of a mobile device beyond the small screen, but also allows users to utilize their kinesthetic sense. Therefore, body-centric peephole interaction has gotten attention. To support its practical implementation, we propose OddEyeCam, which is a vision-based method that tracks the 3D location of a mobile device in an absolute, wide, and continuous manner with respect to the user’s body in both static and mobile environments. OddEyeCam tracks the user’s body with a wide view RGB camera and gets precise depth information with a narrow view depth camera from a smartphone close to the body. We evaluated OddEyeCam quantitatively through an accuracy test and two user studies. The accuracy test showed that OddEyeCam has an average tracking accuracy of 4.17 and 4.47cm in 3D space when the user is standing still and walking, respectively. In the first user study, we implemented various interaction scenarios and observed OddEyeCam was well received by the participants. In the second, we observed a peephole target acquisition task using our system followed Fitts' law. We also analyzed the performance of OddEyeCam using obtained measurements and found that the participants completed the tasks fast and accurately enough.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103937099825203",videos:{Preview:"https://www.youtube.com/watch?v=R56iuEuZyo0"},images:{}},{id:42177,sequence:14,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416184"},authors:["Ryuma Niiyama","Hiroki Sato","kazzmasa TSUJIMURA","Koya Narumi","Young ah Seong","Ryosuke Yamamura","Yasuaki Kakehi","Yoshihiro Kawahara"],affiliations:["The University of Tokyo","mercari R4D"],title:"poimo: Portable and Inflatable Mobility Devices Customizable for Personal Physical Characteristics",keywords:[],abstract:"Despite the recent growth in popularity of personal mobility devices (e.g., e-scooters and e-skateboards), they still suffer from limited safety and narrow design form factors, due to their rigid structures. On the other hand, inflatable interfaces studied in human-computer interaction can achieve large volume change by simple inflation/deflation. Inflatable structure also offers soft and safe interaction owing to material compliance and diverse fabrication methods that lead to a wide range of forms and aesthetics. In this paper, we propose poimo, a new family of POrtable and Inflatable MObility devices, which consists of inflatable frames, inflatable wheels, and inflatable steering mechanisms made of a mass-manufacturable material called drop-stitch fabric. First, we defined the basic material properties of a drop-stitch inflatable structure that is sufficiently strong to carry a person while simultaneously allowing soft deformation and deflation for storage and portability. We then implemented an interactive design system that can scan the user’s desired riding posture to generate a customized personal mobility device and can add the user’s shape and color preferences. To demonstrate the custom-design capability and mobility, we designed several 3D models using our system and built physical samples for two basic templates: a motorcycle and a wheelchair. Finally, we conducted an online user study to examine the usability of the design system and share lessons learned for further improvements in the design and fabrication of poimo.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103939725197393",videos:{Preview:"https://www.youtube.com/watch?v=Xgailzg-jSw"},images:{}},{id:42178,sequence:15,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416162"},authors:["Tomoya Tada","Shigeyuki Hirai"],affiliations:["Graduate Kyoto Sangyo University","Kyoto Sangyo University"],title:"Transmissive LED Touch Display for Engineered Marble",keywords:['"smart display',"interactive surface","touch interface","infrared reflection",'augmented table"'],abstract:"In this paper, we propose a thin display panel system for converting an existing kitchen or vanity tabletop with an engineered marble into a smart touch display. The display system, which consists of an LED matrix panel including IR-LEDs and photodiodes, will be placed on the rear side of a tabletop of a kitchen or washbasins. The normal LED light is used for a visual display transmitted with a thick opaque resin, and the infrared light is used for detecting touch areas and objects using the same resin panel. This display panel can be used for various applications and can be touched with wet and/or dirty hands. In this paper, we describe the system overview, the optical characteristics of blurred display via an engineered marble plate, and an touch-sensing experiment. We also provide some applications of the smart touch display.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103942300762162",videos:{Preview:"https://www.youtube.com/watch?v=KVCbdsZZudM","Short Talk":"https://www.youtube.com/watch?v=we3jLWr9OOc"},images:{}},{id:42179,sequence:16,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416138","Lab Website":"https://www.sony.net/SonyInfo/technology/about/rdc/tech-portfolio/human_interaction/"},authors:["Hiroshi Suzuki","Akihito Nishiike","Kentaro Yoshida","Masahiro Sato","Yota Komoriya","Takayuki Ezaki"],affiliations:["Sony Corporation"],title:"Haptic Reproduction by Pneumatic Control Method Based on Load-Displacement Profile",keywords:["Haptics","Softness","Pneumatic","Virtual reality",""],abstract:"It is known that the pressure and contact area change contribute to hard and soft perception as a cutaneous sensation.\r\nIn this study, we propose a novel method of haptic presentation based on the profiles of physical objects by using the load-displacement measurement.\r\nWe fabricated a pneumatic haptic system with an elastic membrane that enables controlled pressure stimuli.\r\nWe verified that the proposed method is capable of reproducing various profiles by comparing with physical objects.\r\nAs a result, we found that our system could well reproduce the load-displacement profiles of a sponge and a button specimen.\r\n",broadcast:"discord://discordapp.com/channels/734723053036109904/764103944640528404",videos:{Preview:"https://www.youtube.com/watch?v=I0-MPpAJ-Rk","Short Talk":"https://www.youtube.com/watch?v=FnAMxw0Qz10"},images:{}},{id:42180,sequence:17,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416138"},authors:["Ken Nakagaki","Joanne Leong","Jordan L Tappa","Joao Wilbert","Hiroshi Ishii"],affiliations:["MIT Media Lab","MIT","Massachusetts Institute of Technology"],title:"HERMITS: Dynamically Reconfiguring the Interactivity of Self-propelled TUIs with Mechanical Shell Add-ons",keywords:[],abstract:"We introduce HERMITS, a modular interaction architecture for self-propelled Tangible User Interfaces (TUIs) that incorporates physical add-ons, referred to as mechanical shells. The mechanical shell add-ons are intended to be dynamically reconfigured by utilizing the locomotion capability of self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed a proof-of-concept system that demonstrates this novel architecture using two-wheeled robots and a variety of mechanical shell examples. These mechanical shell add-ons are passive physical attatchments that extend the primitive interactivities (e.g. shape, motion and light) of the self-propelled robots. \r\n\r\nThe paper proposes the architectural design, interactive functionality of HERMITS as well as design primitives for mechanical shells. The paper also introduces the prototype implementation that is based on an off-the-shelf robotic toy with a modified docking mechanism. A range of applications is demonstrated with the prototype to motivate the collective and dynamically reconfigurable capability of the modular architecture, such as an interactive mobility simulation, an adaptive home/desk environment, and a story-telling narrative. Lastly, we discuss the future research opportunity of HERMITS to enrich the interactivity and adaptability of actuated and shape changing TUIs.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103947044388914",videos:{Preview:"https://www.youtube.com/watch?v=Ts_f26Xxb_w","Short Talk":"https://www.youtube.com/watch?v=0aM3duOLrrY"},images:{}},{id:42181,sequence:18,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416163"},authors:["Jiahao Li","Meilin Cui","Jeeeun Kim","Xiang 'Anthony' Chen"],affiliations:["UCLA","University of California, Los Angeles","Texas A&M University"],title:"Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities",keywords:[],abstract:"Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design.\r\n\r\nWe present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103949673824258",videos:{Preview:"https://www.youtube.com/watch?v=9juk3HjLKHc"},images:{}},{id:42182,sequence:19,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416154"},authors:["Eric Balagtas Perez","James King","Yugo H Watanabe","Xiang 'Anthony' Chen"],affiliations:["UCLA"],title:"Counterweight: Diversifying News Consumption",keywords:["Online news","political polarity","media bias","bias mitigation"],abstract:"The bias of news articles can strongly affect the opinions and behaviors of readers, especially if they do not consume sets of articles that represent diverse political perspectives. To mitigate media bias and diversify news consumption, we developed Counterweight---a browser extension that presents different perspectives by recommending articles relevant to the current topic. We provide a platform to encourage a more diversified consumption of news and mitigate the negative effects of media bias.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103952596860948",videos:{Preview:"https://www.youtube.com/watch?v=0WJbsC8tksQ","Short Talk":"https://www.youtube.com/watch?v=IQ0zujzOFUM"},images:{}},{id:42183,sequence:20,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416184"},authors:["Richard Lin","Rohit Ramesh","Connie Chi","Nikhil Jain","Ryan Nuqui","Prabal Dutta","Bjoern Hartmann"],affiliations:["University of California, Berkeley","UC Berkeley"],title:"Polymorphic Blocks: Unifying High-level Specification and Low-level Control for Circuit Board Design",keywords:[],abstract:"Mainstream board-level circuit design tools work at the lowest level of design, of schematics and individual components. While novel tools are experimenting with higher levels of design, it is at the expense of the fine-grained control afforded by low-level tools. In this work, we propose a hardware description language (HDL) approach that supports the user at multiple levels of abstraction, from the broad system architecture, to subcircuits, to component selection. We extend the familiar hierarchical block diagram with polymorphism through abstract-typed blocks (eg, generic resistor supertype) and electronics modeling (eg, currents and voltages). Such an approach brings the advantages of reusability and encapsulation from object-oriented programming, while addressing the unique needs of electronics designers such as physical correctness verification. We discuss the system design, including fundamental abstractions, the block diagram construction HDL, and graphical interfaces to inspect and fine-tune the design; demonstrate example designs built with our system; and present feedback from intermediate-level engineers who have worked with our system.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103955163906070",videos:{Preview:"https://www.youtube.com/watch?v=RdG9UkWaCYM","Short Talk":"https://www.youtube.com/watch?v=9fUwieBj454"},images:{}},{id:42184,sequence:21,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416143","Deployed Application":"https://news-navigator.labs.loc.gov/search","Lab Website":"https://hai.cs.washington.edu/","Project Website":"https://bcglee.github.io/newspaper-navigator.html","Source Code":"https://github.com/LibraryOfCongress/newspaper-navigator"},authors:["Benjamin Charles Germain Lee","Daniel Weld"],affiliations:["University of Washington"],title:"Newspaper Navigator: Open Faceted Search for 1.5 Million Images",keywords:["search user interface","exploratory search","human-AI interaction","interactive machine learning","digital humanities"],abstract:"Despite the marked improvements in image classification and recognition tasks over the past decade, affordances and interfaces for searching images have remained largely unchanged on the web. In this demo, we present Newspaper Navigator, an open faceted search system for 1.5 million historic newspaper photographs. In contrast to standard faceted search, which requires facets to be pre-selected and applied to images, Newspaper Navigator empowers users to specify their own facets in an open-domain fashion during the search process by selecting relevant examples and iteratively training a machine learner. With Newspaper Navigator, users can quickly sort 1.5 million images according to dynamically-specified facets such as \"baseball player'' and \"oval-shaped portrait.'' Newspaper Navigator also drives facet exploration by suggesting related keyword search queries for a user to perform. Our demo walks through examples of searching with Newspaper Navigator and highlights the facet learning and exploration affordances.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103958213296178",videos:{Preview:"https://www.youtube.com/watch?v=1WfTFVXx1fg","Short Talk":"https://www.youtube.com/watch?v=9w7ippuo3Gk"},images:{}},{id:42185,sequence:22,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416144"},authors:["Sawa Korogi","Takuya Kitade","Wataru Yamada"],affiliations:["NTT DOCOMO, INC."],title:"Janus Screen: A Screen with Switchable Projection Surfaces Using Polarizers",keywords:['"projection screem", "augument reality", "ubiquitous computing"'],abstract:"In the area of ubiquitous computing, there is research that use surfaces of elements in the environment such as windows and walls as screens.\r\nIt is necessary to install an opaque screen and projectors on  both sides of the screen in order to switch projection surfaces, i.e., front, back, or both sides.\r\nIn this paper, we propose Janus Screen, a novel screen system with polarizers that allow switching of the projection surface to the front, back, or both sides using only projectors on one side.\r\nJanus Screen employs projectors equipped with polarizers and a screen with multiple layers.\r\nWe implement a prototype and confirm that Janus Screen can selectively switch the projection surface.\r\nMoreover, we discuss unique characteristics and possible applications of the proposed system.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103961053364235",videos:{Preview:"https://www.youtube.com/watch?v=gw1ZYQS04KE"},images:{}},{id:42186,sequence:23,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416138"},authors:["Artem Dementyev","Alex Olwal","Richard F Lyon"],affiliations:["Google Research","Google"],title:"Haptics with Input: Back-EMF in Linear Resonant Actuators to Enable Touch, Pressure and Environmental Awareness",keywords:[],abstract:"In today's wearable and mobile devices, sensing and actuation are done by different pieces of hardware. \r\nIn this work, we introduce new opportunities for the Linear Resonant Actuator (LRA), which is ubiquitous in today's mobile devices due to its capability for providing rich haptic feedback. By leveraging strategies to enable active and passive sensing capabilities with LRAs, we demonstrate its benefits and potential as a self-contained I/O device. Specifically, we use the back-EMF voltage to classify if the LRA is tapped, touched, as well as how much pressure is being applied. The back-EMF sensing is already integrated into many motor and LRA drivers. We developed a passive low-power tap sensing method that uses just 37.7 uA. Furthermore, we developed active touch and pressure sensing, which is low-power, quiet (2 dB), and produces minor vibrations. The sensing method works with many types of LRAs. We show applications, such as pressure-sensing side-buttons on a mobile phone, and detection of whether the phone is hand-held or on a table. Also, we show that for haptic devices, this method can be used to determine if the LRA makes good contact with the skin. Our approach can add rich sensing capabilities to the ubiquitous LRA actuators without requiring additional sensors or hardware.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103963955298304",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:42187,sequence:24,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416163","Project Website":"https://skylineprof.github.io","Source Code":"https://github.com/skylineprof/skyline"},authors:["Geoffrey X. Yu","Tovi Grossman","Gennady Pekhimenko"],affiliations:["University of Toronto","Vector Institute"],title:"Skyline: Interactive In-Editor Computational Performance Profiling for Deep Neural Network Training",keywords:[],abstract:"Training a state-of-the-art deep neural network (DNNs) is a computationally-expensive and time-consuming process, which incentivizes deep learning developers to debug their DNNs for computational performance. However, effectively performing this debugging requires intimate knowledge about the underlying software and hardware systems—something that the typical deep learning developer may not have. To help bridge this gap, we developed Skyline: a new interactive tool for DNN training that supports in-editor computational performance profiling, visualization, and debugging. Skyline's key contribution is that it leverages special computational properties of DNN training to provide (i) interactive performance predictions and visualizations, and (ii) directly manipulatable visualizations that mutate the code when dragged. As an in-editor tool, Skyline allows users to leverage these diagnostic features to debug the performance of their DNNs during development. In our demo, we will walk through each of Skyline’s features using ResNet-50 as an example DNN. We will show attendees how they can use Skyline’s interactive features to investigate the performance characteristics (e.g., memory and run time bottlenecks) of ResNet-50 and we will give attendees a chance to try using Skyline for themselves.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103966786322502",videos:{Preview:"https://www.youtube.com/watch?v=4GReNYxxIAo"},images:{}},{id:42188,sequence:25,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416134"},authors:["Takahiro Kusabuka","Takuya Indo"],affiliations:["NTT Coropration","NTT Corporation"],title:"IBUKI: Gesture Input Method Based on Breathing",keywords:["Breathing","Breathing Gestures","Breathing Interaction"],abstract:"In this paper, we propose a method for gesture input which uses a sensor attached to a mask to detect the user’s breathing and trajectory of his or her breath. To prevent the spread of infectious diseases, it is important to keep our hands clean and avoid spreading the virus to our homes. Using a smartphone after touching various objects in public facilities increases the risk of infection. Our research focuses on operating devices such as smartphones using breathing gestures rather than touching the devices directly. The gestures were performed while wearing a commercial mask with a breathing sensor and a 9-DoF IMU attached. The breathing sensor detects inhalation and exhalation patterns, and the 9-DoF IMU estimates the breath direction and movement trajectory by the user’s head. The breathing gesture was estimated by combining the information from the two sensors. Through prototyping, we determined that our method was able to estimate the gestures. We also provide an example application of this interface.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103971257974834",videos:{Preview:"https://www.youtube.com/watch?v=7JFIDXsRqt4","Short Talk":"https://www.youtube.com/watch?v=7JFIDXsRqt4"},images:{}},{id:42189,sequence:26,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416155","Project Website":"https://www.media.mit.edu/projects/defextiles/overview/"},authors:["Jack Forman","Mustafa Doga Dogan","Hamilton Forsythe","Hiroshi Ishii"],affiliations:["Massachusetts Institute of Technology","MIT CSAIL","MIT"],title:"Demonstration of DefeXtiles: 3D Printing Quasi-Woven Fabric via Under-Extrusion",keywords:[],abstract:"We present DefeXtiles, a rapid and low-cost technique to produce tulle-like fabrics on unmodified fused deposition modeling (FDM) printers. The under-extrusion of filament is a common cause of print failure, resulting in objects with periodic gap defects. We demonstrate that these defects can be finely controlled to quickly print thinner, more flexible textiles than previous approaches allow. Our approach allows hierarchical control from micrometer structure to decameter form and is compatible with all common 3D printing materials.\r\n\r\nIn this demo, we introduce the mechanism of DefeXtiles, detail the design space through a set of primitives with detailed workflows, and discuss the mechanical properties of DefeXtiles printed with multiple materials and parameters. Finally, we demonstrate the interactive features and new use cases of our approach through a variety of applications, such as fashion design prototyping, interactive objects, aesthetic patterning, and single-print actuators.\r\n",broadcast:"discord://discordapp.com/channels/734723053036109904/764103973963169812",videos:{Preview:"https://www.youtube.com/watch?v=_o7Duvtbd5M"},images:{}},{id:43214,sequence:1,track:"SIC",session:[42908],links:{DOI:"https://uist.acm.org/uist2020/"},authors:["Kyzyl Monteiro","Arunesh Singh"],affiliations:["Weave Lab, IIIT-Delhi, New Delhi, India"],title:"Everyday Stories: Enabling Richer Interactions for remote AR storytelling experiences",keywords:[],abstract:"Due to the Covid-19 pandemic, real-life interactions have been minimized and have inherently disconnected us. We explore the concept of connecting remotely located family and friends through the medium of storytelling. We propose an AR shared-world experience where the presenter can control the story’s narrative while the viewer interacts with it. Using commonplace devices, the presenter can move virtual characters by moving real-life objects they are mapped to, set up the environment and secondary props by sketching them, and animate them with verbal cues. Parallelly, the viewer can watch and interact with the characters through the screen or hand gestures in the shared world. The integration of AR, speech, objects, sketching, and gestures creates a rich interactive remote storytelling experience.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104117369962536",videos:{Preview:"https://www.youtube.com/watch?v=y5_jmJc9AV4"},images:{}},{id:42190,sequence:27,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416135"},authors:["Junyi Zhu","Yunyi Zhu","Jiaming Cui","Leon Cheng","Jackson C Snowden","Mark Chounlakone","Michael Wessely","Stefanie Mueller"],affiliations:["MIT CSAIL"],title:"Demonstration of MorphSensor: A 3D Electronic Design Tool for Reforming Sensor Modules",keywords:["electronic design","interactive devices","integrating form and function","personal fabrication."],abstract:"MorphSensor is a 3D electronic design tool that enables designers to morph existing sensor modules of pre-defined two-dimensional shape into free-form electronic component arrangements that better integrate with the three-dimensional shape of a physical prototype. \r\n\r\nMorphSensor builds onto existing sensor module schematics that already define the electronic components and the wiring required to build the sensor. Since MorphSensor maintains the wire connections throughout the editing process, the sensor remains fully functional even when designers change the electronic component layout on the prototype geometry. \r\n\r\nWe present the MorphSensor editor that supports designers in re-arranging the electronic components, and discuss a fabrication pipeline based on customized PCB footprints for making the resulting freeform sensor. ",broadcast:"discord://discordapp.com/channels/734723053036109904/764103976592998410",videos:{Preview:"https://www.youtube.com/watch?v=y4pEyWeEfTQ"},images:{}},{id:43215,sequence:2,track:"SIC",session:[42908],links:{DOI:"https://uist.acm.org/uist2020/"},authors:["Navneet Ratnakirti","Shreemoy Nanda","Aman Mandal"],affiliations:["Dept. of Production Engg., VSS University of Technology, Odisha, India","Dept. of Electronics & TelComm. Engg., VSS University of Technology, Odisha, India"],title:"WeR-Rehab: An Enhanced VR Motor Skills Rehabilitation Physio Platform",keywords:[],abstract:"The global pandemic that we are facing has deeply affected most of our lives due to the social isolation norms. Also, if you’re a middle-aged person who has met with a certain mishap leading to a muscle injury or even underwent a recent orthopedic surgery; it is imminent that you will be searching for a physiotherapist who provides personalized assistance for your physical rehab process. But now the physios are unwilling to perform inperson therapy sessions due to the risk of the virus spread. Thus, we want to bridge this gap by introducing a physiotherapy platform that connects the participants in an avatar-based VR environment along with the sensation of telepresence. Here even the VR headsets would be augmented with powerful motion capture and user sensing capabilities at a very low cost. This brings about therapy sessions in the virtual environment. As correct methods and postures are the most important factor during a muscle rehabilitation process so constant supervision is necessary and only an expert must guide the patient closely.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104120197316668",videos:{Preview:"https://www.youtube.com/watch?v=IRQ4f5dMz9Q"},images:{}},{id:42191,sequence:28,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416198","Project Website":"https://wbstx.github.io/grabar/"},authors:["Xiao Tang","Xiaowei Hu","Chi-Wing Fu","Daniel CohenOr"],affiliations:["The Chinese University of Hong Kong","The Chinse University of Hong Kong","Tel Aviv University"],title:"GrabAR: Occlusion-aware Grabbing Virtual Objects in AR",keywords:["Augmented reality","occlusion","interaction","nerual network"],abstract:"Existing augmented reality (AR) applications often ignore the occlusion between real hands and virtual objects when incorporating virtual objects in user's views. The challenges come from the lack of accurate depth and mismatch between real and virtual depth.This paper presents GrabAR, a new approach that directly predicts the real-and-virtual occlusion and bypasses the depth acquisition and inference. Our goal is to enhance AR applications with interactions between hand (real) and grabbable objects (virtual).\r\nWith paired images of hand and object as inputs, we formulate a compact deep neural network that learns to generate the occlusion mask. Further, we demonstrate the performance of our method on various virtual objects, compare our method with others through one user study, and showcase a rich variety of interaction scenarios, in which we can use bare hand to grab virtual objects and directly manipulate them.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103979336073236",videos:{Preview:"https://www.youtube.com/watch?v=TZgepi4ssP0"},images:{}},{id:43216,sequence:3,track:"SIC",session:[42908],links:{DOI:"https://uist.acm.org/uist2020/"},authors:["Han Yao Choong","Jonathan Lehner","Jan Wiegner"],affiliations:["ETH Zurich, Raemistrasse 101, 8092 Zurich, Switzerland"],title:"ErgoChat: A VR-Based Professional Telepresence Platform",keywords:[],abstract:"We propose a telepresence platform that provides a virtual office environment with realistic representations of common office interactions. Covid-19 has made social distancing essential, particularly affecting corporations and the education sector. Improving upon mainstream video chat and confer-encing products, we include body language for 3D avatars to increase communication efficiency and allow users to flexibly drop-in to colleagues’ work spaces as well as the integration of acoustics with virtual spatial models to enable users to overhear colleagues’ conversations with realistic characteristics as in open offices. We solve these issues by using virtual reality (VR) headsets, real-time 3D pose estimation and a virtual auditory display (VAD) system.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104123225604116",videos:{Preview:"https://www.youtube.com/watch?v=0ZBti8kIGF8"},images:{}},{id:42192,sequence:29,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416198"},authors:["Youngbo Aram Shim","Keunwoo Park","Sangyoon Lee","Jeongmin Son","Taeyun Woo","Geehyuk Lee"],affiliations:["KAIST","School of Computing, KAIST"],title:"FS-Pad: Video Game Interactions with a Force Feedback Gamepad",keywords:[],abstract:"Force feedback was not fully explored in modern gaming environments where a gamepad is the main interface. We developed various game interaction scenarios where force feedback through a gamepad's thumbstick could be effective and categorized them into five themes. We built a haptic device and its control system that could support all the presented interactions. The resulting device, FS-Pad, has sufficient fidelity to be used as a haptic game interaction design tool. To verify the presented interactions and the FS-Pad's effectiveness, we conducted a user study with game players, developers, and designers. The subjects used the FS-Pad while playing a demo game and then participated in an interview. Their feedback revealed actual needs for the presented interactions and insights for designing game interactions using the FS-Pad.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103981953187860",videos:{Preview:"https://www.youtube.com/watch?v=CaOFOnD6ADk","Short Talk":"https://www.youtube.com/watch?v=7O4AqhINGlk"},images:{}},{id:43217,sequence:4,track:"SIC",session:[42908],links:{DOI:"https://uist.acm.org/uist2020/"},authors:["Toshiharu Igarashi","Kaoru Takagi"],affiliations:["The University of Tokyo, Kashiwa-no-ha, Chiba, Japan"],title:"Ele-Robo: Safety Monitoring Support Robot for Elders Using Power Waveform of Home Appliances",keywords:[],abstract:"Global pandemic has reduced the elders’ opportunities to go out and interact with others, which can increase their isolation. It is necessary to monitor their physical and mental status medically without increasing the risk of infection through interpersonal contact. However, installation of cameras or sensors in each household conflicts with elders’ desire for privacy. Therefore, we plan to achieve the following human-robot interaction (HRI) by examining home appliances using a clamp-type current sensor and parsing the usage of home appliances into socially assistive robots (SARs).",broadcast:"discord://discordapp.com/channels/734723053036109904/764104128644251679",videos:{Preview:"https://www.youtube.com/watch?v=xyTBNNrwWQw"},images:{}},{id:42193,sequence:30,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416155"},authors:["Ryo Suzuki","Rubaiat Habib Kazi","Li-yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],affiliations:["University of Calgary","University of Colorado Boulder","Adobe Research","Adobe","University of Colorado, Boulder"],title:"Demonstrating RealitySketch: Embedding ResponsiveGraphics and Visualizations in AR with Dynamic Sketching",keywords:["augmented reality","embedded data visualization","real-time authoring","sketching interfaces","tangible interaction"],abstract:"We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.\r\n",broadcast:"discord://discordapp.com/channels/734723053036109904/764103984503193631",videos:{Preview:"https://www.youtube.com/watch?v=YPU_pSHU5-o"},images:{}},{id:42194,sequence:31,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416145"},authors:["Ben Greenspan","Andreea Danielescu"],affiliations:["Accenture Labs"],title:"Designing Low-Cost Sports Prosthetics with Advanced 3D Printing Techniques",keywords:["Prosthetics","adaptive sports","3D printed electronics","accessibility","haptics","multi-material printing"],abstract:"Prosthetic limbs are an extension of the human body and should allow for the same function and control that a natural limb provides. Most prosthetics aim to allow an individual to return to a baseline quality of life, but few allow the individual to return to sports. Sport prosthetics can be difficult to produce due to the specificity of the designs and associated costs. Some prosthetics are becoming more accessible with the availability of 3D-printing. In this work, we create a prototype basketball prosthetic hand to enable a more natural shot. This prototype uses 3D printed springs for energy return and 3D printed electronics for integrated strain sensing that can be mapped to haptic feedback for the user. Combining these technologies will lead to a sport prosthetic limb whose technology advancements can be applied to future prosthetic designs.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103987669631007",videos:{Preview:"https://www.youtube.com/watch?v=TG5dzBZh9nQ","Short Talk":"https://www.youtube.com/watch?v=4e90ivGkOlk"},images:{}},{id:42195,sequence:32,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416156"},authors:["Vishnu Nair","Brian A. Smith"],affiliations:["Columbia University"],title:"Toward Self-Directed Navigation for People with Visual Impairments",keywords:["Audio navigation tools","Self-directed navigation","Blind-accessible games","Visual impairments"],abstract:"Audio navigation tools have the potential to help visually impaired people (VIPs) navigate independently, but today's audio navigation tools merely guide users to destinations rather than give users the full freedom to explore and navigate environments themselves. To address this problem, we present NavStick, a self-directed audio navigation tool for virtual environments. NavStick repurposes a game controller’s thumbstick to allow a user to survey their immediate surroundings by \"scrubbing\" the thumbstick in a circular fashion. We also describe a user study that we are performing to compare NavStick with today's guide-based audio navigation tools.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103990655844362",videos:{Preview:"https://www.youtube.com/watch?v=RLdN1QWv6F0","Short Talk":"https://www.youtube.com/watch?v=tjASdh-niVQ"},images:{}},{id:42196,sequence:33,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3416198"},authors:["Ritam Jyoti Sarmah","Yunpeng Ding","Di Wang","Cheuk Yin Phipson Lee","Toby Jia-Jun Li","Xiang 'Anthony' Chen"],affiliations:["UCLA HCI","University of California, Los Angeles","Carnegie Mellon University","UCLA"],title:"Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications",keywords:[],abstract:'Supporting voice commands in applications presents significant benefits to users. However, adding such support to existing GUI-based web apps is effort-consuming with a high learning barrier, as shown in our formative study, due to the lack of unified support for creating multi-modal interfaces. We develop Geno---a developer tool for adding the voice input modality to existing web apps without requiring significate NLP expertise. Geno provides a unified workflow for developers to specify functionalities to support by voice (intents), create language models for detecting intents and the relevant information (parameters) from user utterances, and fulfill the intents by either programmatically invoking the corresponding functions or replaying GUI actions on the web app. Geno further supports references to GUI context in voice commands (e.g., "add this to the playlist"). In a study, developers with little NLP expertise were able to add the multi-modal support for two existing web apps using Geno.',broadcast:"discord://discordapp.com/channels/734723053036109904/764103997328982046",videos:{Preview:"https://www.youtube.com/watch?v=rogkyPaQWIU"},images:{}},{id:43225,sequence:1,track:"DC",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3415807"},authors:["Kristin Williams"],affiliations:["Human Comuter Interaction Institute, Carnegie Mellon University"],title:"IoT Stickers: Enabling Lightweight Modification of Everyday Objects",keywords:["Paper computing","tangible interfaces","batter-free","Internet of Things"],abstract:"Internet-of-Things (IoT) devices promise to enhance even the most mundane of objects with computational properties by seamlessly coupling the virtual world to the physical. How- ever, IoT’s associated costs and cumbersome setup limits its extension to many everyday tasks and objects, such as those in the home. To address these issues, my dissertation work will enable IoT Stickers—a book of inexpensive, battery-free sensors and composition patterns—to support customizing ev- eryday objects with software and web services using stickers. Using RFID-based paper mechanisms, IoT Stickers integrates common sensors and web services with interactive stickers through a trigger-action architecture. This integration enables computational services to be tailored to everyday activities by setting parameters to be passed to the sticker’s actions and composing the stickers together. Thus, IoT Stickers demonstrates a way to associate IoT services with a dramatically wider set of objects and tasks.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103888122413057",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43226,sequence:2,track:"DC",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3415805"},authors:["Han L. Han"],affiliations:["Université Paris-Saclay, CNRS, Inria"],title:"Designing Representations for Digital Documents",keywords:[],abstract:"Hundreds of millions of users work with digital documents for their everyday tasks but the user interfaces have not fundamentally changed since they were first designed in the late seventies [10]. We focus on two examples of today’s “extreme users” of documents, legal professionals and scientists. Based on their work practices, we designed two document-centered tools: Textlets supports constraints and consistency within documents; FileWeaver automatically tracks dependencies across documents. I then describe two research directions that emerged from our empirical studies and recent literature: collaboration in documents, e.g. Jupyter notebook and direct manipulation for computational documents such as LaTeX. This dissertation will enhance our understanding of how today’s users work with documents; demonstrate novel tools and expand the fundamental theory of interaction.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103890769018901",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43227,sequence:3,track:"DC",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3415804"},authors:["Liang He"],affiliations:["Paul G. Allen School of Computer Science & Engineering, University of Washington"],title:"Designing, Controlling, and Fabricating On-Demand Structures",keywords:["Digital fabrication","everyday objects","body","space","design techniques","design tool","interaction"],abstract:"Emerging 3D printing technology has enabled designers to rapidly create physical forms for interactive devices. However, 3D-printed objects are barely interactive and adding interactivity to printed objects is inherently challenging. To address this problem, I introduce on-demand structures, a class of 3D printable structures that can be integrated with physical objects, human body, and real spaces for interaction. In my research, I create novel design techniques that support interaction (e.g., sensing and actuation) using an interdisciplinary approach. Second, I build interactive design tools that enable end-users to design and control desired behaviors without domain knowledge. Third, I fabricate the on-demand structures with readily available or hybrid fabrication techniques and demonstrate my approach with a suite of applications across different domains. Through these projects, my research aims to open new design spaces for creating interactive and functional 3D-printed devices on demand.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103893121499137",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43228,sequence:4,track:"DC",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3415803"},authors:["Toby Jia-Jun Li"],affiliations:["Carnegie Mellon University"],title:"Multi-Modal Interactive Task Learning from Demonstrations and Natural Language Instructions",keywords:["Interactive task learning","programming by demonstration","natural language instructions","end user development"],abstract:"Interactive task learning (ITL) allows end users to “teach” an intelligent agent new tasks, the corresponding task conditions, and the relevant concepts. This paper presents my research on expanding the applicability, generalizability, robustness, expressiveness, and script sharability of ITL systems using a multi-modal approach. My research demonstrates that a multi-modal ITL approach that combines programming by demonstration and natural language instructions can empower users without significant programming expertise to extend intelligent agents for their own app-based computing tasks.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103895827087390",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43229,sequence:5,track:"DC",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3415801"},authors:["Ken Nakagaki"],affiliations:["MIT Media Lab"],title:"Mechanical Shells: Physical Add-ons for Extending and Reconfiguring the Interactivities of Actuated TUIs",keywords:["Shape Changing Interface","Actuated Tangible Interface"],abstract:"In this paper, I introduce a mechanical shell concept, which is a physical add-on that can adaptively augment, extend, and reconfigure the interactive capabilities of self-actuated tangible user interfaces (TUIs). While a variety of research explores actuated and shape-changing interfaces for providing dynamic physical affordance and tangible displays, the concept of mechanical shell intends to overcome the constraint of existing generic actuated TUI hardware thereby enabling greater versatility and expression. This paper overviews the mechanical shell concept, describes project examples, outlines a research framework, and suggests open space for future research.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103898364379154",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43230,sequence:6,track:"DC",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3415806"},authors:["Jing Qian"],affiliations:["Brown University"],title:"Personalizing 3D Free-Hand Inputs for Intuitive Smartphone Augmented Reality",keywords:["Augmented Reality","Free-hand Interaction","Smartphone Interaction","Intuitive Hand Tracking"],abstract:"The goal of my research is to build practical and intuitive 3D free-hand interaction for smartphone augmented reality (AR) and to explore relevant behavioral-data-driven interaction techniques. In service of that goal, I have developed two preliminary AR systems on the smartphone: a Portalble system powered by visual, auditory and haptic feedback and a set of grabbing accommodations that allows the user’s hand to directly interact with AR contents on smartphones. I have also developed a Portalware system that incorporates a smartphone-wearable interaction scheme to expand visual feedback beyond the smartphone’s display and have leveraged this system to improve 3D mid-air sketching. My next steps are to use behavioral data such as device motion, hand motion, and user postures to create personalized interaction systems to facilitate intuitive AR interaction. Ultimately, these personalized systems could unlock new interaction possibilities for the general public and expand the use scenarios of smartphone AR applications.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103900579758080",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43231,sequence:7,track:"DC",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3379350.3415802"},authors:["Thijs Roumen"],affiliations:["Hasso Plattner Institute"],title:"Portable Laser Cutting",keywords:["Personal Fabrication","Laser Cutting","Device-Independent","Portable Fabrication","Kerf"],abstract:"Laser-cut 3D models found in repositories tend to be basic and trivial—models build over long periods of time and by multiple designers are few/non existent. I argue that this is caused by a lack of an exchange format that would allow continuing the work. At first glance, it may seem like such a format already exist, as laser cut models are already widely shared in the form of 2D cutting plans. However, such files are susceptible to variations in cutter properties (aka kerf) and do not allow modifying the model in any meaningful way (no adjustment of material thickness, no parametric changes, etc.). My first take on the challenge is to see how far we can get by still building on the de-facto standard, i.e., 2D cutting plans. springFit [7] and kerf-canceling mechanisms [6] tackle the challenge by rewriting 2D cutting plans, replacing non-port- able elements with portable ones. However, this comes at a cost of extra incisions, reducing the structural integrity of models and impacting aesthetic qualities and rare mechanisms or joints may go undetected. I thus take a more radical approach, which is to move on to a 3D exchange format (kyub [1]). This eliminates these challenges as it guarantees portability by generating a new machine-specific 2D file for the local machine. Instead, it raises the question of compatibility: Files already exist in 2D—how to get them into 3D? I demonstrate a software tool that automatically reconstructs the 3D geometry of the model encoded in a 2D cutting plan, allows modifying it using a 3D editor, and re-encodes it to a 2D cutting plan. I demonstrate how this approach allows me to make a much wider range of modifications, including scaling, changing material thickness, and even remixing models. The transition from sharing machine-oriented 2D cutting files, to 3D files, enables users worldwide to collaborate, share, and reuse. And thus, to move on from users starting thousands of trivial models from scratch to collaborating on big complex projects.",broadcast:"discord://discordapp.com/channels/734723053036109904/764103902969856000",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43241,sequence:34,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3406865.3418570"},authors:["Omer Anjum","Yucheng Liang","Chak Ho Chan","Tanitpong Lawphongpanich","Shuchen Zhang"],affiliations:["UIUC"],title:"Vertext: An End-to-end AI Powered Conversation Management System for Multi-party Chat Platforms",keywords:["artificial intelligence","collaborative chat platforms for business","transformer","deep average network","semantic search","Slack","Microsoft Teams","conversation disentanglement","search engine for chat","dialog act classification","natural language processing"],abstract:"Online communication platforms like Slack and Microsoft teams have become increasingly crucial for a digitized workplace to improve business efficiency and growth. However, these chat platforms can overwhelm the users with unstructured long streams of back and forth discussions scattered in various places. Thus, discussions become challenging to follow, leading to an increased likelihood of missing valuable information. Moreover, with the unsatisfying keyword-based chat search, users spend a significant amount of time to read, digest, and recall information from the conversations at the cost of productivity. In this paper, we present Vertext, an end-to-end AI system that ingests user conversations and automatically extracts information such as announcements, task assignments, and conversation summary. Moreover, Vertext gives a unique search experience to the users by providing search results along with their context, with an improved performance enabled by semantic search. For the ease of user interaction, all the information is consolidated on a single dashboard provided by Vertext.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104000072712192",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43242,sequence:35,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3406865.3418569"},authors:["Sunny Tian","David Karger"],affiliations:["MIT"],title:"A System for Interleaving Discussion and Summarization in Collaborative Document Writing",keywords:["Online Discussion","Collaboration","Summarization","Writing"],abstract:"In many instances of online collaboration, ideation and deliberation about what to write happen separately from the synthesis of the deliberation into a cohesive document. However, this may result in a final document that has little connection to the discussion that came before. In this work, we present interleaved discussion and summarization, a process where discussion and summarization are woven together in a single space, and collaborators can switch back and forth between discussing ideas and summarizing discussion until it results in a final document that incorporates and references all discussion points. We implement this process into a tool called Wikum+ that allows groups working together on a project to create living summariesÑartifacts that can grow as new collaborators, ideas, and feedback arise and shrink as collaborators come to consensus.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104002693890078",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43243,sequence:36,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3406865.3418567"},authors:["Akhila Sri Manasa Venigalla"],affiliations:["Indian Institute Of Technology Tirupati"],title:"Mood of India During Covid-19 - An Interactive Web Portal Based on Emotion Analysis of Twitter Data",keywords:["Covid-19","Twitter","Emotion","Sentiment Analysis","Web Portal","India"],abstract:"During outbreaks of pandemics such as Covid-19, understanding the emotional state of citizens of a country could be of interest to various organizations to carry out tasks and to take necessary measures. Twitter and other social media platforms have been bridging the gap between the citizens and government in various countries and are of more prominence in India. Analysing real time posts on twitter in India during Covid-19, could help in identifying the mood of the nation. However, most of the existing studies related to Covid-19, on twitter and other social media platforms are performed on data posted during a specific interval. We are not aware of any research that identifies emotional state of India on a daily basis. Hence, we present a web portal that aims to display mood of India during Covid-19, based on real time twitter data. This portal also enables users to select date range, specific date and state in India to display mood of people belonging to the specified region, on the specified date or during the specified date range. Also, the number of Covid-19 cases and mood of people in specific cities and states on specific dates is visualized on the country map. As of June 25 2020, the web portal has about 703618 tweets, and each of these tweets are classified into seven categories that include six basic emotions and a neutral category. A list of Trigger Events are also specified, to allow users to view the mood of India on specific events happening in the country during Covid-19. Mood of India During Covid-19 portal can be accessed from https://moodofindia.herokuapp.com/ and demonstration video can be found at https://youtu.be/2b0k_p1Hq9A.",broadcast:"discord://discordapp.com/channels/734723053036109904/764104005482840084",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}},{id:43244,sequence:37,track:"Demo",session:[42233],links:{DOI:"https://dl.acm.org/doi/10.1145/3406865.3418566"},authors:["Noble Saji Mathews"],affiliations:["Indian Institute Of Technology Tirupati"],title:"YTCoder - Towards Turning YouTube Into A Development Environment",keywords:["Video Tutorials","Youtube","Programming Languages","Code Editors"],abstract:"YouTube is one of the most popular and largest sources of videos, that hosts many video tutorials which aim to teach concepts of various programming languages. Most of these tutorials include code snippets that are displayed and written by the tutor during the video. However, it is important that the learners using these programming tutorials have a hands-on experience while learning various programming concepts. Providing a code editor along with the video tutorial could help learners get a better learning experience, as they have a scope to learn by practice. Existing solutions of accompanying video tutorials with code editors are either pre-programmed or require a separate web portal. We are not aware of any solutions in the literature that aim to support Youtube video tutorials. Hence, we present YTCoder in this paper that aims to improve the learning experience by integrating videos related to various programming languages with the development environments of the respective programming languages. It also aims to help reduce some of the repetitive and monotonous elements of programming, which might turn away newcomers by allowing the extraction of code when required. Demonstration of YTCoder can be found here at : https://youtu.be/iONtO7cuQwo",broadcast:"discord://discordapp.com/channels/734723053036109904/764104008134033409",videos:{},images:{"Gallery Picture":"http://uist.acm.org/uist2020/data/gallery-default-01.jpg"}}],authors:["Axel Antoine","Mathieu Nancel","Ella Ge","Jingjie Zheng","Navid Zolghadr","Géry Casiez","Andrew Schoen","Curt Henrichs","Mathias Strohkirch","Bilge Mutlu","Carlos de la Torre-Ortiz","Michiel Marten Spape","Lauri Kangassalo","Tuukka Ruotsalo","Toby Jia-Jun Li","Jingya Chen","Haijun Xia","Tom Mitchell","Brad A Myers","Mengqi Peng","Li-yi Wei","Rubaiat Habib Kazi","Vladimir Kim","Camilo Luciano Fosco","Vincent M Casser","Amish Kumar Bedi","Zoya Bylinskii","Aaron Hertzmann","Peter O'Donovan","Ritam Jyoti Sarmah","Yunpeng Ding","Di Wang","Cheuk Yin Phipson Lee","Xiang 'Anthony' Chen","Vidya Setlur","Enamul Hoque","Dae Hyun Kim","Angel Xuan Chang","Jeremy Hartmann","Stephen DiVerdi","Cuong Nguyen","Daniel Vogel","Ken Nakagaki","Joanne Leong","Jordan L Tappa","Joao Wilbert","Hiroshi Ishii","Youngkyung Choi","Neung Ryu","Myung Jin Kim","Artem Dementyev","Andrea Bianchi","Danli Luo","Jianzhe Gu","Fang Qin","Guanyun Wang","Lining Yao","Tuochao Chen","Benjamin Steeper","Kinan Alsheikh","Songyun Tao","Francois Guimbretiere","Cheng Zhang","Megan Hofmann","Jennifer Mankoff","Scott E Hudson","Alex Olwal","Kevin Balke","Dmitrii Votintcev","Thad Starner","Paula Conn","Bonnie Chinh","Benoit Corda","Mark Richardson","Robert Wang","Thomas Langerak","Juan José Zárate","Velko Vechev","Daniele Panozzo","Otmar Hilliges","David Lindlbauer","Nianlong Li","Han-Jong Kim","Luyao Shen","Feng Tian","Teng Han","Xing-Dong Yang","Tek-Jin Nam","Lei Zhang","Steve Oney","Jun Gong","Aakar Gupta","Hrvoje Benko","Sorin Lerner","Ryo Takahashi","Masaaki Fukumoto","Changyo Han","Takuya Sasatani","Yoshiaki Narusue","Yoshihiro Kawahara","Ryuma Niiyama","Hiroki Sato","kazzmasa TSUJIMURA","Koya Narumi","Young ah Seong","Ryosuke Yamamura","Yasuaki Kakehi","Humphrey Yang","Kuanren Qian","Haolin Liu","Yuxuan Yu","Matthew McGehee","Yongjie Jessica Zhang","YAN YIXIAN","Kazuki Takashima","Anthony Tang","Takayuki Tanno","Kazuyuki Fujita","Yoshifumi Kitamura","Richard F Lyon","Yifan Wu","Joseph Hellerstein","Arvind Satyanarayan","Youngbo Aram Shim","Keunwoo Park","Sangyoon Lee","Jeongmin Son","Taeyun Woo","Geehyuk Lee","Tianyi Wang","Xun Qian","Fengming He","Xiyun Hu","Ke Huo","Yuanzhi Cao","Karthik Ramani","Kasra Ferdowsifard","Allen Ordookhanians","Hila Peleg","Nadia Polikarpova","Christian Holz","Amy Pavel","Gabriel Reyes","Jeffrey P. Bigham","Elodie Bouzbib","Gilles Bailly","Sinan D Haliyo","Pascal Frey","Jun Nishida","Soichiro Matsuda","Hiroshi Matsui","Shan-Yuan Teng","ziwei Liu","Kenji Suzuki","Pedro Lopes","Sunbum Kim","Youngwoo Yoon","Tae-Kyun Kim","Md. Farhan Tasnim Oshim","Julian Killingback","Dave Follette","Huaishu Peng","Tauhidur Rahman","Marc McGill","Aidan Kehoe","Euan Freeman","Stephen Brewster","Jiahao Li","Meilin Cui","Jeeeun Kim","Blaine Levis","Thomas Preindl","Cedric Honnet","Andreas Pointner","Roland Aigner","Joe Paradiso","Michael Haller","Matthias R. Hohmann","Lisa Konieczny","Michelle Hackl","Brian Wirth","Talha Zaman","Raffi Enficiaud","Moritz Grosse-Wentrup","Bernhard Schölkopf","Junyi Zhu","Yunyi Zhu","Jiaming Cui","Leon Cheng","Jackson C Snowden","Mark Chounlakone","Michael Wessely","Stefanie Mueller","Woojin Lee","Thibault Louis","Jocelyne Troccaz","Amélie Rochet-Capellan","François Bérard","Yen-Ting Yeh","Homei Miyashita","Tony Wu","Shiho Fukuhara","Nicholas Gillian","Kishore Sundara-Rajan","Ivan Poupyrev","C. Ailie Fraser","Julia M. Markel","N. James Basa","Mira Dontcheva","Scott Klemmer","Chaeyong Park","Jinhyuk Yoon","Seungjae Oh","Seungmoon Choi","Eric J Gonzalez","Parastoo Abtahi","Sean Follmer","Erwin Wu","Ye Yuan","Hui-Shyong Yeo","Aaron J Quigley","Hideki Koike","Kris Kitani","Peggy Chi","Zheng Sun","Katrina Panovich","Irfan Essa","Hugo Romat","Christopher Collins","Nathalie Henry Riche","Michel Pahud","Adam Riddle","William Buxton","Ken Hinckley","Danny Leen","Nadya Peek","Raf Ramakers","Thomas Ball","Shannon Kao","Richard Knoll","Daryl Zuniga","Shota Yamanaka","Hiroki Usuba","Haruki Takahashi","Richard Lin","Rohit Ramesh","Connie Chi","Nikhil Jain","Ryan Nuqui","Prabal Dutta","Bjoern Hartmann","Ruofei Du","Eric Lee Turner","Max Dzitsiuk","Luca Prasso","Ivo Ferrao Duarte","Jason Dourgarian","Joao Afonso","Jose Pascoal","Josh Gladstone","Nuno Cruces","Shahram Izadi","Adarsh Kowdle","Konstantine Tsotsos","David Kim","Wenzhe Cui","Suwen Zhu","Mingrui Ray Zhang","Andrew Schwartz","Jacob O. Wobbrock","Xiaojun Bi","Xiao TANG","Xiaowei Hu","Chi-Wing Fu","Daniel CohenOr","Jackie (Junrui) Yang","Monica Lam","James A. Landay","Karan Ahuja","Andy Kong","Mayank Goel","Chris Harrison","Jonas Auda","Roman Heger","Thomas Kosch","Uwe Gruenefeld","Stefan Schneegass","Tzu-Yun Wei","Hsin-Ruey Tsai","Yu-So Liao","Chieh Tsai","Yi Shan Chen","Chi Wang","Bing-Yu Chen","Tim Rietz","Peyman Toreini","Alexander Maedche","Daehwa Kim","Zhenjie Zhao","Xiaojuan Ma","Evan Shimizu","Matthew David Fisher","Sylvain Paris","James McCann","Kayvon Fatahalian","Mayra Donaji Barrera Machuca","Alvaro Cassinelli","Christian Sandor","Rinat Abdrashitov","Fanny Chevalier","Karan Singh","HIROO YAMAMURA","Holger Baldauf","Kai Kunze","Maria Katarina Larsson","Hironori Yoshida","Nobuyuki Umetani","Takeo Igarashi","Suzanne Kobeisse","Lars Erik Holmquist","Thijs Roumen","Ingo Apel","Jotaro Shigeyama","Muhammad Abdullah","Patrick Baudisch","Lingyun Sun","Jiaji Li","Yu Chen","Yue Yang","Ye Tao","Thomas Martin Prinz","Linda Gräfe","Jan Plötner","Tai WANG","Conghui He","Zhe Wang","Jianping Shi","Dahua Lin","Jack Forman","Mustafa Doga Dogan","Hamilton Forsythe","Tzu-Sheng Kuo","Eric Rawn","Balasaravanan Thoravi Kumaravel","Md Tahsin Tausif","RJ Weaver","Sang Won Lee","Robert Kovacs","Eyal Ofek","Mar Gonzalez-Franco","Alexa Fay Siu","Sebastian Marwecki","Mike Sinclair","Reiya Horii","Yurike Chandra","Kouta Minamizawa","Yunwoo Jeong","Gyeongwon Yun","André Zenner","Antonio Krüger","Paul Strohmeier","Seref Güngör","Luis Herres","Dennis Gudea","Bruno Fruchard","Jürgen Steimle","Priyan Vaithilingam","Philip Guo","Jingyuan Liu","Hongbo Fu","Chiew-Lan Tai","Andrew Kurauchi","Wenxin Feng","Ajjen Joshi","Carlos H Morimoto","Margrit Betke","Chiu-Hsuan Wang","Hsin-Yu Chen","Yuan-Syun Ye","Seraphina Yong","Liwei Chan","Martin Feick","Niko Kleer","Christopher Clarke","Doga Cavdir","Patrick Chiu","Laurent Denoue","Don Kimber","Amber Aftab","Ruipu Hu","Devamardeep Hayatpur","Daniel Wigdor","Ayaka Ishii","Kaori Ikematsu","Itiro Siio","Ryo Suzuki","Wilmot Li","Daniel Leithinger","Ryusei Uramune","Hiroki Ishizuka","Takefumi Hiraki","Sei Ikeda","Osamu Oshiro","Jess McIntosh","Hubert Zajac","Andreea Stefan","Joanna Bergström","Kasper Hornbæk","Shoki Kishida","Sho Sakurai","Koichi Hirota","Takuya Nojima","Jennifer Jacobs","Maneesh Agrawala","Cristian Sorescu","Yogesh Kumar Meena","Deepak Ranjan Sahoo","Masahiro Yamaguchi","Shohei Mori","Peter Mohr","Markus Tatzgern","Ana Stanescu","Hideo Saito","Denis Kalkofen","Ashrith Shetty","Ebrima Jarjue","Julien Gori","Han L. Han","Michel Beaudouin-Lafon","Minori Narita","Nolwenn Maudet","Yi Lu","Joseph Chee Chang","Nathan Hahn","Aniket Kittur","Songlin Xu","Zhiyuan Wu","ShunHong Wang","Rui Fan","Nan Lin","Chia-En Tsai","Patrick Coe","Grigori Evreinov","Hasse Sinivaara","Arto Hippula","Roope Raisamo","Hee-Seung Moon","Jiwon Seo","Geoffrey X. Yu","Tovi Grossman","Gennady Pekhimenko","Ahmed Farooq","Hong Z Tan","Parinya Punpongsanon","Donghan Hu","Yoonji Kim","Hyein Lee","Ramkrishna Prasad","Seungwoo Je","Daniel Ashbrook","Ian Oakley","Ziqian Shao","Yuta Noma","Fuminori Okuya","Florin-Timotei Ghiurãu","Mehmet Aydın Baytaş","Casper Wickman","Yu-Jung Ko","HANG ZHAO","Yoonsang Kim","IV Ramakrishnan","Shumin Zhai","Miles Priebe","Esther W Foo","Brad Holschuh","Mary Beth Kery","Donghao Ren","Fred Hohman","Dominik Moritz","Kanit Wongsuphasawat","Kayur Patel","Audrey Labrie","Jinghui Cheng","Frederik Brudy","David Ledo","Anand Waghmare","Hemant Surale","Marcus Peinado","Xiaokuan Zhang","Shannon Joyner","Badrish Chandramouli","Umar Farooq Minhas","Jonathan Goldstein","Kunihiro Kato","Tianyi Zhang","London Lowmanstone","Xinyu Wang","Elena L. Glassman","Noushad Sojib","Syed Masum Billah","Mohammad Amin","Amy X. Zhang","Grant Hugh","Michael S. Bernstein","Dong-Hyun Hwang","Kohei Aso","Philippe Schmid","Sylvain Malacria","Andy Cockburn","Te-Yen Wu","Lu Tan","Yuji Zhang","Teddy Seyed","Pascal Jansen","Fabian Fischbach","Jan Gugenheimer","Evgeny Stemasov","Julian Frommel","Enrico Rukzio","Jorge Garza","Devon J. Merrill","Steven Swanson","Hye Yeon Nam","Iyleah Hernandez","Brendan Harmon","Waleed Hashmi","Moumena Chaqfeh","Lakshmi Subramanian","Yasir Zaki","Yevhen Romaniak","Anastasiia Smielova","Yevhenii Yakishyn","Valerii Dziubliuk","Mykhailo Zlotnyk","Oleksandr Viatchaninov","Rishi Vanukuru","Amarnath Murugan","Jayesh S. Pillai","Tomoya Tada","Shigeyuki Hirai","Hiroshi Suzuki","Akihito Nishiike","Kentaro Yoshida","Masahiro Sato","Yota Komoriya","Takayuki Ezaki","Eric Balagtas Perez","James King","Yugo H Watanabe","Benjamin Charles Germain Lee","Daniel Weld","Sawa Korogi","Takuya Kitade","Wataru Yamada","Takahiro Kusabuka","Takuya Indo","Kyzyl Monteiro","Arunesh Singh","Navneet Ratnakirti","Shreemoy Nanda","Aman Mandal","Xiao Tang","Han Yao Choong","Jonathan Lehner","Jan Wiegner","Toshiharu Igarashi","Kaoru Takagi","Ben Greenspan","Andreea Danielescu","Vishnu Nair","Brian A. Smith","Kristin Williams","Liang He","Jing Qian","Omer Anjum","Yucheng Liang","Chak Ho Chan","Tanitpong Lawphongpanich","Shuchen Zhang","Sunny Tian","David Karger","Akhila Sri Manasa Venigalla","Noble Saji Mathews"],sessions:{42197:{name:"Session 1A: Cross-X Interaction",start:16032888e5,end:160329e7},42198:{name:"Session 2A: Shape-Change",start:16032888e5,end:160329e7},42200:{name:"Session 3A: Tracking Bodies and Faces Using Cameras",start:16032888e5,end:160329e7},42201:{name:"Session 1B: Bridging Programming and Visualization",start:16032903e5,end:16032915e5},42202:{name:"Session 2B: Augmenting Video with Sketch and Gesture Input",start:16032903e5,end:16032915e5},42203:{name:"Session 3B: Haptics for Room-Scale VR",start:16032903e5,end:16032915e5},42204:{name:"Session 4A: Generating, Exploring and Understanding Design Space of Visual Media",start:16032924e5,end:16032936e5},42205:{name:"Session 5A: Fabrication: Joints and Mechanisms",start:16032924e5,end:16032936e5},42206:{name:"Session 6A: AR/VR Authoring Environments",start:16032924e5,end:16032936e5},42207:{name:"Session 4B: Online Communities, Data Collection, and the Web",start:16032939e5,end:16032951e5},42208:{name:"Session 5B: Sensing Touch, Pressure, and Objects",start:16032939e5,end:16032951e5},42209:{name:"Session 7A: Adaptive Input: State Changes and Correction",start:16033698e5,end:1603371e6},42210:{name:"Session 8A: Designing and Fabricating Electronics on Surfaces",start:16033698e5,end:1603371e6},42211:{name:"Session 9A: Haptics for Hands and Feet",start:16033698e5,end:1603371e6},42212:{name:"Session 7B: Live Programming and Synthesis",start:16033713e5,end:16033725e5},42213:{name:"Session 8B: Sensing and Actuation on Textiles",start:16033713e5,end:16033725e5},42214:{name:"Session 9B: Interpreting and Adapting Hand Input",start:16033713e5,end:16033725e5},42215:{name:"Session 10A: Bridging Visual and Audio Content",start:16033788e5,end:160338e7},42216:{name:"Session 11A: Authoring Animation",start:16033788e5,end:160338e7},42217:{name:"Session 12A: Space and Depth for Interaction in VR",start:16033788e5,end:160338e7},42218:{name:"Session 10B: Interaction Models",start:16033803e5,end:16033815e5},42219:{name:"Session 11B: Fabrication of Reconfigurables",start:16033803e5,end:16033815e5},42220:{name:"Session 12B: Haptics for Hand-Held Devices",start:16033803e5,end:16033815e5},42221:{name:"Session 13A: Supporting Expert Use",start:16034634e5,end:16034646e5},42222:{name:"Session 14A: Supporting Users with Visualizations and Tutorials",start:16034634e5,end:16034646e5},42223:{name:"Session 15A: ForceFeedback and Haptics in VR",start:16034634e5,end:16034646e5},42224:{name:"Session 13B: Brain and Taste",start:16034649e5,end:16034661e5},42225:{name:"Session 14B: Transcribing Words and Directing Voice",start:16034649e5,end:16034661e5},42226:{name:"Session 15B: Sensing Hands",start:16034649e5,end:16034661e5},42227:{name:"Session 13C: Authoring Environments: Speech, Video Games, Robots",start:16034664e5,end:16034676e5},42228:{name:"Session 14C: Fabrication of Filaments and Textiles",start:16034664e5,end:16034676e5},42229:{name:"Session 15C: Devices and Approaches For Interacting with 3D virtual content",start:16034664e5,end:16034676e5},42230:{name:"Session 6B: Head-Mounted Projectors for VR",start:16032939e5,end:16032951e5},42233:{name:"Interactive Session A: Demos, Posters, and Doctoral Consortium Posters",start:16033824e5,end:16033878e5},42908:{name:"Interactive Session B: Demos, Posters, and Student Innovation Challenge Demos",start:16034562e5,end:16034616e5}}};const Ee=document.querySelector("main");new class extends n{constructor(e){super(),r(this,e,Le,He,c,{program:0},[-1,-1])}}({hydrate:!0,target:Ee,props:{program:Oe}});
